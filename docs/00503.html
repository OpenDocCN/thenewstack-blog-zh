<html>
<head>
<title>This Robot Can Visualize Its Immediate Future with Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这个机器人可以通过深度学习来想象它的未来</h1>
<blockquote>原文：<a href="https://thenewstack.io/robot-can-visualize-immediate-future-deep-learning/#0001-01-01">https://thenewstack.io/robot-can-visualize-immediate-future-deep-learning/#0001-01-01</a></blockquote><div><div id="tns-post-body-content">


<p class="translated">看着一个婴儿推、拉和抓东西，他们似乎做不了什么。但事实上，当婴儿参与这种无人监督的实验时，他们会学到很多东西，自学他们周围的物理环境以及如何操纵他们遇到的物体。然后，人类儿童能够吸取这些教训，并通过想象和预测如何解决这些问题，利用他们以前学到的知识，将它们应用到新的未知情况中。</p>
<p class="translated">虽然这种通用的、无监督的学习和对因果关系的预测对人类来说很容易掌握和理解，但机器会发现很难做到这一点。为了帮助解决这个问题，加州大学伯克利分校人工智能研究实验室(BAIR)的研究人员正在开发一种被他们称为视觉预见的深度学习技术(T2)。该技术将向机器人灌输一种短期想象力，使它们能够根据它们在以前无人监督的学习情况下可能收集到的信息来预测某些行动的潜在结果——在这种情况下，它们可以自己移动各种物体。</p>
<p class="translated"><iframe loading="lazy" title="Vestri the robot imagines how to perform tasks" src="https://www.youtube.com/embed/Li_vZVpiFSA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p>
<h2 class="translated">机器人可视化</h2>
<p class="translated">虽然这种新的机器人预见并没有走得很远——只有几秒钟的未来——但这是一个相对较大的飞跃，因为它允许机器人在没有人类干预或任何关于物体或环境本身的知识的情况下操纵物体。通常，要让机器四处移动对象，传统方法需要人类程序员用某种标签信息来识别对象，以便它可以与其环境中的对象进行交互。虽然这种方法在像工厂这样的封闭环境中可能足够了，但对于扩大到可能需要提前快速思考的更大的真实世界的情况来说是不切实际的，例如自动驾驶车辆在路上可能遇到的情况。</p>
<p class="translated">“就像我们可以想象我们的行为将如何移动我们环境中的物体一样，这种方法可以让机器人想象不同的行为将如何影响周围的世界，”负责监督这项研究的工程和计算机科学助理教授谢尔盖·莱文说。“这可以在复杂的现实世界情况下智能规划高度灵活的技能。”</p>
<p class="translated">该团队的发现<a href="https://arxiv.org/pdf/1605.07157.pdf" target="_blank" class="ext-link" rel="external ">报告了</a>这个名为Vestri的机器人是如何通过一个独立的“游戏”阶段的，在这个阶段，它被给予一组随机的物体在桌子上推来推去。所有的动作都被维斯特里的摄像机捕捉到了。</p>
<p class="translated"><img decoding="async" loading="lazy" class="aligncenter size-full wp-image-3914582" src="../Images/ec070acd92e69b8bcbef3c927b6dbf49.png" alt="" data-original-src="https://storage.googleapis.com/cdn.thenewstack.io/media/2018/01/7843b1a8-vestri-robot-imagines-its-future-5.jpg"/></p>
<p class="translated"><img decoding="async" loading="lazy" class="aligncenter size-full wp-image-3914583" src="../Images/a61dc9b80282b1baf7a09b6123e3912b.png" alt="" data-original-src="https://storage.googleapis.com/cdn.thenewstack.io/media/2018/01/722978e5-vestri-robot-imagines-its-future-6.jpg"/></p>
<p class="translated">在这个持续约一周的播放阶段，包括与各种对象的超过59，000次交互，机器人使用了一种称为卷积递归视频预测或动态神经平流(DNA)的深度学习技术来建立其世界的预测模型。该模型使用图像像素来帮助它预测未来的运动场景，“想象”不同动作的不同未来结果。这些模型建立在机器人以前在自我游戏中学习的经验基础上，然后可以将这种知识推广到不熟悉的物体，从而允许机器人执行指定的任务，物理地处理新的、以前从未见过的物品，而不需要繁琐的标签或专门的编码。</p>
<p class="translated"><img decoding="async" loading="lazy" class="aligncenter size-full wp-image-3914581" src="../Images/bb01b010ea114ccd85d475878b3e3d8a.png" alt="" data-original-src="https://storage.googleapis.com/cdn.thenewstack.io/media/2018/01/a55660d3-vestri-robot-imagines-its-future-4.jpg"/></p>
<p class="translated"><img decoding="async" loading="lazy" class="aligncenter size-full wp-image-3914579" src="../Images/48e7eb6bf4d9375165d2b83d1ccb688d.png" alt="" data-original-src="https://storage.googleapis.com/cdn.thenewstack.io/media/2018/01/d45dd569-vestri-robot-imagines-its-future-2.jpg"/></p>
<p class="translated">开发一种能够自主学习的人工智能，并“概括”这些经验教训，以便它们可以在更大范围内应用的可能性——是当前人工智能研究的圣杯。我们已经看到了人工智能中这种一般化的、类似人类的学习的曙光，它可以学习并<a href="https://thenewstack.io/innovative-gaming-moves-googles-ai-becomes-go-grandmaster-three-days/" target="_blank" class="local-link">自学如何从头掌握复杂的棋盘游戏</a>，或者机器系统可以可靠地<a href="https://thenewstack.io/ai-human-like-vision-cracks-captcha-code/" target="_blank" class="local-link">独自破解视觉难题</a>，而无需受益于大量训练数据。在这种情况下，我们看到这种<em>白板</em>方法如何应用于现实世界中物理运动的预测模型。</p>
<p class="translated">“孩子们可以通过玩玩具、移动玩具、抓握玩具等方式来了解他们的世界。我们这项研究的目的是让机器人也能做同样的事情:通过自主互动来了解世界是如何运转的，”莱文解释道。“这个机器人的能力仍然有限，但它的技能是完全自动学习的，并允许它通过建立以前观察到的交互模式来预测它从未见过的对象的复杂物理交互。”</p>
<p class="translated">该团队现在正致力于进一步开发这种视频预测系统，作为机器人的一种“视觉想象”，目的是使机器人不仅能够推动物体，还能够组装、抓取和重新定位它们，以及处理可以捆绑或折叠的有延展性的物体。</p>


<div class="tns-logo-slug">
<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 68 31" version="1.1">
<title>Group</title>
<desc>Created with Sketch.</desc>
<g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
<g id="Group">
<path d="M24.002,29.619 L29.77,29.619 L29.77,15.808 C29.77,15.038 29.622,11.265 29.59,10.414 L29.77,10.414 C31.424,14.019 31.473,14.147 32.168,15.322 L39.65,29.618 L44.845,29.618 L44.845,0 L39.075,0 L39.075,11.064 C39.075,12.197 39.075,12.44 39.182,14.472 L39.325,17.468 L39.151,17.468 C39.034,17.267 38.596,16.173 38.467,15.929 C38.164,15.323 37.725,14.512 37.373,13.905 L30.031,0 L24,0 L24,29.619 L24.002,29.619 Z" id="Path-Copy" fill="#FF3287"/>
<path d="M56.948,0 C50.745,0 47.606,3.43 47.606,8.296 C47.606,14.114 51.036,15.404 55.518,17.132 C60.438,18.853 61.782,19.332 61.782,21.539 C61.782,24.225 58.969,24.867 57.401,24.867 C54.579,24.867 52.493,23.342 51.536,20.858 L47,24.185 C49.43,28.937 52.145,30.185 57.713,30.185 C59.364,30.185 62.059,29.74 63.727,28.694 C67.779,26.156 67.779,22.22 67.779,20.898 C67.779,18.129 66.531,16.207 66.178,15.726 C65.049,14.121 63.032,12.918 61.25,12.278 L57.084,10.914 C55.073,10.267 52.928,10.105 52.928,8.019 C52.928,7.707 53.008,5.528 56.288,5.319 L61.465,5.319 L61.465,0 C61.465,0 57.342,0 56.948,0 Z" id="Path-Copy-2" fill="#00AFF4"/>
<polygon id="Path" fill="#00AFF4" points="5.32907052e-15 1.77635684e-15 5.32907052e-15 5.319 7.572 5.319 7.572 29.564 14.132 29.564 14.132 5.319 21.544 5.319 21.544 1.77635684e-15"/>
</g>
</g>
</svg> </div>
</div>
</div>    
</body>
</html>