<html>
<head>
<title>OpenAI Algorithm Allows AI to Learn from Its Mistakes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenAI算法允许AI从错误中学习</h1>
<blockquote>原文：<a href="https://thenewstack.io/openai-algorithm-allows-ai-to-learn-from-its-mistakes/#0001-01-01">https://thenewstack.io/openai-algorithm-allows-ai-to-learn-from-its-mistakes/#0001-01-01</a></blockquote><div><div id="tns-post-body-content">


<p class="translated">人类和许多其他动物的一个关键特征是从错误中学习的能力。当面对要实现的目标或要掌握的特定技能或任务时，我们通常需要尝试很多次才能成功——或者至少做得足够好。在经历一次又一次失败的过程中，我们学会了如何做某事的各种方法。每次我们失败，我们都会学到一些新的东西，可能会帮助我们最终正确地做这件事，事后来看，这让我们明白失败是成功的必要条件。</p>
<p class="translated">虽然这种从后见之明中学习的特性对人类来说是天生的，但对机器人来说却不一定如此。在人工智能模型中，我们可能有最接近的类比是<a href="https://thenewstack.io/reinforcement-learning-ready-real-world/" target="_blank" class="local-link">强化学习</a>，机器的任务是解决某种问题，并根据它解决问题的程度，“赚取”不同数量的奖励。然而，设计这种依赖奖励的机器学习方法可能会变得复杂，并不总是容易转化为现实世界，实际上可能会阻碍人工智能代理的探索。</p>
<p class="translated">为了解决这个难题，同时开发一种更简单的方法让机器从错误中学习，总部位于旧金山的人工智能研究公司<a href="https://openai.com/" class="ext-link" rel="external "> OpenAI </a>最近发布了一种叫做<a href="https://arxiv.org/pdf/1707.01495.pdf" target="_blank" class="ext-link" rel="external ">马后炮体验回放</a> (HER)的开源算法——这是旨在用于人工智能研究开发的模拟机器人环境的更大版本的一部分。下面是她的简要解释:</p>
<p class="translated"><iframe loading="lazy" title="Ingredients for Robotics Research" src="https://www.youtube.com/embed/8Np3eC_PTFo?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p>
<p class="translated">根据研究人员的说法，这里的想法是让机器以更类似于人类的方式从失败中学习。研究人员<a href="https://blog.openai.com/ingredients-for-robotics-research/#content" class="ext-link" rel="external ">在一篇博客文章中写道:“她的关键见解是人类凭直觉做的事情:即使我们没有在一个特定的目标上取得成功，我们至少已经实现了一个不同的目标。”。“那么，为什么不假装我们一开始就想实现这个目标，而不是我们最初打算实现的目标呢？”</a></p>
<p class="translated">HER算法通过使用所谓的“稀疏和二进制”奖励来实现这一点，该奖励仅向代理提供失败或成功的指示。相比之下，传统强化学习中使用的“密集的”、“成形的”奖励会提示代理，他们是否正在“接近”、“接近”、“非常接近”或“非常接近”达到他们的目标。这种所谓的密集奖励可以加快学习过程，但缺点是这些密集奖励通常不包含太多供代理学习的学习信号，并且可能难以设计和实现真实世界的应用。</p>
<p class="translated"><img decoding="async" loading="lazy" class="aligncenter size-full wp-image-4476093" src="../Images/6006c6f56bea2519360f5777fb3da6be.png" alt="" data-original-src="https://storage.googleapis.com/cdn.thenewstack.io/media/2018/04/dc6bb54a-hindsight-replay-experience-algorithm-openai-2.png"/></p>
<p class="translated"><img decoding="async" loading="lazy" class="aligncenter size-full wp-image-4476094" src="../Images/ed7a909e8b46d4a7bcc090f858aa8c9f.png" alt="" data-original-src="https://storage.googleapis.com/cdn.thenewstack.io/media/2018/04/0c552d49-hindsight-replay-experience-algorithm-openai-3.png"/></p>
<p class="translated">相比之下，对她来说，即使一个人工智能代理在执行任务时不成功，它仍然会收到少量的奖励来进一步鼓励它。此外，她还创建了一个“多目标”强化学习框架，将每一次失败的尝试重新定义为成功，每次都略微改变目标，以便有一个学习信号可以利用，同时仍然牢记总体目标。</p>
<p class="translated">通过进行这种替换，强化学习算法可以获得学习信号，因为它已经实现了某些目标；即使这不是你最初想要实现的目标，”该团队写道。“如果你重复这个过程，你最终会学会如何实现任意的目标，包括你真正想实现的目标。”</p>
<p class="translated">该团队使用基于现有硬件机器人的模拟环境测试了HER算法，以及它们在各种操纵任务中的表现，如推、滑和取放。然后，该算法从模拟环境转移到真实的机器人手臂上。与未修改的深度强化学习模型相比，观察添加HER如何提高解决这些任务的相对成功率:</p>
<p class="translated"><iframe loading="lazy" title="Hindsight Experience Replay" src="https://www.youtube.com/embed/Dz_HuzgMxzo?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p>
<p class="translated">相对于实验中的其他强化学习技术，该团队的比较显示，在基于目标的环境中，人工智能代理可能无法在早期实现初始目标，她的表现明显更好。然而，与其他强化学习技术不同，使用她，代理能够在每次尝试完成后事后学习新的东西。</p>
<p class="translated"><img decoding="async" loading="lazy" class="aligncenter size-full wp-image-4476095" src="../Images/0e2112e7c410bb734a23e25c0d3897c4.png" alt="" data-original-src="https://storage.googleapis.com/cdn.thenewstack.io/media/2018/04/75f0e9dd-hindsight-replay-experience-algorithm-openai-4.png"/></p>
<p class="translated">稀疏奖励的使用似乎也增加了成功率，因为代理人不会受到与解决原始任务无关的探索行为的惩罚。稀疏奖励的设计和实施也更简单，不需要对学习发生的领域有更深入的了解——这更接近于模仿现实世界中的学习方式。这些是令人着迷的发现，可能是第一次表明复杂的操纵行为可以仅使用简单的二进制奖励来学习，并且是朝着创造能够像人类一样学习的智能机器迈出的又一步。</p>
<p class="attribution translated">图片:OpenAI</p>


<div class="tns-logo-slug">
<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 68 31" version="1.1">
<title>Group</title>
<desc>Created with Sketch.</desc>
<g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
<g id="Group">
<path d="M24.002,29.619 L29.77,29.619 L29.77,15.808 C29.77,15.038 29.622,11.265 29.59,10.414 L29.77,10.414 C31.424,14.019 31.473,14.147 32.168,15.322 L39.65,29.618 L44.845,29.618 L44.845,0 L39.075,0 L39.075,11.064 C39.075,12.197 39.075,12.44 39.182,14.472 L39.325,17.468 L39.151,17.468 C39.034,17.267 38.596,16.173 38.467,15.929 C38.164,15.323 37.725,14.512 37.373,13.905 L30.031,0 L24,0 L24,29.619 L24.002,29.619 Z" id="Path-Copy" fill="#FF3287"/>
<path d="M56.948,0 C50.745,0 47.606,3.43 47.606,8.296 C47.606,14.114 51.036,15.404 55.518,17.132 C60.438,18.853 61.782,19.332 61.782,21.539 C61.782,24.225 58.969,24.867 57.401,24.867 C54.579,24.867 52.493,23.342 51.536,20.858 L47,24.185 C49.43,28.937 52.145,30.185 57.713,30.185 C59.364,30.185 62.059,29.74 63.727,28.694 C67.779,26.156 67.779,22.22 67.779,20.898 C67.779,18.129 66.531,16.207 66.178,15.726 C65.049,14.121 63.032,12.918 61.25,12.278 L57.084,10.914 C55.073,10.267 52.928,10.105 52.928,8.019 C52.928,7.707 53.008,5.528 56.288,5.319 L61.465,5.319 L61.465,0 C61.465,0 57.342,0 56.948,0 Z" id="Path-Copy-2" fill="#00AFF4"/>
<polygon id="Path" fill="#00AFF4" points="5.32907052e-15 1.77635684e-15 5.32907052e-15 5.319 7.572 5.319 7.572 29.564 14.132 29.564 14.132 5.319 21.544 5.319 21.544 1.77635684e-15"/>
</g>
</g>
</svg> </div>
</div>
</div>    
</body>
</html>