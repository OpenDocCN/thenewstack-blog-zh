# 英伟达专注于用 CUDA 实现类似苹果的人工智能

> 原文：<https://thenewstack.io/nvidia-hones-in-on-apple-like-approach-to-ai-with-cuda/>

“人工智能的 iPhone 时刻已经开始，”英伟达首席执行官黄仁勋在本周该公司在 T2 举行的 GTC 会议上发表主题演讲时说。

他指的是 [OpenAI 的 ChatGPT](https://thenewstack.io/beware-chatgpt-a-language-model-in-the-shape-of-shakespeare/) ，仅半年多时间就吸引了数百万用户。用户的查询是在英伟达的 GPU 上处理的，它会向用户发出响应。

就像苹果拥有自己的硬件和软件堆栈一样，英伟达正在使用其专有的硬件和软件工具将开发者锁定在其生态系统中。为此，Nvidia 在其 GPU 技术大会上宣布了新的工作流程，以开发像 ChatGPT 这样的大型语言模型应用程序。

可以肯定的是，英伟达和苹果一样，拥有最好的人工智能软硬件栈。但是，一旦开发者被锁定在 Nvidia 的专有生态系统中，它可能会很昂贵，并且很难摆脱。

但英伟达在人工智能领域占据主导地位，在将编码人员纳入其阵营方面更胜一筹。Nvidia 的 GPU 在芯片制造商的 CUDA 并行编程框架中编码时性能最佳，该框架编译代码并将工作和数据分派给 GPU。

黄强调了正在进行的向生殖人工智能的转变，这仍然需要人类的接触。像 ChatGPT 这样的工具可以自动化编码，但是开发人员需要考虑一些新的因素，比如针对硬件加速器调整代码以提供最快的结果。

传统编码依赖于 CPU 上的并行性，这在性能上已经达到了极限。对于人工智能，软件需要与 GPU 等专门的加速器对话，这些加速器接受查询，权衡各种参数，并给出最佳可能的答案。

## 开源 CUDA

Nvidia 正在开源 CUDA 库，这使得将工作负载转移到编程框架变得更加容易。开发人员可以在他们认为合适的时候将这些库修改到他们的应用程序中，这使他们更容易过渡到 CUDA 和 GPU 加速。

“加速计算并不容易。它需要全栈发明——从芯片、系统、网络加速库，到重构应用，”黄在主题演讲中说。

ChatGPT 有时对用户不可用，因为服务器达到了峰值容量。对生成性人工智能的突然兴趣造成了运行算法的硬件短缺。人工智能硬件的第一个目标是云本土公司，如脸书、谷歌和微软，它们正在设计数据中心来处理人工智能应用。

有一个基本的编码问题，因为 ISO C++没有本地并行性。程序员使用像 Nvidia 的 CUDA 这样的框架，可以重新编译代码来利用 GPU 的计算能力。

CUDA 提供的库和框架包括高度优化的数学库、数据结构和算法的核心库，以及用于扩展应用程序的通信库。CUDA 支持 C++、Fortran 和 Python，并与 TensorFlow 或 PyTorch 等软件库配合使用。

“不仅仅是(CUDA)直接支持的语言，其他公司和团体开发的数十种其他语言也编译并运行在 GPU 上，”Nvidia 首席软件架构师斯黛芬·琼斯在贸易展上关于 CUDA 的分组会议上表示。

CUDA 可以从许多编程语言重新编译，但不能从 WebAssembly 重新编译。

“我想我听说过一些学术项目，人们甚至在关注它。我认为这是多节点系统的许多发展方向之一:没有一个真正通用的标准。不过，web assembly 的通用性非常有吸引力，”一位 Nvidia 主持人在 CUDA 会议期间表示。

这还包括重新编译代码以在量子计算机上工作，量子计算机可以在英伟达硬件上模拟。程序员可以使用常规代码，在 CUDA 中重新编译，并查看它如何在 GPU 上模拟的代理量子计算机环境中运行。

但 GTC 的焦点完全集中在人工智能和英伟达的 GPU 上。黄说，仅在四年前，GTC 的面对面会议只有 8000 名与会者，而今年大约有 25 万名开发者参加了虚拟会议。

“生成式人工智能是一种新型计算机……每个人都可以指挥计算机解决问题。这是一个只属于计算机程序员的领域。现在，每个人都是程序员，”黄在主题演讲中说。

对于编码人员来说，这听起来可能是个坏消息，但 Nvidia 高管在展会上表示，加速计算将帮助开发人员进入一个涉及推理和预测结果的概率计算新时代。

企业计算副总裁 Manuvir Das 在新闻发布会上表示，应用程序开发正在过渡到创建人工智能模型。

Nvidia 宣布了名为 Foundations 的预打包人工智能模型，因此编码人员和数据科学家可以开发自己的聊天机器人以及图像和视频生成器。

一项名为 NeMo 的服务将允许开发类似 ChatGPT 的体验，人工智能可以在其中生成摘要，寻找市场情报，或回答问题。另一个名为 Picasso 的模块用于生成图像、视频或 3d 模型，BioNeMo 用于蛋白质结构和其他生物技术应用。

“客户可以带来他们的模型，或者从 NeMo 预训练的语言模型开始，在整个过程中，从 GPT-8，GPT-43 和 GPT-5300 亿个参数，”Das 说。

每个模型都可以连接到专有数据集，并且可以随着时间的推移随着更多数据的添加而改进。也有护栏，以防止人工智能过于情绪化或以不良答案回应，这发生在微软的 Bing AI 和谷歌的 Bard 上。

开发人员可以通过 API 访问模型。每种模态都包括调优的推理引擎、数据处理框架和向量数据库。

没有提供基础型号的定价，但从其他 Nvidia 硬件和软件公告来看，这可能需要相当大的投资。

## DGX 云

NeMo 和 Picasso 服务可以通过 Nvidia 的 [DGX 云](https://www.nvidia.com/en-us/data-center/dgx-cloud/)硬件访问，这也是在展会上宣布的。

DGX 云在云中提供对其最新 GPU 和人工智能企业软件工具包的访问，起价为每月 37，000 美元。这大约是 Azure 的 Nvidia GPU 实例价格的两倍，后者每月最高约为 2 万美元。DGX 云服务将通过公共云提供商提供给客户，包括 Azure 和甲骨文。

Nvidia EGX 计算副总裁 Justin Boitano 表示，对人工智能的投资可能有助于降低成本。

“最终，如果你能以较低的成本获得你需要的商业成果，通常会释放投资到新的领域，”Boitano 说。

Nvidia 还推出了其他工具，包括 CvCUDA，用于视频处理，以更快地将多媒体传输到智能手机和其他设备。CvCUDA 提供了 30 个具有 Python 和 C++绑定的操作符，这使得它可以使用加速计算来进行图像变形、视频编辑和图像处理。

“我们想做的是看看 TensorFlow 和 Pytorch 中与人工智能相邻的瓶颈，并确保有非常有效的视频预处理和后处理，”Boitano 说。

CvCUDA 可以在 GitHub 库中获得，并以源代码的形式发布。该公司将与希望在该库基础上进行开发的开发者合作。

“它的开放性基本上允许人们最终根据需要使用它，他们可以创建自己的副本并对其进行改进。然后他们就有了在生产中运行它的许可证，”Boitano 说。

另一个名为 CuOpt 的 CUDA 工具解决了 Boitano 所说的优化最佳路线的“旅行推销员”问题。

开发者可以从 ESRI 的 ArcGIS 云服务中提取数据，并围绕路线建立成本模型。公司可以优化取货点、所需汽车数量、商店位置，并以最短的时间和最低的成本优化路线。

Boitano 说:“我们还将寻求为那些希望将它作为一种服务来消费，而不是在他们的数据中心建立基础设施的人提供一个 API。”

微软也将其软件和云的未来押在了 Nvidia 硬件上。这家软件巨头将部署英伟达的 OVX-2 服务器，将元宇宙应用程序放入微软 Office 365 应用程序中。微软的必应(Bing with AI)运行在英伟达(Nvidia)的 A100 图形处理器上，该公司正在基于名为 Hopper 的架构，使用最新的 H100 图形处理器构建一台超级计算机。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>