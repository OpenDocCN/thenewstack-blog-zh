# 如何在 Kubernetes 上应对 ML 工作负载的硬件挑战

> 原文：<https://thenewstack.io/how-to-meet-the-hardware-challenge-for-ml-workloads-on-kubernetes/>

开发团队正在快速学习机器学习(ML)如何通过以多种方式代替人类进行软件开发和操作来提高效率。在应用开发领域，开发人员越来越多地为新兴应用(如无人驾驶汽车和面部识别元数据应用)创建 ML 基础设施。

但是 ML 应用程序开发和 ML 辅助生产管道代表了一种不同的游戏。一个关键的区别是，与传统的 if-then 计算相比，计算密集型 ML 应用和工具也有不同的硬件要求，传统的 if-then 计算在很大程度上涉及从 CPU 配置到 GPU 世界的转移。换句话说，GPU，而不是传统的工作站和服务器 CPU，通常需要处理神经网络内的数百万并行矩阵运算，该神经网络在许多方面模仿人脑的工作方式。

当投资一个雄心勃勃的 ML 开发项目所需的硬件时，所涉及的潜在成本似乎令人望而生畏，特别是对于没有内部资源来构建必要的内部基础设施的中小型企业。但是再一次，基于云的替代方案可以拯救我们，更具体地说，Kubernetes 平台可以经常——但不总是，如下所述——作为大规模 ML 软件部署和创建的完美渠道。

[AppDynamics](https://www.appdynamics.com/) 的技术宣传员[Ravi lach man](https://www.linkedin.com/in/ravilachhman)说:“由于 Kubernetes 可以被视为跨多个基础设施提供系统调度和资源管理的伟大均衡器，ML 工作负载将被吸引到 Kubernetes。

事实上，通过这种方式，在 GPU 驱动的 Kubernetes 平台上运行 ML 工作负载至少应该能够满足许多组织的需求。

“与专门构建的 GPU 机器的裸机实例相比，运行 Kubernetes 肯定会有开销，”Lachhman 说。“但是，重新构建或重新定位可用基础架构来处理请求的好处以及运行多种类型工作负载的能力是关键优势。Kubernetes 是通过利用容器的可移植性来抽象硬件的。”

然而，拉赫曼说，通常情况下，提高机器学习工作负载性能所需的可调性可能不是在容器-协调器级别。“与水平扩展或扇出型架构相比，组织可能会在单节点性能上受到打击，但让一个 orchestrator 管理部署到工作或请求的最佳基础架构肯定会改变游戏规则，”Lachhman 说。

但是，虽然 GPU 肯定是神经网络计算的理想选择，但 CPU 有时也可以完成某些大规模的 ML 任务。“这真的取决于你需要的是规模还是你想要恒定的时间，使用任何专门为用例设计的芯片，没有人可以否认这一点。但是你可以在使用容器方面获得一些灵活性，并且实际上在你需要扩大规模时加入可以随你扩大规模的技术，” [Harness](https://harness.io/) 的销售工程高级总监[尼克·杜尔金](https://www.linkedin.com/in/nick-durkin-0b19029)说。“因此，使用云提供商在底端添加更多基础架构，这最终会让您获得更多 pod 或更容易运行的任务，以便消除负载，如果您愿意，可以将负载放在所有一台物理机上。”

最终，在一天结束时，这是一个关于“你的工作量看起来像什么”的决定，Durkin 说。“工作负载应该决定你是打算直接使用 GPU，还是希望可扩展性潜在地使用多个 CPU，这在每个云提供商之间更容易、更容易获得，”Durkin 说。

例如，Durkin 说，GPU“在需要持续计算的标准用例中使用时非常出色”。这可能包括一个总是在处理的工作负载，例如一个无人驾驶汽车的决策树，它带有嵌入式传感器，带有神经网络引导的控制，可以告诉汽车何时需要左转或右转。

Durkin 说，特斯拉全电动汽车的自动驾驶能力是 GPU 适用于即时计算任务的一个很好的例子。“汽车需要实时监控，才能做出真正的决策。然而，也许你寻求的东西实际上是规模和灵活性，因为你的工作负载不是全天候运行的，”Durkin 说。“就像我们的工作负载一样，当人们进行部署时，我们的工作负载会出现高峰和低谷，但他们不会同时进行部署。因此，我们会有巨大的高峰和巨大的低谷——就像你看到的金融交易一样。”

[企业管理协会(EMA)](https://www.enterprisemanagement.com/) 的分析师 [Torsten Volk](https://www.linkedin.com/in/torstenvolk) 说，在实际层面上，特定的 ML/AI 算法需要 GPU，这些算法需要大规模并行处理来训练它们的推理模型。Volk 说，该算法不断调整权重和其他模型参数，以优化输入和输出之间的拟合，有时基于数百万或更多的例子。“搞清楚这一点包括数量惊人的迭代计算，这些计算根据其前任产生的错误率不断调整。无论是在 Kubernetes、vSphere 还是裸机上完成，都无关紧要，因为这些资源都允许您分配特定的工作负载，例如将模型培训分配给一个或多个 GPU。”

然而，Kubernetes 对于 ML 计算任务确实有其优势。“对于 Kubernetes，你需要做的就是安装 AMD 或 Nvidia GPU 的设备插件，”Volk 说。“然后，所有开发人员需要做的就是将他们的 GPU 要求添加到他们的容器清单中，然后他们就设置好了。CI/CD 管道保持与以前相同的方式运行。

除了 GPU 神经网络或 CPU 并行处理提供的优势之外，Kubernetes 在许多方面为 ML 应用程序提供了优势。例如，Kubernetes 自主分配资源。[MapR](https://www.linkedin.com/in/kingmesal/)企业架构总监 Jim Scott 表示:“在 Kubernetes 风格的环境中，无论组合中是否有其他 GPU，这种负担都会被消除，因为它变成了系统管理员标记环境的能力。”。“假设不同类型的机器上可以运行哪些类型的工作负载，然后跟踪系统的利用率……它的工作是，‘我如何采购并证实我们需要更多特定类型的硬件来满足特定的工作负载，这要容易得多？’"

事实上，Kubernetes 被视为 ML 应用程序和它们在 GPU 上的部署(或者，如上所述，在某些情况下，在 CPU 上)之间的一座巨大桥梁。对于 ML 开发和工作负载管理，“人们会发现自己在处理多种分布式系统技术，而那些拥有数据操作或集群操作技能的人可能很难找到，”拉赫曼说。“如果描述在 Kubernetes 内部运行的工作负载，Kubernetes 可以被视为一个拥有‘一个’资源管理器的出色均衡器。”

Harness 是新堆栈的赞助商。

通过 Pixabay 的特征图像。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>