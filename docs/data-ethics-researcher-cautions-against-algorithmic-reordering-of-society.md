# 数据伦理研究者警告不要对社会进行算法重组

> 原文：<https://thenewstack.io/data-ethics-researcher-cautions-against-algorithmic-reordering-of-society/>

当人们谈论人工智能和机器学习时，它经常以乐观的术语表达，如“预测分析”和“优化”。但是这些乐观的术语经常方便地掩盖了机器学习模型可能产生的[非常现实的道德问题](https://thenewstack.io/big-questions-for-the-ethical-use-of-ai/)，特别是当人类的偏见被无意地融入到机器学习系统中时，这些系统被用于[大规模监控](https://thenewstack.io/clearviews-controversial-facial-recognition-ai-automates-mass-surveillance/)或[识别犯罪嫌疑人](https://thenewstack.io/industry-facial-recognition-ai-moratoriums-dont-address-flaws-privacy-concerns/)，或决定谁[获得贷款或工作，或谁被从监狱释放](https://thenewstack.io/hidden-gender-racial-biases-algorithms-can-big-deal/)。

但正如旧金山大学[应用数据伦理中心](https://www.usfca.edu/data-institute/initiatives/center-applied-data-ethics) (CADE)最近的一篇论文所表明的那样，使用人类学的语言——特别是那些研究官僚机构、国家和权力的语言——来描述机器学习算法和人工智能对社会的更广泛的结构性影响可能是有用的。

这篇题为“[生活在他们的乌托邦中:为什么算法系统会产生荒谬的结果](https://ali-alkhatib.com/papers/chi/utopia/utopia.pdf)”的论文的前提断言，在没有批判性评估的情况下使用强大的机器学习工具，可能会对已经边缘化的人群造成进一步的伤害。

## 艾的“简略地图”

特别是，该论文的作者、社会计算研究员和经济发展委员会研究员阿里·阿尔卡提卜(Ali Alkhatib)借鉴了美国政治学家和人类学家詹姆斯·斯科特在他的著作《看起来像一个国家》(Seeing Like A State)中的工作，该书考察了官僚机构对世界的肤浅理解和真实人民的生活经历之间的脱节。这些由“官僚想象力”创造的不准确和“删节地图”导致了“自然和社会的行政秩序”——就像艾对世界的有限理解如何导致这些系统做出错误的决定，这些决定可能会不可挽回地改变生活。虽然这种机器学习模型可能在短期内提供便利，但当它们产生并强加基于这些“删节”的不均衡世界观时，它们可能会造成伤害。最糟糕的是，Alkhatib 指出，基于这些不完整地图的荒谬决定往往对那些最不能挑战它们的人产生最大的负面影响。

“机器学习系统构建世界的计算模型，然后将它们强加给我们，”Alkhatib 在为 2021 年 ACM 计算系统中人的因素会议(CHI)录制的[视频](https://youtu.be/ClGIosevT0Y)中解释道。“不只是用粗制滥造的地图在世界上导航，而是积极地改造它。当我们获取所有这些数据并告诉机器学习系统产生一个使所有这些数据合理化的模型时，这些系统从“使世界有意义”变成“使世界有意义”时，它们变得更加危险。”

> “机器学习系统从数据中推断出的规则背后没有潜在的意义或原因。例如，它们只是模式，没有任何关于为什么黑人入狱率比白人高得多的见解。

阿里·阿尔卡提卜

这篇论文强调了机器学习出现严重错误的几个例子，例如[研究](https://www.aclu.org/news/privacy-technology/how-is-face-recognition-surveillance-technology-racist/)显示执法部门使用的面部识别技术如何一致地[错误识别有色人种](https://thenewstack.io/industry-facial-recognition-ai-moratoriums-dont-address-flaws-privacy-concerns/)，因为用于训练这些系统的数据通常严重偏向白人男性个体作为“默认”对于其他任何不符合这一标准的人，机器学习模型将根据最初从未充分代表他们的数据，将他们标记为异常。此外，这样的人工智能系统往往没有考虑到历史压迫和“黑人的犯罪化”，以及 20 世纪 70 年代“ [carceral state](https://storymaps.arcgis.com/stories/7ab5f5c3fbca46c38f0b2496bcaa5ab0) ”的扩张等背景。

“机器学习系统从数据中推断出的规则背后没有潜在的意义或原因，”Alkhatib 说。“例如，它们只是模式，没有任何关于为什么黑人入狱率比白人高得多的见解。世界上没有任何数据集能充分传达白人至上、奴隶制或殖民主义。因此，充其量，这些系统产生了一个世界的复制品，历史的阴影投射在地面上——扭曲，扁平，总是缺乏深度，只有亲身经历才能带来这种深度。这些规则毫无意义，它们惩罚或奖励我们适应它们产生的模型——换句话说，它们构建的世界。”

Alkhatib 警告说，我们对算法越来越多的依赖——特别是科技巨头创造的有缺陷的[大规模人工智能模型——可能最终导致社会的计算重组，潜在地削弱公民社会，并为一种专制的](https://thenewstack.io/google-grapples-with-ethical-ai/)[算法治理](http://web.stanford.edu/class/sts175/NewFiles/Algocratic%20Governance.pdf)铺平道路，这可能取代今天的官僚制度。

“我们不应该允许这些系统拥有那种允许它们首先破坏一个人生活的力量，”Alkhatib 补充道。“我们应该尽可能地削弱算法系统的力量，并尽我们所能帮助人们在感到受到这些系统的虐待时逃离这些系统。”

为了解决这个日益严重的问题，Alkhatib 建议在这些算法系统中倡导人类审查机制，以便它们不会最终加剧预先存在的社会不平等。他还呼吁尽可能废除这项技术，并拒绝参与这项技术本身的开发。

“我们必须不断关注我们部署的系统和该系统所针对的人之间的权力动态，”他总结道。“如果我们不小心，它会试图迫使每个人都生活在它构建的算法想象中——生活在他们的乌托邦里。”

*详见[论文](https://ali-alkhatib.com/papers/chi/utopia/utopia.pdf)。*

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>