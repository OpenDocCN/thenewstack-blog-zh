# 数据质量问题及其对应用程序性能的影响

> 原文：<https://thenewstack.io/the-data-quality-problem-and-its-impact-on-application-performance/>

[](https://www.lightup.ai/)

 [马努·班萨尔

马努·班萨尔是 Lightup Data 的联合创始人兼首席执行官。他之前是 Uhana 的联合创始人，u HANA 是一个基于人工智能的移动运营商分析平台，于 2019 年被 VMware 收购。他在斯坦福大学获得了电子工程博士学位。](https://www.lightup.ai/) [](https://www.lightup.ai/)

您在任何地方都可以看到它…您的应用程序存在重大问题，但是您的 it 和应用程序性能监控工具没有发现任何问题。影响应用程序性能的大范围中断都表明您的数据管道存在越来越多的问题。

因此，数据质量再次成为热门话题，新工具开始出现。但是为什么会这样呢？为什么我们需要解决一个从数据本身就存在的问题，并且这个问题已经有了一堆现有的遗留工具？

两个字:大数据。

过去 10 年数据量的增长导致了数据质量工具需求的结构性转变，而传统工具已经无法满足这些需求。

原因如下。

## 传统工具:IDQ 和其他国家在大数据之前是如何建立的

遗留数据质量工具旨在服务于不同的数据世界。信息数据质量于 2001 年发布。 [Talend 发布于 2005 年](https://www.talend.com/blog/2016/10/18/looking-back-at-ten-years-of-growth/)。类似的工具出现在同一个窗口。但是“大数据”的世界是由三个很晚才出现的事件创造的。

### 事件 1:大数据和 ETL 的诞生

大数据的 ETL 始于 [Hadoop，它于 2006 年](https://en.wikipedia.org/wiki/Apache_Hadoop)发布，但在接下来的十年中并没有渗透到主流的财富 500 强企业细分市场。

### 事件 2:云的诞生

主流云的采用始于 2006 年公开发布的[亚马逊网络服务](https://aws.amazon.com/?utm_content=inline-mention)，但直到 2013 年 [Redshift 完全可用后才完全可用。](https://mediatemple.net/blog/cloud-hosting/brief-history-aws/)

### 事件 3:云数据仓库和 ELT 的诞生

云数据仓库(CDW)使得每个人都可以访问数据仓库。但是[雪花](https://www.snowflake.com/?utm_content=inline-mention)直到 2012 年才被[创立，接着是 2013 年](https://en.wikipedia.org/wiki/Snowflake_Inc.)的[数据砖块。](https://databricks.com/company/about-us)

简而言之:传统的数据质量工具早在大数据到来之前就被创造出来了。因此，它们从来就不是为解决大数据世界中的数据质量而设计的。虽然他们试图迎头赶上，但他们根本无法满足我们从 2010 年到 2020 年看到的数据量增长[44 倍带来的独特需求。](https://www.forbes.com/sites/gilpress/2020/01/06/6-predictions-about-data-in-2020-and-the-coming-decade/?sh=286fdce94fc3)

## 基本不匹配:遗留工具不满足的 12 个需求

大数据使得传统工具无法满足多种需求，包括:

1.  **数据量增加:**传统工具通常会在分析数据集之前加载完整的数据集。但是大数据湖和数据仓库有如此多的数据，以至于这种方法昂贵、缓慢或不可行。
2.  **增加了数据基数。**遗留工具和手动方法并不是为了处理数千个表而构建的，每个表都有数百或数千列。
3.  **数据随机性增加。**传统工具检查单个数据完整性违规。但是，当我们有如此多的数据量和多样性时，当一个小问题可以破坏许多数据元素时，这是站不住脚的，没有意义的。
4.  **源源不断的数据。**当数据每小时或每分钟到达且必须立即使用时，传统系统无法跟上步伐，并且必须近乎实时地检测问题以防止损坏。
5.  **加工管道。**传统工具使用传统的数据质量定义。但是现在我们已经有了自动化的 ELT 管道，它带有额外的故障模式，这些模式是该设置所特有的，并且不包括在传统的数据质量定义中。
6.  **更改数据形状。**传统工具是在每个组织都成为数据驱动型组织之前设计的。但现在，数据已经深入到产品和分析渠道中，数据模型也随着产品的发展而发展。
7.  **数据流拓扑/谱系。**传统工具是为了在单个主数据集上运行校验而构建的。但我们现在有十几个阶段和许多分支的数据管道，这给数据质量问题增加了一个空间维度。
8.  **时间系列问题。**传统工具旨在使用绝对标准测量单批数据的数据质量。但是现在数据以小批量的方式连续流动，给数据质量问题增加了时间维度。

我们也经历了文化的变化，这些变化创造了他们自己的新要求。

1.  **协作。**数据问题和解决方案现在触及组织中的每个人。
2.  **消费化。**现在，每个组织都在努力应对数据量和复杂性。
3.  **原料药。**平台现在需要开发友好、自动化和可互操作。
4.  **法律。**平台必须构建安全、合规和隐私架构。

这些新要求在过去十年中悄然形成，并突然开始推动围绕数据质量的新对话，这是一个核心原因。

## 引爆点:为什么现在是重新审视数据质量的时候了

在 ETL 丛林中经历了一段时间的大变动后，[一个新的稳定的 ELT 数据栈出现了](https://a16z.com/2020/10/15/the-emerging-architectures-for-modern-data-infrastructure/)。与传统数据库相比，新体系的核心—数据仓库—实施的数据完整性检查和约束更少。

与此同时，对数据质量的支持比以前少了，公司比以前更依赖他们的数据。现在每个公司都是数据驱动的，没有人能再承受坏数据，传统工具的缺陷真的开始造成伤害。

总之，已经变得非常明显的是，太多的东西已经发生了变化，传统工具在新的数据世界中不起作用，我们需要从头开始重新思考数据质量问题。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>