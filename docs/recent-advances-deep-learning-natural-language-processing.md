# 深度学习如何加速自然语言处理

> 原文：<https://thenewstack.io/recent-advances-deep-learning-natural-language-processing/>

[](https://insightengines.com/)

 [雅各布·帕金斯(Jacob Perkins)是 InsightEngines.com 的联合创始人兼首席技术官，创造了针对复杂数据提出简单自然语言问题的技术。Insight Engines 目前专注于网络安全客户，帮助实现机器数据访问的民主化，使任何人都可以执行有用的查询，同时帮助专家节省时间和资源。Jacob 是 nltk 项目的贡献者，这是一个针对 Python 的开源自然语言工具包，也是 nltk-trainer 的创造者，它可以轻松地使用 NLTK 训练您自己的自然语言机器学习模型。他是《用 NLTK 进行 Python 文本处理的 T4》一书的作者，也是 OReilly 的《坏数据手册》一书的撰稿人，有时他也在 streamhacker.com 撰写关于自然语言技术的文章。](https://insightengines.com/) [](https://insightengines.com/)

语音搜索、智能助手和聊天机器人正在成为现代技术的共同特征。用户和客户在与计算机交互时要求更好、更人性化的体验。根据 Tableau 的商业趋势报告，IDC 预测，到 2019 年，智能助理将成为企业员工的常用工具，而 Gartner 预测，到 2020 年，50%的分析查询将涉及某种形式的自然语言处理。聊天机器人、智能助手、自然语言查询和支持语音的应用程序都涉及各种形式的自然语言处理。为了充分实现这些新的用户体验，我们将需要构建最新的方法，其中一些我将在这里介绍。

先说基础:什么是自然语言处理？自然语言处理(NLP)是帮助机器理解人类语言的技术集合。例如，一项基本技术是记号化:将文本分解成“记号”，比如单词。给定序列中的单个单词，你可以开始对它们进行推理，并做类似情感分析的事情来确定一段文本是积极的还是消极的。但是，即使像单词识别这样简单的任务也可能相当棘手。这个词到底是一个词还是两个词(what +是，还是 what +曾是)？使用字符来表示多单词概念的语言怎么样，比如[kanji](https://en.wikipedia.org/wiki/Kanji)？

深度学习是一种使用神经网络的高级机器学习。它变得流行是因为这些技术成功地解决了一些问题，如图像分类(根据视觉内容标记图像)和语音识别(将声音转换成文本)。许多人认为，当深度学习技术应用于自然语言时，会很快达到类似的表现水平。但由于自然语言的所有特质，该领域没有像图像处理等其他领域一样，在深度学习方面取得突破性成功。然而，这种情况似乎正在改变。在过去的几年里，研究人员一直在将更新的深度学习方法应用于自然语言处理，我将分享一些最近的成功。

深度学习——通过最近对单词嵌入的改进，对注意力、移动支持的关注以及它在家庭中的出现——正在开始捕捉自然语言处理，就像它以前捕捉图像处理一样。在本文中，我将介绍一些最近基于深度学习的 NLP 研究成果，这些成果对该领域产生了影响。由于这些改进，我们将看到更简单和更自然的用户体验，更好的软件性能，以及更强大的家庭和移动应用程序。

## 单词嵌入

单词对于每个自然语言处理系统都是必不可少的。传统的 NLP 将单词视为字符串，但深度学习技术只能处理数字向量。[单词嵌入](http://ruder.io/word-embeddings-2017/)是作为一种[将单词转换成向量](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/)的方式被发明出来的，这使得新型的数学特征分析成为可能。但是单词的矢量表示只能和它被训练的文本一样好。

更常见的单词嵌入是在维基百科上训练的，但是维基百科的文本可能不代表你正在处理的任何文本。它通常被写成结构良好的事实陈述，这与 twitter 上的文本完全不同，这两者都不同于餐馆评论。因此，如果你用维基百科上训练的向量来分析不同风格的文本，那么这些向量可能会产生数学上的误导。来自[公共爬行](https://aws.amazon.com/public-datasets/common-crawl/)的文本为训练单词嵌入模型提供了更多样化的文本集。 [FastText](https://fasttext.cc/) 库提供了一些很棒的预训练英语单词向量，以及训练你自己的工具。如果你在处理英语以外的任何语言，训练你自己的向量是必不可少的。

字符级嵌入也显示了令人惊讶的结果。这种技术试图学习单个字符的向量，其中单词将被表示为单个字符向量的组合。为了学习如何预测评论中的下一个人物，研究人员发现了一个[情绪神经元](https://blog.openai.com/unsupervised-sentiment-neuron/)，他们可以控制它产生正面或负面的评论输出。使用情感神经元，他们能够打破情感树库之前的最高准确率。对于其他研究发现的副作用来说，这是一个令人印象深刻的结果。

## CNN，RNNs，注意

除了向量，深度学习还需要为各种任务训练[神经网络](https://en.wikipedia.org/wiki/Artificial_neural_network)。向量是输入和输出，中间是网络中连接在一起的节点层。节点表示输入数据上的函数，每个函数从上一层获取输入，并为下一层生成输出。网络的结构和节点如何连接在很大程度上决定了学习能力和性能。

一般来说，网络越深越复杂，训练的时间就越长。当使用大型数据集时，许多网络只能使用图形处理器(GPU)集群进行有效训练，因为 GPU 针对必要的浮点数学进行了优化。这使得一些类型的深度学习超出了任何大型公司或机构的能力范围，这些公司或机构能够负担得起大数据深度学习所需的昂贵的 GPU 集群。

标准神经网络是前馈网络，其中一层中的每个节点都前向连接到下一层中的每个节点。递归神经网络(RNN)是一种网络，其中每层中的节点也连接回前一层。这创造了一种记忆，对于从序列中学习非常有用，比如一个句子中的单词。

卷积神经网络(CNN)是一种前馈网络，但具有更多层，其中前向连接已被操纵或卷积，以实现某些属性。CNN 往往擅长提取位置不变的特征，这意味着它们不太关心序列排序。因此，与 rnn 相比，CNN 可以以更并行的方式训练，从而实现更快的训练和优化。

虽然 CNN 可能在原始速度方面胜出，但这两种类型的神经网络往往具有相当的性能特征。事实上，当涉及到面向序列的任务时，如词性标注，RNNs 有一点优势，在这种情况下，您试图识别句子中每个单词的词性(如“名词”或“动词”)。对于应用于 NLP 的 CNN 和 RNNs 的详细性能比较，参见:[CNN 和 RNN 用于自然语言处理的比较研究。](https://arxiv.org/abs/1702.01923)

最成功的 RNN 模型是 LSTM(长短期记忆)和 GRU(门控循环单位)。这些使用注意门，作为网络的一种短期记忆。然而，一份更新的研究报告暗示，你所需要的可能就是[的注意力。通过去除递归网络和卷积，并且只保留注意力机制，这些模型可以像 CNN 一样被并行训练，但是甚至更快，并且在一些序列学习任务(如机器翻译)上具有比 rnn 更好的性能。](https://arxiv.org/abs/1706.03762)

在保持可比性能的同时降低训练成本，意味着较小的公司和个人可以向他们的深度学习模型投入更多数据，并有可能更有效地与较大的公司和机构竞争。

## 软件 2.0

神经网络模型的一个很好的特性是核心算法和数学基本相同。一旦基础设施、模型定义和训练算法都设置好了，这些模型是非常可重用的。“[软件 2.0](https://medium.com/@karpathy/software-2-0-a64152b37c35) ”是一个应用程序或系统的重要组件可以被神经网络模型取代的想法。开发人员不再编写代码，而是:

1.  收集培训数据
2.  清理并标记数据
3.  训练模特
4.  整合模型

虽然最有趣的部分通常是第三和第四步，但是大部分工作发生在数据准备的第一和第二步。收集和管理好的、有用的、干净的数据可能是一项巨大的工作，这就是为什么像[语料库自举](https://www.slideshare.net/japerk/corpus-bootstrapping-with-nltk)这样的方法对于更快地获得好的数据很重要。从长远来看，制造更好的数据往往比设计更好的算法更容易。

过去几年已经证明，神经网络可以实现比许多替代方案更好的性能，有时甚至在机器学习传统上没有触及的领域。最近最有趣的进展之一是在[学习数据索引结构](https://arxiv.org/abs/1712.01208)。b 树索引是一种常用的数据结构，它提供了一种有效的查找数据的方法，假设树的结构良好。然而，这些新学习的索引在速度和内存使用方面明显优于传统的 B 树索引。如果能够集成到标准的开发实践中，这种低级数据结构的性能改进将会产生深远的影响。

随着研究的进展，必要的基础设施变得更便宜、更可用，深度学习模型可能会被用于软件栈越来越多的部分，包括移动应用。

## 移动机器学习

大多数深度学习需要昂贵的 GPU 集群和大量 RAM。这种级别的计算能力只有那些负担得起的人才能获得，通常是在云中。但消费者越来越多地使用移动设备，而世界上许多地方没有可靠和负担得起的全天候无线连接。让机器学习进入移动设备将使更多的开发者能够创建各种新的应用程序。

*   苹果的 [CoreML](https://developer.apple.com/machine-learning/) 框架在 iOS 设备上实现了许多 NLP 功能，比如语言识别和命名实体识别。
*   百度为移动深度学习开发了一个 CNN 库，可以在 iOS 和 Android 上运行。
*   高通为其移动处理器创建了一个[神经处理引擎](https://developer.qualcomm.com/software/snapdragon-neural-processing-engine)，使流行的深度学习框架能够在移动设备上运行。

随着移动设备变得越来越强大和无处不在，预计在不久的将来会有更多这样的事情发生。马克·安德森有句名言“[软件正在吞噬世界](https://a16z.com/2016/08/20/why-software-is-eating-the-world/)”，现在机器学习似乎正在吞噬软件。它不仅在我们的口袋里，也在我们的家里。

## 家庭中的深度学习

[Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa) 和其他语音助手在 2017 年成为主流，将 NLP 带入了数百万家庭。移动用户已经熟悉了 [Siri](https://en.wikipedia.org/wiki/Siri) 和[谷歌助手](https://en.wikipedia.org/wiki/Google_Assistant)，但是 Alexa 和 [Google Home](https://en.wikipedia.org/wiki/Google_Home) 的流行表明有多少人已经习惯了用声控对话系统进行对话。这些系统在多大程度上依赖深度学习有些未知，但可以肯定的是，他们的[对话系统的重要部分使用深度学习模型](https://arxiv.org/abs/1711.01731)来实现核心功能，如语音到文本、词性标注、自然语言生成和文本到语音。

随着研究的进展和这些公司从用户那里收集越来越多的数据，深度学习能力也将提高，“软件 2.0”的实施将变得无处不在。虽然一些大公司正在创建强大的数据护城河，但边缘总是有空间用于高度专业化、特定领域的自然语言应用，如网络安全、IT 运营和数据分析。

深度学习已经成为现代自然语言处理系统的核心组成部分。

然而，许多传统的自然语言处理技术仍然相当有效和有用，特别是在缺乏深度学习所需的海量训练数据的领域。我将在下一篇文章中介绍这些传统的统计技术。

通过 Pixabay 的特征图像。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>