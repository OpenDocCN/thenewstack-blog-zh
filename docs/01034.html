<html>
<head>
<title>DeepMind's AI Agents Teach Themselves to Play 3D Multiplayer Game</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepMind的人工智能代理自学玩3D多人游戏</h1>
<blockquote>原文：<a href="https://thenewstack.io/deepminds-ai-agents-teach-themselves-to-play-3d-multiplayer-game/#0001-01-01">https://thenewstack.io/deepminds-ai-agents-teach-themselves-to-play-3d-multiplayer-game/#0001-01-01</a></blockquote><div><div id="tns-post-body-content">


<p class="translated">我们已经看到了人工智能在战略双人游戏中的力量，如国际象棋和围棋——人工智能不仅能够在这些游戏中击败人类冠军，它还能够通过<a href="https://thenewstack.io/new-google-ai-achieves-alien-superhuman-mastery-chess-shogi-go-mere-hours/" target="_blank" class="local-link">在很短的时间内自学游戏来掌握它们。但是，尽管取得了这些巨大的成就，这些游戏并不一定是现实世界中需要合作和集体策略的最具代表性的情况。</a></p>
<p class="translated">这就是为什么让人工智能自主掌握更复杂的环境，如第一人称多人游戏中的环境，是下一个合乎逻辑的步骤。来自谷歌附属人工智能实验室<a href="https://deepmind.com/" class="ext-link" rel="external "> DeepMind </a>的最新工作揭示，使用无监督强化学习，让人工智能代理像人类一样自学如何合作和玩多人游戏确实是可能的。</p>
<p class="translated">正如DeepMind的团队在他们最近发表在<a href="https://science.sciencemag.org/content/364/6443/859" class="ext-link" rel="external "> <em> Science </em> </a>上的论文中概述的那样，这项研究涉及使用一个修改版的<a href="https://en.wikipedia.org/wiki/Quake_III_Arena" class="ext-link" rel="external "> Quake III Arena </a>，这是一个流行的第一人称多人游戏，让玩家在三维地图中导航。在这项研究中，由人类玩家和人工智能代理组成的团队以捕捉旗帜(CTF)模式玩游戏，合作以便在五分钟的时间内尽可能多地捕捉对手的旗帜。队伍从位于3D游戏地图两端的两个大本营开始，每场比赛的布局都在变化，以防止代理记住对他们有利的布局。</p>
<p class="translated">该团队让他们的人工代理使用卷积神经网络，以便像人类玩家一样看待这个世界:作为一个像素流，他们必须从中独立学习游戏的规则和目标，并以分散的方式操作。玩家配备了一种激光武器，可以通过射击来“标记”对手，使他们返回起点“重生”。如果他们被贴上标签时带着旗帜，他们必须放下旗帜返回基地。</p>
<div id="attachment_7773959" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-7773959" decoding="async" loading="lazy" class="wp-image-7773959 size-full" src="../Images/13910669155581d31849d327a26d7a7c.png" alt="" data-id="7773959" data-original-src="https://cdn.thenewstack.io/media/2019/06/26261d50-deepmind-ai-agents-quake-iii-arena-1.jpg"/><p id="caption-attachment-7773959" class="wp-caption-text translated">数以千计的捕捉国旗(CTF)游戏并行进行，以便使用室内或室外的一组不同的3D地图生成训练数据。</p></div>
<p class="translated"><iframe loading="lazy" title="Capture the Flag: from pixels to actions" src="https://www.youtube.com/embed/OjVxXyp7Bxw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p>
<p class="translated">代理人开始超越随机行动，而不是被给予一个预先确定的目标。随着时间的推移，研究人员发现，通过相互之间以及与人类玩家一起玩，代理人能够学习他们自己的“内部奖励信号”，例如获得游戏积分。这种自我学习的内部奖励信号允许他们产生自己的内部目标——例如抓住对手的旗帜以获得更多的分数。双层优化过程用于增强代理的内部奖励信号，以获得越来越多的分数，其中<a href="/reinforcement-learning-ready-real-world/" target="_blank">强化学习</a> (RL)方法用于进一步强化代理在选择行动时的策略，使其更接近实现其内部目标。</p>
<p class="translated">为了帮助他们评估和学习他们的进展，代理人配备了一个“带外部存储器的多时间尺度递归神经网络”，这有助于他们在游戏期间和游戏结束时跟踪分数。从本质上讲，代理人依赖于游戏的结果来继续进化和发展他们在未来会采取什么样的战术行动。</p>
<div id="attachment_7773958" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-7773958" decoding="async" loading="lazy" class="wp-image-7773958 size-full" src="../Images/f7be740b8463a2290c4e96567ee52dbd.png" alt="" data-id="7773958" data-original-src="https://cdn.thenewstack.io/media/2019/06/848fff25-deepmind-ai-agents-quake-iii-arena-2.jpg"/><p id="caption-attachment-7773958" class="wp-caption-text translated">人工智能主体行为分析。</p></div>
<h2 class="translated">紧急行为</h2>
<p class="translated">有趣的是，这种设置意味着每个代理都能够在没有任何监督的情况下制定自己的专门策略。例如，一些代理人能够在游戏中模仿人类的行为，例如跟随队友，监视敌人的基地，以及保护自己的基地免受对手的攻击。每1000场比赛后，系统通过比较每个代理人的政策来评估团队的表现如何；赢的不多的代理人会模仿表现更好的玩家。</p>
<p class="translated">“没有人告诉(人工智能)如何玩游戏——除非他们击败了对手，”论文主要作者Max Jaderberg在<a href="https://venturebeat.com/2019/05/30/deepminds-ai-can-defeat-human-players-in-quake-iii-arenas-capture-the-flag-mode/" class="ext-link" rel="external "> Venture Beat </a>上解释道。“使用这种方法的好处在于，你永远不知道代理学习时会出现什么样的行为。从研究的角度来看，真正令人兴奋的是算法方法的新颖性。我们训练[人工智能]的具体方式是如何扩大和实施一些经典进化思想的一个很好的例子。”</p>
<p class="translated">实验的结果是惊人的:即使代理人故意延迟反应时间，他们仍然在88%的时间里击败了中级玩家，在79%的时间里击败了高级玩家。虽然现在说这一步将把我们带到哪里还为时过早，但该团队的发现指出了利用强化学习帮助人工智能掌握新的未知情况的强大潜力，以及推进对混合系统的研究，这些系统让人类和机器合作，朝着同一目标努力。</p>
<p class="attribution translated">图片:DeepMind</p>


<div class="tns-logo-slug">
<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 68 31" version="1.1">
<title>Group</title>
<desc>Created with Sketch.</desc>
<g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
<g id="Group">
<path d="M24.002,29.619 L29.77,29.619 L29.77,15.808 C29.77,15.038 29.622,11.265 29.59,10.414 L29.77,10.414 C31.424,14.019 31.473,14.147 32.168,15.322 L39.65,29.618 L44.845,29.618 L44.845,0 L39.075,0 L39.075,11.064 C39.075,12.197 39.075,12.44 39.182,14.472 L39.325,17.468 L39.151,17.468 C39.034,17.267 38.596,16.173 38.467,15.929 C38.164,15.323 37.725,14.512 37.373,13.905 L30.031,0 L24,0 L24,29.619 L24.002,29.619 Z" id="Path-Copy" fill="#FF3287"/>
<path d="M56.948,0 C50.745,0 47.606,3.43 47.606,8.296 C47.606,14.114 51.036,15.404 55.518,17.132 C60.438,18.853 61.782,19.332 61.782,21.539 C61.782,24.225 58.969,24.867 57.401,24.867 C54.579,24.867 52.493,23.342 51.536,20.858 L47,24.185 C49.43,28.937 52.145,30.185 57.713,30.185 C59.364,30.185 62.059,29.74 63.727,28.694 C67.779,26.156 67.779,22.22 67.779,20.898 C67.779,18.129 66.531,16.207 66.178,15.726 C65.049,14.121 63.032,12.918 61.25,12.278 L57.084,10.914 C55.073,10.267 52.928,10.105 52.928,8.019 C52.928,7.707 53.008,5.528 56.288,5.319 L61.465,5.319 L61.465,0 C61.465,0 57.342,0 56.948,0 Z" id="Path-Copy-2" fill="#00AFF4"/>
<polygon id="Path" fill="#00AFF4" points="5.32907052e-15 1.77635684e-15 5.32907052e-15 5.319 7.572 5.319 7.572 29.564 14.132 29.564 14.132 5.319 21.544 5.319 21.544 1.77635684e-15"/>
</g>
</g>
</svg> </div>
</div>
</div>    
</body>
</html>