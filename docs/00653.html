<html>
<head>
<title>Mobile Machine Learning: AI Offload Engines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">移动机器学习:人工智能卸载引擎</h1>
<blockquote>原文：<a href="https://thenewstack.io/mobile-machine-learning-ai-offload-engines/#0001-01-01">https://thenewstack.io/mobile-machine-learning-ai-offload-engines/#0001-01-01</a></blockquote><div><div id="tns-post-body-content">


<p class="translated">英特尔正在推广这样一种观点，即其新的CPU是运行机器学习和其他人工智能工作负载的最具成本效益的硬件，因为你也可以将它们用于其他计算——使它们比主要用于高性能机器学习的GPU更灵活。但是从手机到云服务，我们看到了大量既不是CPU也不是GPU的人工智能专用硬件。</p>
<p class="translated">正如微软杰出的工程师道格·伯格(Doug Burger)在谈到Azure如何使用<a href="https://thenewstack.io/developers-fpgas-cloud/" class="local-link">FPGA</a>来加速机器学习时向新堆栈解释的那样，“现在有一场关于什么是正确的架构的大辩论？是软逻辑吗，是硬逻辑吗，数据类型和运算符是什么，应该在CPU上运行什么，应该在这套加速器上运行什么，系统架构是什么？这都是可以争取的。”</p>
<h2 class="translated">移动机器学习</h2>
<p class="translated">加速器正在手机中出现，以加速运行训练有素的数据模型，从人脸识别到提高照片质量。苹果iOS的核心ML框架包括许多预训练的模型，这些模型运行在A11的仿生神经引擎上，以及用于从TensorFlow和Apache MXNet等框架转换模型的工具。</p>
<p class="translated">ARM即将推出的<a href="https://thenewstack.io/arms-project-trillium-brings-dedicated-machine-learning-hardware-smart-devices/" class="local-link">机器学习和物体检测处理器</a>类似于苹果的仿生神经芯片。ARM不是通用处理器或重新设计的DSP芯片，而是专门设计来运行精度降低的整数和矩阵乘法累加器算法，这些算法构成了机器学习的大部分工作负载，特别是推理——运行经过训练的机器学习模型，而不是训练它。</p>
<p class="translated">开发人员可以在Linux上使用<a href="https://developer.arm.com/products/processors/machine-learning/arm-nn" class="ext-link" rel="external "> ARM NN SDK </a>来翻译Caffe模型(以及最终用于其他机器学习框架的模型)，以便在使用ARM硬件的设备上运行，如果有ARM ML处理器，代码将针对它进行优化。</p>
<p class="translated">高通的<a href="https://developer.qualcomm.com/software/qualcomm-neural-processing-SDK" class="ext-link" rel="external ">神经处理引擎SDK </a>涵盖了更多的框架，尽管在高通拥有包括ARM ML处理器的平台之前，它针对语音检测等工作负载的Hexagon DSP和用于对象检测和风格转换的Adreno GPU，使用Caffe、Caffe2、TensorFlow、ONNX、CNTK和MxNet训练的模型。</p>
<p class="translated">谷歌有一个定制的神经处理单元，它在<a href="https://www.blog.google/products/pixel/pixel-visual-core-image-processing-and-machine-learning-pixel-2/" class="ext-link" rel="external "> Pixel 2 </a>的相机应用中使用它来代替Hexagon DSP进行机器学习，华为也创建了自己的NPU(事实上其最新的手机拥有双NPU)。第三方应用程序也将能够利用这些npu，但在花费大量时间支持它们之前，有必要研究一下它们能带来多大的性能提升。</p>
<p class="translated">ARM的ML处理器不仅仅用于手机；它们将出现在特定的硬件设备中，如智能监控摄像头和平板电脑。VA的NeuPro AI处理器也是为物联网设备、可穿戴、无人机和AR/VR头戴设备设计的；他们有专门的矩阵乘法引擎和可编程矢量DSP(CEVA称之为矢量处理单元)，可以用新的神经网络算法进行更新。Imagination Technology的PowerVR Series2NX是一款神经网络加速器，专为智能手机、智能相机和自动驾驶汽车设计，针对处理卷积、激活、轮询和归一化等张量运算进行了优化。它与Android神经网络API (NNAPI)一起工作，开发人员可以使用<a href="https://www.imgtec.com/developers/neural-network-sdk/" class="ext-link" rel="external ">想象力神经网络SDK </a>来转换来自Caffe等框架的模型。</p>
<h2 class="translated">服务器和云</h2>
<p class="translated">Nvidia也一直在为其一些<a href="https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/" class="ext-link" rel="external ">显卡</a>添加张量处理器，以加速深度学习。在云中，谷歌有一个定制的ASIC张量处理单元，使用8位简化精度计算浮点和整数张量。</p>
<p class="translated">降低精度(或微软称之为窄精度)每次运算使用更少的晶体管，使其更有效地计算较低精度的数字，深度神经网络使用较低精度的权重来提高速度。Burger告诉我们:“当你进行推理时，你的效率会有一个超线性的飞跃。”</p>
<p class="translated">用于训练向16位整数和浮点数精度发展的神经网络，但用于在运行从32或16位浮点表示向8位整数发展的训练模型时进行推理，因为它提高了指令吞吐量，减少了内存消耗，而没有太多的精度损失。</p>
<p class="translated">正如英特尔人工智能产品事业部总经理Naveen Rao所解释的那样，“更低的精度允许芯片上有更多的并行性，因为更多的这些操作同时发生，功耗更低，并且在不降低分类率或语音翻译成文本的效果等算法性能的情况下做得更多。这并没有降低性能，同时还能以更低的功耗实现更高水平的并行性。”混合使用数字精度来实现性能和准确性也变得越来越普遍。</p>
<p class="translated">英特尔已经在高级向量扩展中添加了矩阵运算，以补充最新至强处理器中的x86指令集。今年出货的Cascade Lake Xeon增加了新的向量神经网络指令，<a href="https://simplecore.intel.com/nervana/wp-content/uploads/sites/53/2018/05/IntelAIDC18_Formenko_Theatre_052418_final.pdf" class="ext-link" rel="external "> DL Boost </a> (PDF)，可以用更少的指令处理深度学习推理中常见的8位整数卷积。2019铜湖至强CPU将拥有新版本的DL Boost，使用谷歌的bfloat16 16位浮点格式来训练工作负载，以及支持8位乘法和32位累加的向量神经网络指令(VNNI)集扩展。</p>
<p class="translated">英特尔即将推出的“神经处理器”Nervana NNP-L1000神经处理器将于2019年底上市，它拥有自己的新混合精度数字格式，称为Flexpoint，可以将标量计算转换为定点乘法和加法。</p>
<p class="translated">微软在Azure的每台服务器上都安装了FPGA来运行机器学习服务，现在客户可以在这些服务器上运行<a href="https://github.com/azure/aml-real-time-ai" class="ext-link" rel="external "> ResNet </a>，并与HPE和戴尔合作，提供配备FPGA的服务器来在边缘运行其认知服务。这些项目脑电波系统有一个<a href="https://www.slideshare.net/albertspijkers/brain-wave-hotchips2017" class="ext-link" rel="external ">专用指令集</a>，针对密集矩阵乘法、向量运算和张量运算(如卷积、非线性激活和嵌入)进行了优化，使用微软自己的窄精度浮点格式ms-fp8和ms-fp9，包装在float16接口中。</p>
<p class="translated">但是，与所有这些人工智能加速器和卸载硬件一样，开发人员不会在指令集的层面上工作；他们将继续使用TensorFlow、MXnet和CNTK之类的框架，这些框架将利用更有效的数字格式和指令。</p>
<h2 class="translated">不要忘记基准测试</h2>
<p class="translated">硬件供应商声称特定于人工智能的硬件可以快多少，但在市场的早期阶段，开发人员可能希望自己做更多的测试，以找出这些有多重要，以及哪些加速器对他们使用的模型最有好处。</p>
<p class="translated">当与一家OEM合作时，微软被要求移植其Azure认知服务API之一，以使用OEM设备上的NPU运行。完成这项工作后，团队在设备上对服务进行了基准测试。“我们真的发现，我们在CPU上做得和在他们的神经处理器上一样好，”微软首席小组项目经理Andy Hickl告诉我们。</p>
<p class="translated">最后，他们把这个问题留给了原始设备制造商来决定。“我们说，我们的算法可以在你提供的任何计算平台上运行，”Hickl说。这类似于Rao的说法，即推断“通用计算和GPU等特定计算之间的性能差距不是100倍，而是3倍”。</p>
<p class="translated">越来越多的工具将试图为可用的硬件优化机器学习模型和系统，但这些工具中有许多来自ARM和英特尔等硬件供应商，而不是跨平台的。英特尔的开源<a href="https://ai.intel.com/ngraph-a-new-open-source-compiler-for-deep-learning-systems/" class="ext-link" rel="external "> nGraph编译器</a>承诺优化模型TensorFlow、MXNet、neon和ONNX(增加了对CNTK、PyTorch和Caffe2的支持)，以在Xeon、Nervana、Movidius(专用视觉处理芯片)、Stratix FPGA和GPU上运行。如果你需要在移动设备上运行相同的机器学习模型，或者甚至在AMD CPU上运行，那就没有帮助了，所以我们期待看到它变得更加跨平台，以真正帮助开发人员，或者看到一个通用的处理管道，可以根据目标硬件使用nGraph或其他优化器。</p>
<p class="translated">这是一个快速发展的空间。随着如此广泛的人工智能卸载硬件和加速采用如此多的不同方法，开发人员可以预期必须采用更广泛的工具来利用它们，在投资时间支持特定硬件之前做一些基准测试是值得的。</p>
<p class="attribution translated">微软是新堆栈的赞助商。</p>


<div class="tns-logo-slug">
<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 68 31" version="1.1">
<title>Group</title>
<desc>Created with Sketch.</desc>
<g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
<g id="Group">
<path d="M24.002,29.619 L29.77,29.619 L29.77,15.808 C29.77,15.038 29.622,11.265 29.59,10.414 L29.77,10.414 C31.424,14.019 31.473,14.147 32.168,15.322 L39.65,29.618 L44.845,29.618 L44.845,0 L39.075,0 L39.075,11.064 C39.075,12.197 39.075,12.44 39.182,14.472 L39.325,17.468 L39.151,17.468 C39.034,17.267 38.596,16.173 38.467,15.929 C38.164,15.323 37.725,14.512 37.373,13.905 L30.031,0 L24,0 L24,29.619 L24.002,29.619 Z" id="Path-Copy" fill="#FF3287"/>
<path d="M56.948,0 C50.745,0 47.606,3.43 47.606,8.296 C47.606,14.114 51.036,15.404 55.518,17.132 C60.438,18.853 61.782,19.332 61.782,21.539 C61.782,24.225 58.969,24.867 57.401,24.867 C54.579,24.867 52.493,23.342 51.536,20.858 L47,24.185 C49.43,28.937 52.145,30.185 57.713,30.185 C59.364,30.185 62.059,29.74 63.727,28.694 C67.779,26.156 67.779,22.22 67.779,20.898 C67.779,18.129 66.531,16.207 66.178,15.726 C65.049,14.121 63.032,12.918 61.25,12.278 L57.084,10.914 C55.073,10.267 52.928,10.105 52.928,8.019 C52.928,7.707 53.008,5.528 56.288,5.319 L61.465,5.319 L61.465,0 C61.465,0 57.342,0 56.948,0 Z" id="Path-Copy-2" fill="#00AFF4"/>
<polygon id="Path" fill="#00AFF4" points="5.32907052e-15 1.77635684e-15 5.32907052e-15 5.319 7.572 5.319 7.572 29.564 14.132 29.564 14.132 5.319 21.544 5.319 21.544 1.77635684e-15"/>
</g>
</g>
</svg> </div>
</div>
</div>    
</body>
</html>