# ScyllaDB 年 IO 计划的经验教训

> 原文：<https://thenewstack.io/lessons-learned-from-6-years-of-io-scheduling-at-scylladb/>

[](https://github.com/xemul)

 [帕维尔(Xemul) Emelyanov

帕维尔是 ScyllaDB 的首席工程师。他是一名前 Linux 内核黑客，现在正在加速行缓存，调整 IO 调度程序，并帮助偿还组件相互依赖的技术债务。](https://github.com/xemul) [](https://github.com/xemul)

调度任何类型的请求总是服务于一个目的:获得对那些请求的优先级的控制。在无优先级系统中，不需要调度；只需将到达的内容放入队列中并等待它结束就足够了。

我是 [ScyllaDB](https://www.scylladb.com/) 的首席工程师，ScyllaDB 是一个开源的 NoSQL 数据库，用于需要高性能和低延迟的数据密集型应用程序。当在 ScyllaDB 中处理 IO 请求时，我们不能将这些请求丢到磁盘中，然后等待它们完成。在 ScyllaDB 中，不同类型的 IO 流有不同的优先级。

例如，从磁盘读取数据以响应用户查询很可能是一个“同步”操作，因为客户端确实在等待它发生，即使 CPU 很可能正忙于其他事情。在这种情况下，如果在查询请求到达时有一些 IO 正在运行，ScyllaDB 必须尽最大努力让查询请求及时得到服务，即使这意味着在其他事情之前将它提交到磁盘。

一般来说，我们可以说 OLTP 工作负载在上述意义上是同步的，因此对延迟敏感。这与 OLAP 工作负载有些相反，后者只要获得足够的吞吐量，就可以容忍更高的延迟。

## **海星的 IO 调度程序**

ScyllaDB 将其 IO 调度程序作为 [Seastar](http://seastar.io/) 框架库的一部分来实现。Seastar 是一个先进的开源 C++框架，用于现代硬件上的高性能服务器应用程序。当一个 IO 请求被提交时，它被传递到 Seastar IO 调度器，在那里它在几个队列中找到自己的位置，并最终被分派到磁盘中。

嗯，不完全是磁盘。ScyllaDB 对驻留在文件系统上的文件执行 IO。因此，当我们说“请求被发送到磁盘”时，我们实际上是指请求[被发送到 Linux 内核 AIO](https://www.scylladb.com/2017/10/05/io-access-methods-scylla/) *、*，然后它[进入文件系统](https://www.scylladb.com/2016/02/09/qualifying-filesystems/)，然后到 Linux 内核 IO 调度器*、*，最后才到达磁盘。

调度器的目标是确保根据分配的优先级及时处理请求。为了保持优先级之间的公平性，IO 调度程序维护一组请求队列。当请求到达时，根据请求优先级选择目标队列。稍后，当调度时，调度器使用类似虚拟运行时的算法来平衡队列(读优先级)，但是这个主题超出了本文的范围。

调度器的关键参数称为“延迟目标”在这段时间之后，磁盘保证已经处理了到目前为止提交的所有请求。新请求如果到达，可以立即被分派，并且在“延迟目标”时间过去之后完成。

为了做到这一点，调度程序会尝试预测有多少数据可以放入磁盘，以便在延迟目标内完成所有数据。请注意，满足延迟目标并不意味着请求在分派后“不”在某处排队。事实上，现代磁盘的速度如此之快，以至于调度程序调度的请求超过了磁盘在不排队的情况下所能处理的数量。尽管如此，总的执行时间(包括花费在内部队列中的时间)足够短，不会违反延迟目标。

上述预测基于连接到调度程序大脑中的磁盘模型，该模型使用一组可测量的磁盘特征。对磁盘建模是困难的，不可能有 100%精确的模型，因为正如我们所了解的，磁盘总是非常令人惊讶。

## **IO 的基础**

选择磁盘时，通常要考虑它的四个参数:读/写 IOPS 和读/写吞吐量(以 Gbps 为单位)。将这些数字相互比较是宣称一个磁盘优于另一个磁盘的流行方式，在大多数情况下，真实的磁盘行为符合基于这些数字的用户预期。

在这里应用 [Little 定律](https://en.wikipedia.org/wiki/Little%27s_law)可以清楚地看到，延迟目标可以在某个并发级别上实现——放入磁盘的请求总数——而调度程序完成其工作所需的全部工作就是在某个磁盘内并发级别上停止调度。

实际上，即使只调度一个请求，也可能会违反延迟目标。这样，调度器应该在提交这个请求之前停止调度，这反过来意味着不应该发生 IO。

幸运的是，这只能在老式的旋转磁盘上观察到，这可能会给每个请求带来毫秒级的开销。ScyllaDB 也可以使用这些磁盘，但是用户的延迟预期必须大大放宽。

## **几乎不分享任何东西**

让我们回到“在‘延迟目标’时间内尽可能多地向磁盘发送请求”的概念，并在游戏中加入一些数字。延迟目标是毫秒量级的值；默认目标是 0.5 毫秒。在此期间，每秒 1 GB 的普通磁盘能够处理 500 kB。给定一个有 20 个碎片的系统，每个碎片在一个时钟周期内有 25 kB 可以调度。这个值实际上相当低。

部分原因是 ScyllaDB 需要太多的请求才能工作，因此开销很大。主要原因是磁盘通常需要大得多的请求才能在其最大带宽下工作。例如，AWS 实例使用的 NVMe 磁盘可能需要 64 k 个请求才能达到峰值带宽。使用 25 k 请求会给你 80%的带宽，即使利用高并发性。

这个简单的数学表明，当涉及到磁盘时，Seastar 的“[无共享](https://www.scylladb.com/product/technology/shard-per-core-architecture/)”方法不能很好地工作，所以碎片“必须”在分派请求时进行通信。在过去，ScyllaDB 带有 IO 协调器碎片的概念；[后来这被改成了](https://www.scylladb.com/2021/04/06/scyllas-new-io-scheduler/)IO 组。

## **为什么选择 Iotune？**

当决定是否调度一个请求时，调度器总是问自己:“如果我提交下一个请求，它是否会使磁盘中的并发性足够高，以至于它不符合延迟目标契约？”反过来，回答这个问题取决于调度程序大脑中的磁盘模型。该模型可以通过两种方式进行评估:岸上评估，即可以在系统用于生产之前预先评估磁盘，或者在运行中评估，或者这两种方式的结合。

在飞行中做这件事很有挑战性。令人惊讶的是，磁盘不是确定性的，它的性能特征在工作时会发生变化。即使像带宽这样简单的数字也没有一个确定的固定值，即使我们将统计误差应用到我们的测量中。

同一个磁盘可以显示不同的读取速度，这取决于它是否处于所谓的突发模式或负载是否持续；如果是读或写(或混合)IO；如果它受到磁盘使用历史记录、服务器机房的空气温度和大量其他因素的严重影响。试图估计这个模型的运行时间是非常困难的。

与此相反，ScyllaDB 借助一个名为 iotune 的工具提前测量磁盘性能。该工具测量磁盘的一系列参数，并将结果保存在一个文件中，我们称之为“IO 属性”

然后，Seastar 在启动时加载这些数字，并将其输入 IO 调度程序配置。调度器手头有四维“容量”,并被允许在其中的一个子区域内操作。该区域由每个轴上的四个限制来定义，调度器必须确保在提交请求时不会在数学意义上离开该区域。但是真的，这四点还不够。调度程序不仅需要对提到的“安全区域”进行更精细的配置，还必须小心处理请求的长度。

## **纯工作负载**

首先，让我们来看看，如果使用我们所谓的“纯”负载，即“仅”读取或“仅”写入，磁盘会有什么表现。如果将最大磁盘带宽除以其最大 IOPS 速率，得到的数字将是某个请求大小。如果磁盘负载过重，请求小于该大小，磁盘将被 IOPS 饱和，其带宽将得不到充分利用。如果使用大于该阈值的请求，磁盘将因带宽而饱和，其 IOPS 容量将得不到充分利用。

但是，所有“大”请求都足以使用磁盘的全部带宽吗？我们的实验表明，当使用 64 k 请求和 512 k 请求时，一些磁盘显示出明显不同的带宽值(当然，请求越大，带宽越大)。

因此，要从磁盘获得最大带宽，您需要使用较大的请求，反之亦然—如果使用较小的请求，您将永远无法从磁盘获得峰值带宽，即使仍未达到 IOPS 限制。幸运的是，请求大小有一个上限，超过这个上限，吞吐量将不再增长。我们称这个极限为“饱和长度”

这一观察有两个结果。首先，可以通过 iotune 测量饱和长度，如果是这样，调度程序稍后会将它作为 IO 大小进行通告，如果子系统想要从磁盘获得最大吞吐量，它们应该使用这个 IO 大小。表管理代码使用该长度的缓冲区来读写表。

然而，这个公布的请求大小不应该太大。它必须小于磁盘仍能满足延迟目标的最大值。这两个要求——大到足以使带宽饱和，小到足以满足延迟目标——可能“不同步”，即后者可能低于前者。我们见过这样的圆盘。为此，您需要在延迟和吞吐量之间做出选择。在所有其他情况下，如果有其他有利的情况，你将能够享受这两者。

第二个结果是，如果调度程序看到中等大小的请求，它必须调度比请求更大时更少的数据。这是因为实际上磁盘带宽将低于峰值，延迟目标要求将无法满足。Seastar 在阶梯函数的帮助下对这种行为进行了建模，这似乎既是很好的近似，又不需要维护太多的配置参数。

[https://www.youtube.com/embed/B7qzLZrvxzw?feature=oembed](https://www.youtube.com/embed/B7qzLZrvxzw?feature=oembed)

视频

## **混合工作负载**

复杂性的下一个层面是我们所说的“混合工作负载”这时，磁盘必须同时执行读取和写入。在这种情况下，如果我们计算输入之间的线性比率，总吞吐量和 IOPS 都将不同于预期。这种差异是双重的。

首先，读取流和写入流以不同的方式变小。让我们以一个每秒可以运行 1 GB 读取或每秒 500 MB 写入的磁盘为例。磁盘写得比读得慢并不奇怪。现在，让我们尝试用两个相等的无限制读写流来饱和磁盘。我们会看到什么样的输出带宽？线性比率使我们认为每个流将获得其一半，读取将显示 500 MB/s，写入将获得 250 MB/s。实际上，不同磁盘型号之间的结果会有所不同，常见的情况似乎是写入请求的延迟不会有太大变化，而读取请求的延迟会增加几倍。例如，我们可以看到两个流的速率相同，都是 200 MB/s，其中 80%用于写入，只有 40%用于读取。或者，在最坏(也可能是最好)的情况下，写操作可以继续以峰值带宽工作，而读操作只能满足于剩余的百分比。

其次，这种抑制很大程度上取决于所使用的请求大小。例如，当饱和的读取流受到一次一个写入请求的干扰时，小型写入的读取吞吐量可能会降低两倍，大型写入的读取吞吐量可能会降低 10 倍。这种观察对调度程序通告给系统的最大 IO 长度施加了另一个限制。配置后，调度程序会额外限制最大写入请求长度，以便有机会调度混合工作负载，同时仍保持在延迟目标内。

## **不稳定的工作负载**

深入挖掘，你会发现实际上有两倍多的磁盘速度数字。事实上，每个速度特性都可以用两种模式来衡量:突发模式或持续模式。EBS 磁盘甚至[明确记录了](https://aws.amazon.com/ru/blogs/database/understanding-burst-vs-baseline-performance-with-amazon-rds-and-gp2/)以这种方式工作。这种意外通常是磁盘基准测试测量的第一件事 ads 中记录的磁盘吞吐量通常是“突发的”(如果在 100%优化的情况下测量，磁盘芯片将显示的峰值带宽)。但是，一旦工作负载持续时间超过几秒钟或变得“随机”，磁盘内部就会开始后台活动，导致速度下降。因此，在对磁盘进行基准测试时，人们常说必须清楚地区分短期和持续工作负载，并指出测试中使用的是哪一种工作负载。

顺便说一下，iotune 测量持续参数，主要是因为 ScyllaDB 不希望利用突发模式，部分是因为很难确定这个“突发”。

## **在我的 P99CONF 演讲中了解更多:“关于持久存储，我们需要忘记什么”**

如果你已经做到了这一步，并且你仍然渴望更多，我邀请你观看我最近在 [P99CONF](https://www.p99conf.io/) 上发表的演讲，这是一个新的供应商中立会议，汇集了世界上最顶尖的开发人员，对高性能、低延迟设计策略进行技术深度探讨。

我分享了我们在 ScyllaDB 对现代硬件进行的一些关键性能测量，以及我们对数据库和系统软件设计的看法。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>