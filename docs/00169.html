<html>
<head>
<title>Driverless Cars Recognize Peds Better With Deep Learning Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无人驾驶汽车通过深度学习算法更好地识别ped</h1>
<blockquote>原文：<a href="https://thenewstack.io/deep-learning-algorithm-helps-driverless-cars-recognize-pedestrians-better/#0001-01-01">https://thenewstack.io/deep-learning-algorithm-helps-driverless-cars-recognize-pedestrians-better/#0001-01-01</a></blockquote><div><div id="tns-post-body-content">


<p class="translated">自动驾驶汽车使用各种技术，如<a href="https://en.wikipedia.org/wiki/Radar" class="ext-link" rel="external ">雷达</a>、<a href="https://en.wikipedia.org/wiki/Lidar" class="ext-link" rel="external ">激光雷达</a>、<a href="https://en.wikipedia.org/wiki/Odometry" class="ext-link" rel="external ">里程计</a>和计算机视觉来检测道路上的物体和人，促使它相应地调整轨迹。但是这些工具可能会提高无人驾驶汽车的成本，并且在视觉上区分一些物体和行人方面仍然不如人脑有效。</p>
<p class="translated">为了解决这个问题，来自加州大学圣地亚哥分校的电气工程师在最近的一次实验中使用了强大的机器学习技术，该实验将所谓的<a href="https://thenewstack.io/deep-learning-neural-networks-google-deep-dream/" class="local-link">深度学习算法</a>整合到一个行人检测系统中，该系统仅使用视觉数据，几乎实时执行。</p>
<p class="translated">“我们的目标是建立计算机视觉系统，帮助计算机更好地理解他们周围的世界，”加州大学圣地亚哥分校的电气工程教授努诺·赛洛斯说，他领导了这项研究，并在加州大学圣地亚哥分校雅各布工程学院发布的一篇文章中引用。</p>
<p class="translated">这些发现在智利圣地亚哥举行的国际计算机视觉会议上公布，是对目前使用的<a href="https://en.wikipedia.org/wiki/Pedestrian_detection" class="ext-link" rel="external ">行人检测</a>方法的改进，后者使用了一种叫做级联检测的方法。在<a href="https://en.wikipedia.org/wiki/Computer_vision" class="ext-link" rel="external ">计算机视觉</a>中，这种传统形式的分类架构采用多阶段方法，首先将图像分解成更小的图像窗口。然后，使用形状和颜色等标记，根据这些子图像是否包含行人来处理这些子图像。</p>
<p class="translated">虽然这种方法相对较快，但唯一的问题是，这种方法不太擅长区分不同大小和相对于摄像机不同位置的行人图像，因为数百万个窗口必须以每秒5到30帧的不同速度进行处理。观看行人检测如何工作的视频:</p>
<p class="translated"><iframe loading="lazy" title="Pedestrian Detection" src="https://www.youtube.com/embed/Ajb5FRuKtTg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p>
<p class="translated">另一个使事情复杂化的因素是级联检测系统在级联过程的最后阶段遇到类似人类的物体。像树木这样的东西，可能具有行人的形状和轮廓，很难正确区分，因为级联不能很好地执行复杂的分类。这是因为这种方法依赖于所谓的“弱学习器”，或简单的分类器，在每个阶段处理窗口。显然，在级联的所有阶段中使用的弱学习器是相同的，这意味着它只是在后面更复杂的行人检测阶段中不那么有效。</p>
<h2 class="translated">深度学习</h2>
<p class="translated">因此，在这个级联的最后阶段，深度学习开始发挥作用。使用深度学习算法，通过向机器学习模型输入数以千计的例子，可以训练它识别复杂的模式。在开发他们新的级联架构时，除了传统的分类方法之外，该架构还实现了深度学习模型，研究团队能够让他们的系统以每秒2到4帧图像的近实时速率进行处理，同时将错误率减半-与可比设计相比，这是一个显著的改进。</p>
<p class="translated">但是深度学习算法太慢，不能用于级联检测的早期阶段，因此非常适合该过程的后期更复杂的部分。因此，该团队的新方法结合了不同类型的分类器。简单的分类器或“弱学习者”被用在该过程的早期阶段，而复杂的深度学习分类器被用在随后的阶段，从而产生更有效的解决方案。</p>
<p class="translated">“对于具有如此不同复杂性的级的级联，以前没有算法能够优化检测精度和速度之间的权衡，”赛洛斯说。“事实上，这些是第一批包含深度学习阶段的级联。我们通过这种新算法获得的结果在实时、准确的行人检测方面明显更好。”</p>
<p class="translated"><img decoding="async" loading="lazy" class="aligncenter size-full wp-image-1085853" src="../Images/3bcd7a34249274b77c8a56c70bb08d98.png" alt="uc-diego-deep-learning-cars-algorithm" data-id="1085853" data-original-src="https://thenewstack.io/wp-content/uploads/2016/02/uc-diego-deep-learning-cars-algorithm.jpg"/></p>
<p class="translated">该系统将是无人驾驶汽车的福音，降低成本，同时提高安全性和响应时间。除了改进机器人技术、图像和视频搜索中使用的计算机视觉系统之外，现有汽车的现有防撞系统也将受益匪浅。</p>
<p class="translated">当然，仍有进一步发展的空间:目前，该系统仅适用于特定的检测功能，如对行人进行分类。但科学家们希望将其扩展到同时检测各种类型的物体，尽管这意味着提高未来智能汽车的计算能力。</p>
<p class="translated">“解决这个问题的一种方法是训练，例如，五个不同的探测器来识别五个不同的物体，”赛洛斯说。“但我们只想训练一个探测器来做这件事。开发这种算法是下一个挑战。”</p>
<p class="translated">在加州大学圣地亚哥分校阅读更多信息。</p>


<div class="tns-logo-slug">
<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 68 31" version="1.1">
<title>Group</title>
<desc>Created with Sketch.</desc>
<g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
<g id="Group">
<path d="M24.002,29.619 L29.77,29.619 L29.77,15.808 C29.77,15.038 29.622,11.265 29.59,10.414 L29.77,10.414 C31.424,14.019 31.473,14.147 32.168,15.322 L39.65,29.618 L44.845,29.618 L44.845,0 L39.075,0 L39.075,11.064 C39.075,12.197 39.075,12.44 39.182,14.472 L39.325,17.468 L39.151,17.468 C39.034,17.267 38.596,16.173 38.467,15.929 C38.164,15.323 37.725,14.512 37.373,13.905 L30.031,0 L24,0 L24,29.619 L24.002,29.619 Z" id="Path-Copy" fill="#FF3287"/>
<path d="M56.948,0 C50.745,0 47.606,3.43 47.606,8.296 C47.606,14.114 51.036,15.404 55.518,17.132 C60.438,18.853 61.782,19.332 61.782,21.539 C61.782,24.225 58.969,24.867 57.401,24.867 C54.579,24.867 52.493,23.342 51.536,20.858 L47,24.185 C49.43,28.937 52.145,30.185 57.713,30.185 C59.364,30.185 62.059,29.74 63.727,28.694 C67.779,26.156 67.779,22.22 67.779,20.898 C67.779,18.129 66.531,16.207 66.178,15.726 C65.049,14.121 63.032,12.918 61.25,12.278 L57.084,10.914 C55.073,10.267 52.928,10.105 52.928,8.019 C52.928,7.707 53.008,5.528 56.288,5.319 L61.465,5.319 L61.465,0 C61.465,0 57.342,0 56.948,0 Z" id="Path-Copy-2" fill="#00AFF4"/>
<polygon id="Path" fill="#00AFF4" points="5.32907052e-15 1.77635684e-15 5.32907052e-15 5.319 7.572 5.319 7.572 29.564 14.132 29.564 14.132 5.319 21.544 5.319 21.544 1.77635684e-15"/>
</g>
</g>
</svg> </div>
</div>
</div>    
</body>
</html>