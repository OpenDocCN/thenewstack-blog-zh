# 停止谈论负责任的人工智能，并付诸实践

> 原文：<https://thenewstack.io/stop-talking-about-responsible-ai-and-put-it-into-practice/>

[](https://www.linkedin.com/in/hasuni/)

 [李然哈森

李然是 Aporia 的联合创始人兼首席执行官，Aporia 是一个全栈 ML 可观测性平台，被世界 500 强公司和数据科学团队用来确保负责任的 AI。在创建 Aporia 之前，Liran 是微软收购的 Adallom 的 ML 架构师和 Vertex Ventures 的投资者。Liran 在亲眼目睹了没有护栏的 AI 的效果后，创建了 Aporia。2022 年，福布斯将 Aporia 列入了下一个十亿美元创业公司名单。](https://www.linkedin.com/in/hasuni/) [](https://www.linkedin.com/in/hasuni/)

在过去的几年里，许多组织都在竞相构建、推出和扩展他们的人工智能(AI)和[机器学习](https://thenewstack.io/category/machine-learning/) (ML)模型。然而，护栏和治理的缺乏意味着黑箱 ML 模型被释放到现实世界中，在那里它们对企业和公众产生重大的、通常是意想不到的影响。这就是负责任的人工智能的概念和实践发挥作用的地方。

负责任的人工智能(RAI)结合了社会采用人工智能并享受其改变游戏规则的好处同时最大限度地减少意外后果所必须具备的方法和工具。在过去的一年里，负责任的人工智能已经被人工智能社区详细讨论过，许多大型组织——如 [、微软](https://mailtrack.io/trace/link/481a2719a4290909f757393aaa4e9aae50175760?url=https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fai%2Fresponsible-ai%3Factivetab%3Dpivot1%253aprimaryr6&userId=7791813&signature=7ba09e6b038525e6) 和 [、谷歌](https://mailtrack.io/trace/link/2d8d087e4b25d7a70791d3f896fbfcef4002fbf8?url=https%3A%2F%2Fai.google%2Fresponsibilities%2Fresponsible-ai-practices%2F&userId=7791813&signature=c3c66afff2798d05)——创建了他们自己的 RAI 指南和 [政府提议立法](https://mailtrack.io/trace/link/c179724df7a0f189c4ee16c3c9a556735283045d?url=https%3A%2F%2Fartificialintelligenceact.eu%2F&userId=7791813&signature=431fc8422fe1b4f9) 。虽然进行这种对话很重要，但容易迷失的是将这些概念付诸实践。

通过优先考虑负责任的人工智能，各种ML 问题，包括模型性能退化、预测漂移和数据漂移等，可以在影响业务或用户之前避免或解决。实现 RAI 概念甚至可以帮助防止无意中有偏见的模型歧视用户(例如， [史蒂夫·沃兹尼亚克和他的妻子获得了显著不同的信贷限额，尽管他们共享相同的资产](https://mailtrack.io/trace/link/64826b803f5533688c5dd6fbf4907fb4366b4322?url=https%3A%2F%2Fwww.reuters.com%2Farticle%2Fus-goldman-sachs-apple-idUSKBN1XL038&userId=7791813&signature=53212019599393a7) )。

正是这些问题将我们带回了这样一个问题:如何实现 RAI？

让我们深入探讨四个快速制胜的方法，它们将帮助你将负责任的人工智能付诸实践。

## 模型可见性是关键

拥有一个单一的仪表板，一个组织中的每个人都可以看到和跟踪模型健康状况、分析它并从功能和业务角度探索它的集中位置，是实现负责任的人工智能的核心。

组织中的每个人都需要承担 RAI 的所有权，因此拥有一个单一的位置，利益相关者和 ML 团队都可以看到哪些模型正在生产中运行——模型正在做出哪些预测，模型正在为其用户做出哪些决策，以及模型与其训练相比实际表现如何——这是至关重要的。

一旦您的模型的可见性机制到位，并且您可以看到和调查它的真实(而不是预期的)性能，您就准备好避免您的模型和业务可能会陷入的许多陷阱。最值得注意的是，模型可能会产生意想不到的偏差——例如，阿拉巴马州 25-35 岁的人是否会获得与加州同龄人相同的信用评分，这是预期的结果吗？公平是人工智能成功的主要驱动力之一，因此拥有正确的可见性系统必须是重中之重，以确保避免模型退化，并控制非预期的偏差。

**在您的团队中实现可见性:**

1. **在你的数据湖中存储**来自生产的推理数据，包括模型输入和输出。 2。 **跟踪**生产中的所有模型，包括来自不同数据切片的模型工件、训练数据和指标。 3。 **创建**所有模型及其状态的集中概述——并确保所有团队成员和利益相关者可以访问和理解它们。 

## 积极参与 ML 活动

当 ML 事件发生时变得积极主动是非常重要的。

由于人工智能中无穷无尽的独特的边缘情况，以及由于其黑盒性质，不可能事先精确地定义所有意想不到的行为。因此，组织必须定义他们对模型的了解，并对超出已知标准的任何预测设置警报。

一旦部署到生产中，您的 ML 模型会受到它所输入的数据的很大影响。从您的数据管道到通过 web 应用程序、移动应用程序、API 等与现实世界的接口。—有大量复杂的活动部件会影响它。因此，当像新冠肺炎疫情这样的全球性事件发生时，它会改变人们的反应方式，因此，数据变得扭曲，这反过来会影响模型的预测，并最终影响其性能。

建立一个预警系统是有意义的，这样当改变游戏规则的事件改变了大联盟的生命周期，你会第一个知道。最核心的是，每个数据科学家都想知道他们的模型一旦投入生产，是否按照他们的预期运行。否则，企业可能会失去信誉，这可能会导致财务损失，从一开始就否定了模型的目的。通过利用警报系统，您可以在模型性能下降影响到客户之前了解到这一情况，从而为您提供做出响应所需的宝贵时间以及最先了解的优势。

再举一个例子，想象一下，你的一个客户在你还没意识到之前，就开始在推特上抱怨你的模型的性能出现了明显的偏差。一个歧视性的模型不仅带有有问题的社会公正和公平成分，而且也不符合商业目标。拥有一个持续跟踪数据、预测以及业务和数据科学 KPI 的[自动化 ML 监控系统](https://www.aporia.com/)可确保您始终了解您的 ML 模型如何影响您的业务。该系统必须无缝集成到组织的主要沟通渠道(例如，Slack、团队、电子邮件等)中。)以便团队成员能够在不中断工作流程的情况下对事件做出有效的响应和反应。

**实施先发制人:**

1。**定义**您的模型的预期行为:预期预测分布、输入数据分布、数据统计、数据科学 KPI 和业务 KPI。
2。 **设置**对任何超出定义标准的偏差发出警报。 3。 **整合**您组织的主要沟通渠道，让所有用户都能获得提醒，并鼓励讨论。 

## 生产性能预览

通过这一步，您已经建立了不同模型的集中概览，并可以看到它们的预测，您可以清楚地了解它们的性能，以及在出现问题时的自动警报系统。负责任的人工智能难题的下一部分是建立一个节奏，与相关的利益相关者召开会议，以审查模型的结果和性能。

您应该争取每周或每两周与以下人员会面:

*   产品经理 
*   内部洗钱客户，如果有的话 
*   数据科学家 
*   工程队 

这些会议将提出关于目标、绩效和其他关键指标的问题，并且它们还将有助于定义会议产生的行动项目的所有权。为了确保取得进展，下一次会议应该从回顾上一次会议的行动项目开始，确保所有的方框都已勾选，任务都已完成。随着时间的推移，你会看到明显和一致的改善。

**实施生产绩效:**

1. **设置**每周或每两周与相关利益相关者召开一次会议，以评估模型的生产绩效。
2。为那次会议写一份议程，这样会更有效。确保执行以下操作:

1.回顾上一次会议的行动项目。
2。 **对照预期 KPI 审查实际绩效指标。
3。
**

 **3. **回顾**上次会议的行动项目、当前绩效与业务目标，列出未决问题，并为其设置行动项目和负责人。 

## 事故响应工作流程

最后，定义处理不同事件(漂移、性能下降和潜在偏差)的工作流程非常重要。当事故发生时，混乱倾向于控制局面，压力会延迟或扰乱反应。谁负责？是数据科学家吗？是数据工程师还是软件工程师？我们对顾客做了什么？为事件响应定义明确的所有权和角色，确保每个人都知道该做什么，并避免可预防的灾难性失败。

在我们深入细节之前，让我们先弄清楚响应和补救之间的区别。这两个步骤对于事件反应和修复有问题的模型同样重要。响应是问题出现后立即采取的行动。这意味着这些是你用来灭火的措施。它们可能包括短期修复和补丁，这些修复和补丁将促进业务连续性，允许团队更彻底地调查事件的根本原因，并最终决定补救的最佳途径。补救是一项长期解决方案，需要在调查结束后实施，以确保同一问题不会引发任何新的火灾。

既然每个人都知道自己的角色，那么创建一个决策树是很有用的，这样每个人都知道应该如何处理不同的事件。预定义的决策树应该确定正确的响应工作流，这样，如果警报在凌晨两点响起，从待命工程师到数据科学家，每个人都知道根据事件的特征应该做什么。

这就导致了下一步——建立回退机制，这可能意味着恢复到先前的模型版本，按照您的预期运行。采取 HITL 路线并让一个人同时回顾事件是一种选择，以及激活在部署 ML 模型之前使用的非基于 ML 的算法或试探法。有了这些后备机制，您应该为任何场景做好准备，这样就不需要考虑停止生产。

最后，在事件发出警报并得到处理后，优先进行调查。你想总结事件，从中吸取教训，并向相关利益方解释。这就是组织如何改进自己，并为下一次事故准备好改进的流程和实践。

**实施事故响应工作流程:**

1.  ****定义**处理不同 ent ML 事件的工作流程——第一响应者 sh 应遵循的问题流程和决策树是什么？谁先回应，如何回应？活动期间和之后会出现什么问题？**
***   ****在定义紧急程度时成为**目标。**T17*******   ****决定**在每个场景中应该激活哪些回退机制。*****   ****总结**事件，学习如何提高自己对下一次事件的应对能力。**T33**********

 ****## **结论**

随着机器学习模型在商业应用中获得更多的牵引力，政府和监管机构呼吁更多的责任和监管，将负责任的人工智能实践纳入您的 ML 工作流程至关重要。实施这四个速赢将使您能够实现您的 AI 目标，并确保您的组织高效和负责任地采用 AI。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>******