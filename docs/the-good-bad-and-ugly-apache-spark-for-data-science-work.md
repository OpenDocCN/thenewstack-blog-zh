# 好、坏、丑:数据科学工作的 Apache Spark

> 原文：<https://thenewstack.io/the-good-bad-and-ugly-apache-spark-for-data-science-work/>

从 Pivotal 进行测试并起草这篇文章的团队包括首席作家罗伯特·贝内特、斯科特·哈杰克、乔希·普洛特金、蒂姆·科普、大卫·盖尔曼、布兰登·施罗耶和玛丽安·米西奈。

[](https://tanzu.vmware.com/)

[Robert Bennett](https://tanzu.vmware.com/)

[Robert 是 Pivotal 的首席数据科学家，他帮助各行各业的客户利用数据和分析解决紧迫的业务挑战。在加入 Pivotal 之前，Robert 是 Princeton Associates 的高级顾问，他开发了一个逻辑模拟器，帮助优化新铁路位置的设计和部署。他拥有石溪大学的物理学博士学位，他的论文是关于= sqrt{200} GeV 质子-质子碰撞中直接光子产生的截面和双螺旋度不对称性。他在麻省理工学院(MIT)获得了物理学学士学位。](https://tanzu.vmware.com/)

[](https://tanzu.vmware.com/)[](https://tanzu.vmware.com/)

[Apache Spark](https://spark.apache.org/) 是一个内存数据分析引擎。由于其速度、可伸缩性和易用性，它广受数据科学家的欢迎。另外，它恰好是运行在 Kubernetes 上的理想工作负载。

许多 [Pivotal](https://pivotal.io/) 客户希望将 Spark 作为他们现代架构的一部分，因此我们希望分享我们使用该工具的经验。这篇文章开启了一个系列，其中我们将讲述一年多来在各种基础设施上使用多个版本的 Spark 的经验，包括内部部署、[谷歌云计算(GCC)](https://cloud.google.com/) 和[亚马逊网络服务(AWS)](https://aws.amazon.com/) 。到目前为止，我们已经将 Spark 用于数据清理和转换、特征工程、模型构建、模型评估评分和生产数据科学管道。

在 Pivotal，我们的数据科学团队正在不断测试最新的机器学习工具和技术。我们希望工具能够让我们轻松地专注于工作的数据探索和建模方面，并最大限度地减少数据工程、数据准备和其他繁琐工作。这些目标使得像 R 和 PyData 生态系统这样的工具如此受欢迎。它们的抽象，比如数据框架，使得交互式分析变得容易和愉快。它们还促进了单台机器上的并行计算，帮助我们更快地学习和迭代。但是，一旦数据集超过了单机的容量，R 和熊猫就再也无法满足这些需求了。

Spark 是一个开源的分布式计算框架，它承诺提供类似于熊猫的干净和愉快的体验，同时通过一个分布式架构扩展到大型数据集。在许多方面，Spark 实现了其对大型数据集的易用、高性能分析的承诺。然而，Spark 并非没有偶尔会增加复杂性的怪癖和特质。

与任何工具一样，Spark 有我们喜欢的功能，有我们不喜欢的功能，也有我们无法理解其设计的功能。这是好的、坏的和丑陋的预览。我们将在以后的帖子中添加更多细节。在这个过程中，我们将分享充分利用 Apache Spark 的技巧和诀窍。

## 好人

很容易理解为什么 Apache Spark 如此受欢迎。它进行内存中、分布式和迭代计算，这在使用机器学习算法时特别有用。其他工具可能需要将中间结果写入磁盘并将它们读回内存，这使得使用迭代算法非常慢。但这并不是喜欢 Spark 的唯一理由。

### 吸引人的 API 和延迟执行

Spark 的 [API](https://spark.apache.org/docs/2.2.0/api.html) 确实很吸引人。用户可以选择多种语言:Python、R、Scala 和 Java。Spark 提供了一个数据框架抽象，使用面向对象的方法进行转换、连接、过滤等等。这种面向对象的方式使得创建定制的可重用代码变得容易，这些代码也可以用成熟的测试框架进行测试。

“延迟执行”特别有用，因为它允许您定义一系列复杂的转换，以对象的形式表示。此外，您可以检查最终结果的结构，甚至不需要执行单独的中间步骤。Spark 在提交之前检查执行计划中的错误，这样坏代码很快就会失败。

### 容易转换

PySpark 提供了一个“toPandas()”方法来无缝地将 Spark 数据帧转换为 Pandas，它的“SparkSession.createDataFrame()”可以做相反的事情。“toPandas()”方法允许您在 Spark 将数据处理成更小的数据集后在内存中工作。当与 Pandas 的绘图方法结合使用时，您可以将命令链接在一起，以便在一个命令中加入您的大型数据集、过滤、聚合和绘图。Python 是一种支持快速操作数据的语言，PySpark 包将这一功能扩展到了大规模数据集。

### 轻松转换

旋转是许多大数据框架面临的挑战。在 SQL 中，它通常需要许多 case 语句。Spark 有一种简单而直观的方式来旋转数据帧。用户只需对目标索引列执行“groupBy ”,将目标字段作为列使用，最后执行聚合步骤。经过过去一年的广泛使用，我们发现它的执行速度惊人地快，而且易于使用。

Spark 的另一项资产是“地图端加入”广播方法。当其中一个表比另一个表小，并且可以完整地安装在单独的机器上时，这种方法可以显著地加快连接的速度。较小的表被发送到所有节点，因此较大的表中的数据不需要到处移动。这也有助于缓解不对称带来的问题。如果大表在连接键上有很多偏差，它会尝试将大量数据从大表发送到少量节点来执行连接，并淹没这些节点。

### 开源社区

Spark 背后有一个庞大的开源社区。该社区改进了核心软件，并提供了实用的附加包。例如，一个团队为 Spark 开发了一个[自然语言处理库。以前，用户要么必须使用其他软件，要么依赖缓慢的用户定义函数来利用 Python 包，如自然语言工具包。](https://databricks.com/blog/2017/10/19/introducing-natural-language-processing-library-apache-spark.html)

## 坏事

没有一个工具是完美的，所以让我们来回顾一下您使用 Spark 可能面临的挑战。

### 集群管理

众所周知，Spark 很难调整和维护。这意味着确保最高性能，使其不会在繁重的数据科学工作负载下崩溃是一项挑战。如果您的集群没有得到专业的管理，这可能会抵消我们上面描述的“好处”。由于内存不足错误而导致的作业失败是非常常见的，并且拥有许多并发用户使得资源管理更加具有挑战性。

您是采用固定内存分配还是动态内存分配？您允许 Spark 使用多少个集群核心？每个执行者得到多少内存？Spark 在混洗数据时应该使用多少分区？让所有这些设置适合数据科学工作负载是很困难的。

### 排除故障

调试 Spark 可能会令人沮丧。PySpark 中“DataFrame”操作的客户端类型检查可以捕捉到一些错误(比如试图对具有不兼容类型的字段进行操作)。但是内存错误和用户定义函数中发生的错误可能很难跟踪。

分布式系统天生复杂，因此 Spark 也是如此。错误消息可能具有误导性或被隐藏，从 PySpark 用户定义函数(UDF)进行日志记录很困难，并且对当前流程进行内省也不可行。为本地运行的 UDF 创建测试会有所帮助，但有时在集群上运行时，通过本地测试的函数会失败。在这些案例中找出原因是具有挑战性的。

### PySpark UDFs 的慢度

PySpark UDFs 比 Scala 和 Java UDFs 慢得多，而且占用更多内存。向 Scala 和 Java 倾斜的性能是可以理解的，因为 Spark 是用 Scala 编写的，运行在 Java 虚拟机(JVM)上。Python UDFs 需要将数据从执行者的 JVM 移动到 Python 解释器，这很慢。如果 Python UDF 性能有问题，Spark 确实可以让用户创建 Scala UDFs，可以在 Python 中运行。然而，这减慢了开发时间。

### 难以保证的最大并行度

Spark 的一个关键价值主张是分布式计算，但很难确保 Spark 尽可能并行化计算。Spark 试图根据作业的需要弹性地扩展作业使用的执行程序的数量，但它通常无法自行扩展。所以，如果你把执行人的最小数量设置得太低，你的工作可能在需要的时候利用不到更多的执行人。此外，Spark 将 RDDs(弹性分布式数据集)/数据帧划分为分区，这是执行器承担的最小工作单元。如果设置的分区太少，那么可能没有足够的工作块供所有的执行器处理。此外，更少的分区意味着更大的分区，这会导致执行器耗尽内存。

## 丑陋的

Spark 丑陋的一面往往分为两类:API 笨拙或没有意义的一面，以及 Apache Spark 项目缺乏成熟度和功能完整性。

### API 笨拙

由于 Spark API 的大部分都非常优雅，因此不优雅的部分非常突出。例如，我们认为访问数组元素是 Spark 生命中丑陋的一部分。虽然在 Spark 中实现这一点本身并没有什么问题，但考虑到 DataFrame API 在其他领域的无缝表现，这是违反直觉的。我们经常发现自己希望将模型的结果存储在 Spark 数据帧中。当这些结果包含值的数组时，访问数组的元素一点也不简单。这在本质上是不可避免的，因为许多 Spark-ML 函数都返回数组。

### 缺乏成熟度和功能完整性

自 2009 年其伯克利大学起源和 2014 年其 Apache 顶级首次亮相以来，Spark 已经走过了漫长的道路。尽管它的崛起令人眩晕，但 Spark 仍在不断成熟，缺乏一些重要的企业级功能。

值得称赞的是，在过去几年中，Apache Spark 已经获得了新的数据帧和数据集抽象，其机器学习库中的更多算法(MLlib 和 ML)以及改进的性能。不幸的是，企业升级到新版本可能会很慢。作为数据科学家，我们不得不应对较旧的版本，如 1.6、2.0 和 2.1，它们缺乏重要的功能。请记住，这些版本还不到两年。

Spark 的机器学习库缺少一些基本功能。例如，在 Spark 2.0(2016 年 7 月发布)之前，Random Forest 在其新的 ML 库中没有特性重要性。直到 Spark 2.2(2017 年 7 月发布)，梯度增强树才暴露了一个概率得分。这使得它不能用于大多数用例。即使对于公开浮点分数的模型，也会返回 ArrayType，例如[0.25，0.75]。令人震惊的是，没有内置函数来提取 0.75。它需要一个 UDF，如上所述，在 Python 中很慢，应该少用。结果，我们发现自己退回到使用更成熟的 scikit-learn 库在本地训练模型。

另一个示例特性差距是难以用 Spark 创建连续的唯一记录标识符。顺序的唯一索引列对于某些类型的分析很有帮助。根据[文档](https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/sql/functions.html),“monotely _ increasing _ id()”为每一行生成一个唯一的 ID，但不保证 ID 是连续的。如果连续 id 对您很重要，那么您可能需要使用 Spark 的旧 RDD 格式。

## 结论

从好的、坏的和丑陋的角度来看，从外面看，Spark 是一个有吸引力的工具。在全押之前，要知道有哪些陷阱。请继续关注本系列的后续文章，这些文章详细介绍了如何充分利用 Apache Spark 来处理数据科学工作负载。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>