# Meta 的 MultiRay，一个用于运行大型基础模型的 ML 平台

> 原文：<https://thenewstack.io/metas-multiray-a-ml-platform-for-running-large-foundational-models/>

训练一个人工智能模型并非易事。专业团队需要大量数据来训练一个大型模型来完成一项非常具体的任务，比如阅读数百万条帖子来学习如何识别有害言论。非常有帮助。但是这项任务也很昂贵，而且范围有限。考虑到培训每个模型的相关成本，很容易看出这是如何失控的。这导致巨大的成本，可能会将最先进的人工智能模型排除在生产级代码之外。

没有办法绕过人工智能理解内容的昂贵的数据密集型计算。机器必须学习。但是在哪里学习和如何学习可能会改变。社交媒体聚合 Meta [开发了一个新的平台](https://ai.facebook.com/blog/multiray-large-scale-AI-models/)，用于运行最先进的人工智能模型。MultiRay 的主要目标是使 Meta 的大型基础模型的访问民主化。

作为 Meta[推动其人工智能系统更高效](https://proceedings.mlsys.org/paper/2022/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf)的一部分，MultiRay 使用大型通用基础 ML 模型，这些模型经过训练，可以在各种任务和领域中表现良好。基础模型针对各种任务的功能进行了优化，包括相似性和分类。多个专门化的、较小的模型现在可以脱离通用模型的输入(也称为嵌入)。

随着大部分计算更加集中，Meta 能够购买更先进的加速器(专用硬件)来进行昂贵的计算。软件开发也从中受益，因为开发团队现在可以快速迭代和改进 ML 模型。

目前，MultiRay 支持超过 125 个跨 Meta 的用例，支持每秒 2000 万次查询(QPS)，同时每天处理 8000 亿次查询。

## **多瑞模式**

MultiRay 的第一个模型(自 2020 年开始生产)TextRay 专注于文本理解应用程序，可以执行从检测不真实内容到改善用户搜索体验的任务。

在 TextRay 的基础上，第二个模型 PostRay 加入了文本和图像理解，因为为了真正理解 post，它可以包括图像、视频和文本，系统需要有能力在彼此的上下文中单独分析每一个。

在 PostRay 之前，这种描绘功能需要将几个不同的模型组合在一起，并且消耗了太多的计算和功率资源来将 ML 模型实际投入生产。

训练、部署和维护后射线模型是复杂的，因为它们结合了多个领域的先进研究，但只需要训练一次。它有几个跨元的用例，包括用于卷轴的主题分类。

## **multi ray 如何工作**

MultiRay 通过加速来集中执行，并使用缓存来节省重新计算的成本。

MultiRay 的大型基础模型返回高维向量空间中代表输入的一个点。重点是“嵌入”,它是原始输入的更 ML 友好的版本。与处理原始输入(文本和图像)不同，特定于任务的模型可以从更容易处理的多分类中提取嵌入内容。

嵌入是巨大的，比输入本身大得多(几千字节)。

## 为什么要集中化？

**软件视角**这篇博文概括了更小的个体团队工作流的上限，即创建、维护和保养个体模型的负担，以及应用复杂的优化技术的困难。集中的工作流缓解了大部分问题，团队可以专注于开发和迭代特定于任务的模型。

**硬件角度**大型模型和延迟限制对图形处理单元(GPU)要求非常高，这些图形处理单元是用于 MultiRay 的加速器。集中式模型允许顶级 GPU 在团队之间共享，而不是多个团队拥有自己的 GPU。

## 多瑞的缓存

多层缓存以每层的速度为代价来换取命中率。这些层从每个多级服务器的 RAM 中快速但较小的每主机本地缓存开始，以闪存中较慢但大得多的全局分布式缓存结束。缓存存储是有限的，因此不可能长时间存储缓存结果。

多速率测量客户端的请求模式，以确定最佳缓存设置(大小、生存时间、更新策略)，从而降低服务成本。例如，Meta 使用测量的数据来模拟各种缓存生命周期设置所需的能量，权衡重新计算加速请求的成本与从缓存提供服务的成本。这个反馈循环允许我们提高 MultiRay 的效率，即使客户端行为不断变化。

## **集中式服务的挑战**

大规模系统(即数据库)已经解决的一些挑战，如客户管理、配额和成本归属，必须适应人工智能领域。查询大小和缓存命中率都会影响处理查询所需的能量，因此配额更加复杂。另一个挑战是，在构建这些模型的过程中产生的费用只有在模型被使用时才有意义。这是一个不断变化的目标，在新的模型架构中不断创新，在模型更新和培训流方面进行大量投资。

## **附加学习**

MultiRay 已经成为 Meta 的 ML 和系统专家贡献关键优化的沙箱，以支持更广泛的 PyTorch 和加速器生态系统。MultiRay 是在 meta 的生产中部署 PyTorch 的 [Better Transformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) 的第一个大型用例。这在不影响质量的情况下节省了大量容量。

下面的研究来自 Meta 的基础人工智能研究(FAIR)团队，该团队领导了它的开发。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>