# 启动运行:人工智能希望提高人工智能工作负载的 GPU 利用率

> 原文：<https://thenewstack.io/startup-runai-looks-to-improve-gpu-utilization-for-ai-workloads/>

一家初创公司正在使用 VMware 风格的技术，让数据科学家能够更好地利用 GPU 计算能力来运行他们的人工智能(AI)工作负载。

[Run:AI](https://www.run.ai/) ，其平台旨在使组织能够利用他们需要的所有计算能力来加速他们的人工智能开发和部署工作，最近推出了两项新技术，瘦 GPU 供应和工作交换，允许数据科学家共享他们分配给他们的人工智能工作的[GPU](https://thenewstack.io/tutorial-deploy-the-nvidia-gpu-operator-on-kubernetes-based-on-containerd-runtime/)。

精简 GPU 配置为计算加速器所做的事情与 [VMware](https://tanzu.vmware.com?utm_content=inline-mention) 的精简配置为存储区域网络(San)所做的事情一样，后者根据需要重新分配可用的磁盘空间。数据科学家经常被分配到 GPU 进行与人工智能相关的工作，但并不总是能够使用他们获得的所有加速器计算能力。结合起来，运行:AI 的两项新技术自动优化了 GPU 的分配，因此一个数据科学家没有使用的 GPU 计算能力可以自动调配给另一个数据科学家使用。

Run:AI 的联合创始人兼首席执行官 Omri Geller 告诉 The New Stack:“很多时候，人们要求资源，但并没有真正使用这些资源。“我们的技术是保密的。其他请求计算能力的数据科学家可能会获得之前分配给其他人的计算能力，因为我们的系统识别出这个人实际上并没有使用 GPU。这对用户来说是透明的，这是一个优势。这意味着，如果“数据科学家 A”没有使用他或她的 GPU，“数据科学家 B”可以使用已调配的 GPU，并且不知道它之前已被分配。”

## **GPU 和人工智能工作负载**

自从 [Nvidia](https://thenewstack.io/nvidia-offers-hosted-large-scale-processing-for-ai/) 开始将其作为加速器在具有传统 CPU 的系统中运行，以提高性能和能效以来，GPU 在数据中心和高性能计算(HPC)环境中变得越来越重要。在两年一度的世界最快系统 500 强榜单[中，排名前 10 的超级计算机中有 6 台采用了英伟达的 GPU。](https://www.top500.org/lists/top500/2021/06/)

凭借其并行处理能力和英伟达的 CUDA 框架，GPU 可以加速人工智能和[深度学习工作负载](https://thenewstack.io/demystifying-deep-learning-and-artificial-intelligence/)的计算过程，使其成为数据科学家的必备工具。盖勒说，问题是尽管它们在数据中心越来越重要，但帮助企业优化它们的工具却很少。

“今天，已经有很多技术应用于计算机中的普通 CPU，”他说。“如今，所有操作系统对 CPU 的利用都非常好。谈到芯片交易，在大多数情况下，GPU 不是一等公民，不是在 Kubernetes，也不是在云原生生态系统中。如何在领先的 Kubernetes 部署中利用这些加速器，现在还只是刚刚起步。因此，我们的路线图是让这些加速器成为新环境中的一流公民。通过这样做，我们将能够提高这些计算资源的利用率。”

## **运行:AI 编排 GPU 功率**

这家以色列初创公司成立于三年多前，拥有约 60 名员工，正在设计其软件平台，以便在数据科学家需要时为他们提供尽可能多的 GPU 能力。它将工作负载从底层硬件中分离出来，支持资源池化，并使用高级调度功能来确保资源得到充分利用，并获得最需要的工作负载。这使得数据科学家能够进行更多的实验，并更快地获得结果。

该公司声称，对 GPU 调度和动态配置的实时可见性和控制导致现有基础设施的利用率翻了一番。

"构建应用程序时，最重要的事情之一就是计算能力."盖勒说。“这些应用程序变得越来越大、越来越复杂，它们需要强大的计算能力。…我们接受了技术上的挑战，即开发出能够充分利用这些计算资源的最佳软件。”

## **虚拟资源池**

该平台创建了一个 GPU 系统的虚拟池，并包括一些功能，如将 GPU 分成可用于不同工作负载的部分，以及一个基于 Kubernetes 的工作负载调度程序。它现在还将包括精简 GPU 配置和作业交换功能。该平台的目标是使它执行的所有任务对用户透明，并在运行他们习惯的工具时为他们提供所需的功能，无论是在内部还是在云中。

Run:AI 今年推出了其 [Run:it your way](https://www.run.ai/blog/runai-launches-researcherui-kubeflow-mlflow/) 计划，该计划使数据科学家能够从本质上运行任何机器学习工具——从管道和数据预处理到 [Kubeflow](https://thenewstack.io/tutorial-install-kubernetes-and-kubeflow-on-a-gpu-host-with-nvidia-deepops/) (一个在 Kubernetes 上运行的开源机器学习平台)——他们想要在供应商的计算编排平台上管理建模等流程。

“各种部署的一致之处在于，数据科学家正在使用 GPU，”首席执行官表示。“不一致的是数据科学家使用什么工具在 GPU 上运行工作负载。世界上的每个数据科学家都有他们选择的工具，我们在运行:人工智能，我们不想改变这一点。”

## **透明供应**

精简 GPU 配置对于使用该平台的数据科学家来说是透明的。最初分配到 GPU 的用户看不到是否有任何加速器计算能力被重新分配给另一位数据科学家，该数据科学家也看不到他们能够获得更多计算能力。盖勒说，他们都知道他们的工作负载正在获得所需的计算能力。

Run:AI 营销副总裁法拉·海恩(Fara Hain 告诉新堆栈,“工作交换功能是这一切的关键。”。她说，这个工具可以识别哪里需要 GPU 计算能力，识别可用 GPU 的位置，然后重新分配它们。它是基于企业的 It 和数据科学团队为特定工作设定的优先级和策略来实现的。

“交换是基于自由资源以及预先设定的政策和优先级而发生的，”Hain 说。

运行:AI 正在客户实验室中测试精简 GPU 配置和作业交换。两者都有望在第四季度上市。

盖勒说，Run:AI 的软件平台目前已经筹集了 4300 万美元的资金，通常可以获得，并被一系列大型企业使用。客户中有 [Wayve](https://wayve.ai/) ，这是一家总部位于伦敦的公司，为自动驾驶汽车开发人工智能软件，以及伦敦医疗成像和基于价值的医疗保健人工智能中心，该中心在该国的国民健康服务(NHS)中开发、测试和部署人工智能系统。

他说，Run:AI 的大多数客户都在金融服务、汽车和医疗保健领域，尽管也有一些客户在其他垂直领域。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>