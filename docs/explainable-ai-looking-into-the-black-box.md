# 可解释的人工智能:观察黑盒

> 原文：<https://thenewstack.io/explainable-ai-looking-into-the-black-box/>

[解释可解释的艾:卡米尔·艾迪把目光投向黑匣子](https://thenewstack.simplecast.com/episodes/explaining-explainable-ai-camille-eddy-on-looking-into-the-black-box)

在本期 [The New Stack Makers](https://thenewstack.io/podcasts/makers) 播客中，我们与 [Camille Eddy](https://www.linkedin.com/in/camilleeddy/) 聊天，她对可解释人工智能(XAI)的兴趣始于在惠普公司高级机器人实习期间，当时她开始担心社交机器人缺乏多样性的影响。

她在 [2018 年奥赖利开源大会](https://www.oreilly.com/ideas/recognizing-cultural-bias-in-ai)上发表的演讲“[认识人工智能](https://www.oreilly.com/ideas/recognizing-cultural-bias-in-ai)中的文化偏见”以这个问题开始:  “像一个人的肤色这样不变的东西怎么能阻止他们享受我创造的任何产品？”

埃迪说，当我们想到我们如何生活以及如何与科技互动时，我们并不总是想到其他人的互动。例如，著名的[史诗般的失败，脸书总部的洗手液分配器](https://www.youtube.com/watch?v=YJjv_OeiHmo)没有给黑人的手分配肥皂，因为它没有被校准以观察肤色。

作为一名工程师，Eddy 关注着她所创造的技术的意想不到的后果。

### 可解释的人工智能

尽管 Eddy 最初认为自己是一名机械工程师，但她也对人工智能(AI)感兴趣。她经常同时玩弄双方。“当我打开一堆代码时，没有人奇怪地看着我，当我在机械车间时，也没有人奇怪地看着我，”她说。

这种观点使她能够进行对话，首先是在内部，然后是与其他人，讨论软件如何被放入硬件，以及软件如何影响客户的整体体验。

理解算法为什么做出决定的想法是 XAI 的精髓。她说:  我们已经看到了无法识别肤色或做出结论的例子，但不理解为什么人工智能算法会做出这样的结论。

随着行业正在处理关于创建更好的工具以及更好地理解和认识人工智能的想法，XAI 或公平工具已经开始出现。

你必须非常小心，确保你得到的结果确实是你想要的结果，她说。Eddy 引用了华盛顿大学的一项研究，他们认为他们在计算狼对哈士奇，但意识到该算法实际上是在计算交易照片中的雪对无雪。  算法工作正常，只是不像研究人员预想的那样。

因此，最近谷歌和 IBM 推出了一系列公平工具。  [**谷歌的假设工具**](https://ai.googleblog.com/2018/09/the-what-if-tool-code-free-probing-of.html) ，允许你改变杠杆，四处移动，以了解你的数据的背景，从整体上看待数据，而不是专注于某些点，或问一些具体的问题，如我这样做对吗？我是否已经包含了我需要的所有用例？

对于一个研究人员来说，这是我的研究，这是结论，这是所有的数据点，回答关于结果背后的推理的问题。

这是一种可演示的方法，可以看到为什么算法会得出它返回的结果，也是一种观察人工智能黑盒内部的方式。“我们可以做出明智的决定，”埃迪说，“人们可以回去了解这些决定来自哪里。”

请收听有关哈士奇与斯诺项目的更多信息，以及我们使用算法的方式如何对我们的个人知识产生严重影响。

如果你有兴趣了解更多，Eddy 女士将在今年晚些时候的敏捷测试会议上讲授一个关于在 XAI 工具中使用公平性的 8 小时教程。

### 在这个版本中:

[3:35:](https://thenewstack.simplecast.com/episodes/explaining-explainable-ai-camille-eddy-on-looking-into-the-black-box?t=3:35) 处于硬件和软件交汇点的工程师。
[5:53:](https://thenewstack.simplecast.com/episodes/explaining-explainable-ai-camille-eddy-on-looking-into-the-black-box?t=5:53) “可交代的艾”又是什么。
[10:20:](https://thenewstack.simplecast.com/episodes/explaining-explainable-ai-camille-eddy-on-looking-into-the-black-box?t=10:20) 从各方面看数据。
[13:04:](https://thenewstack.simplecast.com/episodes/explaining-explainable-ai-camille-eddy-on-looking-into-the-black-box?t=13:04) 我们使用算法的方式会对我们的个人知识产生严重的后果。
[16:53:](https://thenewstack.simplecast.com/episodes/explaining-explainable-ai-camille-eddy-on-looking-into-the-black-box?t=16:53) 将值应用到我们编写的软件中。
[19:42:](https://thenewstack.simplecast.com/episodes/explaining-explainable-ai-camille-eddy-on-looking-into-the-black-box?t=19:42) 讨论算法的兴起。

通过 Pixabay 的特征图像。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>