# Nvidia 的 CUDA 12 是为了在 GPU 中展现这种动物

> 原文：<https://thenewstack.io/nvidias-cuda-12-is-here-to-bring-out-the-animal-in-gpus/>

随着 Nvidia 公司周一发布的新软件工具，程序员终于可以利用该公司代号为 Hopper 的最新 GPU 的全部计算能力。

Nvidia 现在正在推出其 CUDA 12 编程工具，这是该公司在人工智能和图形领域未来的驱动力。CUDA 的足迹跨越了 Nvidia 公司的大部分硬件和软件产品。

“CUDA 在某些方面是 GPU 的接口。但它是一个接口，是一个编程模型、一整套工具和大量的库，”[英伟达](https://www.nvidia.com/en-us/about-nvidia/#slide-14-095fee10)的 CUDA 架构师[斯黛芬·琼斯](https://www.linkedin.com/in/stephen-jones-profile/)在对新堆栈的独家采访中说道。

CUDA 提供了与 GPU 通信的核心基础。Nvidia 在此基础上构建了多层软件。如果没有 CUDA 12，开发者将无法利用 Nvidia 最新的 H100 GPU，这是该公司迄今为止最快的图形处理器。

“硬件一准备好，我们就想出去把 CUDA 12 放到人们手中，”琼斯说。

## 新建筑

Nvidia 通常会随着每个新的 GPU 架构发布新版本的 CUDA。之前的版本是 CUDA 11，发布时搭载了基于 Ampere 架构的上一代 GPU。

CUDA 的第一个版本于 2007 年问世，作为一套编程工具，用来编写可以利用 GPU 上更快计算速度的应用程序。Nvidia 的 GPU 现在已经成为人工智能计算的中流砥柱，CUDA 作为一个软件栈的受欢迎程度也在飙升。

CUDA 12 的发布与 Nvidia 上周发布的 AI Enterprise 3.0 软件保持一致，该软件包括机器人、安全、自动驾驶汽车和医疗保健等领域应用的预配置人工智能工具和模型。一些新工具建立在 CUDA 12 之上。

为了获得 CUDA 12 的最佳性能，程序员需要拥有带 Hopper GPUs 的系统，这种系统还没有广泛使用。GPU 预计将很快批量出货。

CUDA 12 框架随着硬件的开发而开发，并利用芯片的并行性来加速图形和 AI。

Hopper 可以并行处理 16，384 个任务和 132 个流式多处理器，比十年前开普勒架构中的 15 个处理器有所改进。由于支持 PCIe Gen5 互连、900GB/s 带宽的 NVLink 互连和 HBM3 内存，数据在 GPU 内的移动速度也更快。

CUDA 程序为执行 GPU 和 CPU 的代码提供了单独的部分，包括执行环境中的内存分配和硬件管理。

CUDA 的运行时系统包括库和一个将代码转换成可执行文件的编译器。CUDA 有一个名为 PTX 的汇编代码部分，它为 CUDA 的所有版本提供了向前和向后兼容层，一直到版本 1.0。

CUDA 12 最大的进步之一是使 GPU 更加自给自足，并减少对 CPU 的依赖。此外，Hopper 缩短了数据传输的距离，并减少了数据与内存交换的次数。CUDA 12 利用这些硬件功能来加快计算速度和训练人工智能模型。

“如果不更新数据结构，就无法引入新的硬件，Hopper 上以前不存在的新设备属性——我们必须去更新它们，”Jones 说。

GPU 通常严重依赖 CPU 来完成解压缩和图像处理等任务。英伟达表示，可以通过将更多计算转移到 GPU 来减少这种持续的依赖。CUDA 12 和 Hopper 可以在不退出 GPU 的情况下动态地做到这一点，这是一种被 Nvidia 称为“动态并行”的技术。

“我们的想法是让 GPU 保持满负荷运行，”琼斯说，

例如，GPU 上的神经网络可以根据数据输入和数据中的值范围来定义最有效的精度。

“这些类型的事情是非常动态的决定，完全在 GPU 上完成这些决定的能力……使 GPU 更加忙碌，它使 CPU 不必不断关注和运行控制代码，”琼斯说。

琼斯说，历史上使用 GPU 生成工作的成本很高，但现在已经下降到 CPU 和 GPU 生成工作的成本相同的水平，并补充说“现在 GPU 上的决策可以非常快。”

CUDA 编程模型将图像处理等工作分解成一个个相邻放置的小块。每个块都像一个独立的程序一样运行。当处理完成时，结果被组合成一个答案。Hopper 可以解决数千个块上的独立问题，并将其进一步分解到自己的线程中。Hopper GPU 可以同时运行 16，384 个线程。

“当我们认识到 GPU 变得更大时，现在有更多的空间来激活更多的线程，同时承担更大的问题，因为我有更多的处理能力、更多的资源和更多的线程，”琼斯说。

CUDA 12 利用了重新设计的 Hopper，它被组织成一个新的网格层次结构，Nvidia 称之为“线程块集群”。新的结构为 Hopper 提供了更多独立的工作块和线程来处理高要求的任务。

“它保证了并发性，这是关键部分。我可以在它们之间同步，我可以在它们之间交换数据。这是我们在软件和硬件之间进行大量协同设计的地方之一，”琼斯说。

## 本地化事务

CUDA 12 还可以通过本地化事务和缩短数据传输距离来执行应用程序。流式多处理器彼此相邻，这意味着电子不必走很远。更大的并行性意味着更好的性能，CUDA 12 程序员不必去找 CPU 要资源。

“利用本地资源是硬件试图实现的目标。提供分级并行是编程模型正在得到的东西，”琼斯说。

Hopper 有一个称为异步事务屏障的功能，线程可以等待，直到来自其他线程的数据到达，以完成一个事务。

通常，线程相互同步和协调，并跟踪数据移动，但 Hopper 节省了查找数据所需的时间，从而节省了移动数据所需的能量、精力和带宽。

异步移动数据的能力，以及等待完成信号而不是一直监视它的能力，是在线程之间解耦工作的更有效的方式。

“这些东西彼此非常接近，这是利用了局部性因素。发送……这些计数信号的速度非常快。它还将许多这种管理放入硬件中，这样既可以更快，又可以避免在计算时花费周期，还可以停下来查看数据的位置，”琼斯说。

一种称为张量内存加速器的新处理单元 Nvidia 称之为数据移动引擎——允许大型数据块在全局和共享内存层次结构之间双向移动。TMA 还接管集群中线程块之间的异步内存复制。

CUDA 12.0 支持 C++20 标准，该标准支持 GCC 10、Clang 11 和 ArmC/C++ 22.x 等主机编译器。Nvidia 拥有自己的 ARM CPU，并与其 GPU 配对，因此，CUDA 的改进主要集中在 ARM 架构上。

英特尔拥有用于并行编程的 OneAPI 工具，其中还包括 SYCL 工具，该工具可以剥离专有的 CUDA 特定代码，以便应用程序可以在其硬件上运行。AMD 正在为其 GPU 支持 ROCm，这是一个开放的并行编程框架。Khronos 支持 OpenCL 并行编程框架。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>