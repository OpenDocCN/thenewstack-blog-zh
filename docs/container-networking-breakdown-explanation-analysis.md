# 集装箱联网:分解、解释和分析

> 原文：<https://thenewstack.io/containers/container-networking-breakdown-explanation-analysis/>

[](http://blog.gingergeek.com/about)

虽然许多人倾向于将网络覆盖作为解决跨主机容器网络的流行方法，但是容器网络的功能和类型有很大不同，当您考虑适合您环境的类型时，值得更好地理解。有些类型与容器引擎无关，而其他类型则被锁定在特定的供应商或引擎中。一些人关注简单性，而另一些人关注功能的广度或 IPv6 友好性和组播能力。哪一种适合您取决于您的应用程序需求、性能要求、工作负载放置(私有云或公共云)等。让我们回顾一下更常见的容器网络类型。

有多种方式提供容器到容器和容器到主机的连接。本文主要关注当前容器网络类型的分类，包括:

*   没有人
*   桥
*   覆盖物
*   衬垫物

## 过时的容器网络类型

随着容器技术的进步，联网的方法也在发展。两种网络模式已经出现，但都已经消失了。

### 链接和大使

在拥有多主机网络支持和 Swarm 协调之前，Docker 从单主机网络开始，通过链接促进网络连接，作为一种机制，允许容器通过环境变量或/etc/hosts 文件条目发现彼此，并在容器之间传输信息。链接功能通常与大使模式相结合，以便于跨主机链接容器，并减少硬编码链接的脆弱性。这种方法的最大问题是它太静态了。一旦创建了容器并定义了环境变量，如果相关的容器或服务移动到新的 IP 地址，那么就不可能更改这些变量的值。

### 容器映射网络

在这种网络模式下，一个容器重用(映射到)另一个容器的网络名称空间。这种联网模式只有在运行 Docker 容器时才会被调用，比如:–net:container:*some _ container _ name _ or _ id*。

这个 run 命令标志告诉 Docker 将这个容器的进程放入已经在另一个容器中创建的网络堆栈中。虽然与第一个容器共享相同的 IP 和 MAC 地址以及端口号，但是新容器的进程仍然受限于它自己的文件系统、进程列表和资源限制。两个容器上的进程将能够通过环回接口相互连接。

这种类型的网络对于在运行的容器上执行诊断是有用的，并且该容器缺少必要的诊断工具(例如，curl 或 dig)。可以创建具有必要诊断工具的临时容器，并将其连接到第一容器的网络。

容器映射网络可用于模拟 pod 式网络，其中多个容器共享相同的网络名称空间。好处，比如共享本地主机通信和共享相同的 IP 地址，是容器在同一个 pod 中运行这一概念所固有的，这是 rkt 容器的行为。

## 集装箱网络的当前类型

网络的划分围绕着每个容器的 IP 与每个容器的 IP 模型，以及需要网络地址转换(NAT)与不需要转换。

### 没有人

没有一个是直接的，因为容器接收网络栈，但是缺少外部网络接口。但是，它会收到一个环回接口。rkt 和 Docker 容器项目在不使用或不使用网络时都提供了类似的行为。这种容器网络模式有许多用途，包括测试容器、为以后的网络连接准备容器，以及分配给不需要外部通信的容器。

### 桥

Linux 桥提供了一个主机内部网络，同一主机上的容器可以在该网络中通信，但是分配给每个容器的 IP 地址不能从主机外部访问。网桥网络利用 iptables 进行 NAT 和端口映射，从而提供单主机网络。网桥联网是默认的对接网络类型(即对接 0)，其中虚拟网络接口对的一端连接在网桥和容器之间。

以下是创建流程的一个示例:

1.  主机上提供了一个网桥。
2.  每个容器的名称空间都在该桥中提供。
3.  容器的 ethX 被映射到私有桥接口。
4.  带有 NAT 的 iptables 用于在每个私有容器和主机的公共接口之间进行映射。

NAT 用于提供主机之外的通信。虽然桥接网络解决了端口冲突问题，并为在一台主机上运行的容器提供了网络隔离，但使用 NAT 会带来性能成本。

### 主持

在这种方法中，新创建的容器与主机共享其网络命名空间，提供了更高的性能(接近金属速度)并消除了对 NAT 的需要；但是，它确实存在端口冲突。尽管容器可以访问主机的所有网络接口，但除非以特权模式部署，否则容器可能无法重新配置主机的网络堆栈。

主机联网是 Mesos 中使用的默认类型。换句话说，如果框架没有指定网络类型，新的网络命名空间将不会与容器相关联，而是与主机网络相关联。有时也称为本机网络，主机网络在概念上很简单，更易于理解、故障排除和使用。

### 覆盖物

覆盖层使用网络隧道来传递跨主机的通信。这允许容器通过从一个主机到下一个主机的隧道网络子网来表现得好像它们在同一台机器上一样；实质上，跨越一个网络的多台主机。存在许多隧道技术，例如虚拟可扩展局域网(VXLAN)。

VXLAN 一直是 Docker libnetwork 选择的隧道技术，其多主机网络在 1.9 版本中作为本机功能加入。随着这一功能的引入，Docker 选择利用 HashiCorp 的 Serf 作为 gossip 协议，选择它是因为它在邻居表交换和收敛时间方面的效率。

对于那些需要其他隧道技术支持的人来说，法兰绒可能是个不错的选择。它支持 udp、vxlan、host-gw、aws-vpc 或 gce。每种云提供商隧道类型都会在提供商的路由表中创建路由，仅用于您的帐户或虚拟私有云(VPC)。对公共云的支持对于覆盖层驱动程序尤为关键，因为覆盖层能够最好地解决混合云使用案例，并在无需开放公共端口的情况下提供扩展和冗余。

多主机网络在启动 Docker 守护进程时需要额外的参数，以及一个键值存储。一些覆盖依赖于分布式键值存储。如果您正在进行容器编排，您已经有了一个分布式键值存储。

覆盖侧重于跨主机通信的挑战。连接到两个不同覆盖网络的同一主机上的容器不能通过本地网桥相互通信——它们彼此是分段的。

### 成为……的基础

底层网络驱动程序将主机接口(即 eth0 的物理网络接口)直接暴露给运行在主机上的容器或虚拟机。两个这样的底层驱动程序是媒体访问控制虚拟局域网(MACvlan)和互联网协议 VLAN (IPvlan)。网络工程师非常熟悉 MACvlan 和 IPvlan 驱动程序的操作和行为。这两种网络驱动程序在概念上都比网桥网络简单，不需要端口映射，而且效率更高。此外，IPvlan 有一个 L3 模式，这与许多网络工程师有很好的共鸣。考虑到大多数公共云中的限制(或缺乏功能)，当您需要处理内部工作负载、安全问题、流量优先级或合规性时，基础架构尤其有用，非常适合棕色地带使用。底层网络允许每个子接口有一个 VLAN，而不是每个 VLAN 需要一个网桥。

#### 麦克法兰

MACvlan 允许在主机的单个物理接口后创建多个虚拟网络接口。每个虚拟接口都分配有唯一的 MAC 和 IP 地址，但有一个限制:IP 地址必须与物理接口位于同一个广播域中。虽然许多网络工程师可能更熟悉子接口这一术语(不要与辅助接口混淆)，但用来描述 MACvlan 虚拟接口的说法通常是上层接口或下层接口。MACvlan 联网是一种消除对 Linux 网桥、NAT 和端口映射的需要的方式，允许您直接连接到物理接口。

MACvlan 对每个容器使用一个唯一的 MAC 地址，这可能会导致网络交换机出现问题，因为这些网络交换机具有防止 MAC 欺骗的安全策略，每个物理交换机接口只允许一个 MAC 地址。

容器流量被过滤，不能与底层主机通信，这将主机与其运行的容器完全隔离。主机无法到达容器。容器与主机隔离。这对于服务提供商或多租户场景非常有用，并且比桥接模型具有更高的隔离性。

MACvlan 需要混杂模式；MACvlan 有四种工作模式，Docker 1.12 仅支持桥接模式。MACvlan 网桥模式和 IPvlan L2 模式在功能上几乎相同。两种模式都允许广播和组播流量进入。这些底层协议在设计时考虑了内部使用案例。您的公共云里程会有所不同，因为大多数都不支持其虚拟机接口上的混杂模式。

提醒一句:MACvlan 桥接模式在跟踪网络流量和端到端可见性方面，为每个容器分配一个唯一的 MAC 地址可能是一件好事；然而，对于典型的网络接口卡(NIC ),例如 Broadcom，具有 512 个唯一 MAC 地址的上限，应该考虑这个上限。

#### IPvlan

IPvlan 与 MACvlan 相似，它创建新的虚拟网络接口，并为每个接口分配唯一的 IP 地址。区别在于主机上的所有 pod 和 containers 使用相同的 MAC 地址，即物理接口的相同 MAC 地址。之所以需要这种行为，主要是因为许多交换机通常配置的安全状态是关闭流量来自多个 MAC 地址的交换机端口。

在内核 4.2 或更新版本上运行最佳，IPvlan 可以在 L2 或 L3 模式下运行。与 MACvlan 一样，IPvlan L2 模式要求分配给子接口的 IP 地址与物理接口位于同一个子网中。但是，IPvlan L3 模式要求容器网络和 IP 地址与父物理接口位于不同的子网上。

当使用 IP 链接创建时，Linux 主机上的 802.1q 配置是短暂的，因此大多数运营商使用网络启动脚本来保存配置。随着容器引擎运行底层驱动程序并公开用于 VLANs 的编程配置的 API，自动化水平将会提高。例如，当在架顶式交换机上创建新的 VLAN 时，可以通过暴露的容器引擎 API.ico 将这些 VLAN 推入 Linux 主机

#### MACvlan 和 IPvlan

在这两种底层类型之间进行选择时，考虑是否需要网络能够看到单个容器的 MAC 地址。

对于地址解析协议(ARP)和广播流量，这两种底层驱动程序的 L2 模式就像连接到交换机的服务器一样，使用 802.1d 数据包进行泛洪和学习。然而，在 IPvlan L3 模式中，网络堆栈是在容器内处理的。不允许多播或广播流量进入。从这个意义上说，IPvlan L3 模式的运行方式与您对 L3 路由器的预期一样。

请注意，上游 L3 路由器需要知道使用 IPvlan 创建的网络。网络广告和重新分发到网络中仍然需要完成。如今，Docker 正在试验边界网关协议(BGP)。虽然静态路由可以在机架交换机上创建，但像 [goBGP](http://osrg.github.io/gobgp/) 这样的项目已经作为一种容器生态系统友好的方式涌现出来，以提供邻居对等和路由交换功能。

尽管给定主机支持多种网络模式，但 MACvlan 和 IPvlan 不能在同一物理接口上同时使用。简而言之，如果你习惯于将中继下行到主机，L2 模式适合你。如果规模是一个主要问题，L3 有巨大规模的潜力。

#### 直接路由

出于与 IPvlan L3 模式引起网络工程师共鸣相同的原因，他们可能会选择避开 L2 挑战，转而专注于解决第 3 层的网络复杂性。这种方法得益于利用现有的网络基础设施来管理容器网络。专注于 L3 的容器网络解决方案使用路由协议来提供连接，这可以说更容易与现有的数据中心基础设施互操作，连接容器、虚拟机和裸机服务器。此外，就过滤和隔离网络流量而言，L3 网络可扩展并提供粒度控制。

[Calico](https://www.projectcalico.org) 就是这样一个项目，它使用 BGP 为每个网络分配路由，特别是使用 a /32 为该工作负载分配路由，这使它能够与现有的数据中心基础设施无缝集成，而无需覆盖。没有覆盖或封装的开销，结果是网络具有卓越的性能和规模。容器的可路由 IP 地址向外界公开了 IP 地址；因此，港口天生就暴露在外面。经过培训并习惯于使用路由协议部署、诊断和操作网络的网络工程师可能会发现直接路由更容易理解。然而，值得注意的是，Calico 不支持重叠的 IP 地址。

#### 粉丝网络

Fan 网络是获得更多 IP 地址的一种方式，从一个分配的 IP 地址扩展到 250 个 IP 地址。这是一种无需覆盖网络就能获得更多 IP 的有效方式。当在公共云中运行容器时，这种类型的网络特别有用，在公共云中，单个 IP 地址被分配给一个主机，并且不允许运行额外的网络，或者运行另一个负载平衡器实例的成本很高。

#### 点对点

点对点可能是最简单的网络类型，也是 CoreOS rkt 使用的默认网络。默认情况下，它使用 NAT 或 IP 伪装(IPMASQ)创建一个虚拟以太网对，将一个放在主机上，另一个放在容器 pod 中。点对点网络利用 iptables 来提供端口转发，不仅用于到 pod 的入站流量，还用于 pod 中其他容器之间通过环回接口的内部通信。

## 能力

除了纯粹的连接，还需要考虑对其他网络功能和网络服务的支持。许多容器网络模式要么利用 NAT 和端口转发，要么有意避免使用它们。选择网络时，IP 地址管理(IPAM)、多播、广播、IPv6、负载平衡、服务发现、策略、服务质量、高级过滤和性能都是额外的考虑因素。

问题是这些功能是否得到支持，以及开发者和运营商如何通过这些功能获得支持。即使您选择的运行时、orchestrator 或插件支持容器网络功能，您的基础设施也可能不支持它。虽然一些二级公共云提供商提供对 IPv6 的支持，但顶级公共云中缺乏对 IPv6 的支持强化了对其他网络类型的需求，如覆盖和风扇网络。

在 IPAM 方面，为了提高易用性，大多数容器运行时引擎默认使用 host-local 为容器分配地址，因为它们连接到网络。主机本地 IPAM 包括定义一个要选择的固定 IP 地址块。动态主机配置协议(DHCP)在容器网络项目中得到普遍支持。集装箱网络模型(CNM)和集装箱网络接口(CNI)都有 IPAM 内置和插件框架，用于与 IPAM 系统集成，这是在许多现有环境中采用的关键功能。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>