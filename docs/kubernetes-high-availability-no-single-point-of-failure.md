# Kubernetes 高可用性:无单点故障

> 原文：<https://thenewstack.io/kubernetes-high-availability-no-single-point-of-failure/>

对于即将到来的

[KubeCon + CloudNativeCon Europe](https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2018/attend/)

下个月在哥本哈根，kld strm 将讲述如何打造一个“生产就绪”的 Kubernetes 集群。

[Kubernetes](https://kubernetes.io/) 开源容器编排引擎的主要优势之一是，它通过使用容器的动态调度，为分布式应用程序带来了更高的可靠性和稳定性。但是，当一个组件甚至整个数据中心停机时，您如何确保 Kubernetes 本身保持正常运行呢？

这就是 Kubernetes 高可用性规划发挥作用的地方。K8s HA 不仅仅是关于 Kubernetes 本身的稳定性。Kubernetes 专家[Lucas kld strm](https://speakerdeck.com/luxas)解释说，这是关于建立 Kubernetes 以及支持组件，如 etcd，以这样一种方式，没有单点故障。kld strm 是[云本地计算基金会](https://www.cncf.io/)的 CNCF 志愿者大使，并在赫林斯基组织了 CNCF 和 Kubernetes 芬兰 meetup 小组。

Alexis Richardson 向我们透露，kld strm 是 Kubernetes 高可用性方面的世界专家之一，他是[Cloud Native Computing Foundation](https://www.cncf.io/)技术监督社区的主席，也是 [Weaveworks](https://www.weave.works/) 的首席执行官，在他今年晚些时候去芬兰履行强制兵役之前，kld strm 正在那里做一些 Kubernetes 的合同工作。这次采访发生在 12 月的 Kubecon + CloudNativeCon 2017。

**人们经常提起“多主”和“高可用性”。你有什么看法？**

高可用性和多主之间是有区别的。例如，如果您有三个主服务器，并且只有一个 Nginx 实例在这些主服务器前面进行负载平衡，那么您就有了一个多主服务器集群，但不是一个高度可用的集群，因为您的 Nginx 可能会在任何时候停机，好吧，就是这样。

**高可用性正在消除单点故障？**

是的，基本上，这意味着消除群集中的单点故障。当我有一个高度可用的集群时，我应该能够失去一些主服务器。

这是高可用性 Kubernetes 的基本定义。但是 Kubernetes 很大，根据定义，有很多组件可能会出现故障。kube-dns 就是一个很好的例子。假设我们有一个普通的集群，它有多个主服务器、多个 etcd 副本，但仍然只运行一个 kube-dns。因此，如果运行 kube-dns 的主服务器出现故障，您的集群将会经历某种中断，因为现在突然之间，您的所有服务发现查询都可能无法解决。因此，我们真的需要更深入地分析我们在群集中的关键组件，然后尝试消除它们的单点故障。

例如，对于 etcd，我们需要至少有三个才能形成法定人数，然后我们可以失去一个。如果我们失去了一个，我们仍然可以继续，领导人选举将会发生(选择一个新的主人)，他们中的一个将会接管。但是，它不再对失败有弹性。如果我们有三个群集，其中一个已经消失，现在我们有 66%的群集覆盖率。但是，如果我们再失去一个，它将只有 33 %,并将进入只读模式，此时可用性低于 50 %,基本上不允许任何更改。

三个人的法定人数是一。我们能承受一次失败。如果我们将它扩展到 5 个，我们可以从两个故障节点中恢复过来。

因此，使用多个活动的领导者选举的对等体是使组件高度可用的一种方式。但是，我们还有控制器管理器和调度器等组件，它们以主动/被动模式运行。例如，如果我们有五个控制器管理器在运行，其中四个是被动的，一个是主动的。因此，如果活动服务器一旦关闭，这四个服务器中的一些将会争用 API 服务，“嘿，我是应该接管的那个。”当他们获得锁时(使用端点或 ConfigMap Kubernetes 对象)，他们将进行操作。

在确保至少有一个实例启动并运行方面，处理 etcd 有什么不同吗？

在高可用性环境中管理 etcd 确实非常具有挑战性。假设您有三个节点，并且处于可信环境中。他们都加入了，一切都很好，我们在一个快乐的世界，然后其中一个倒下了。例如，在云环境中，当一个节点启动时，它可能具有不同的 IP，并得到类似“这是一个新节点”的触发器，而不是意识到它是同一节点，只是暂时不可用。这可能很有挑战性，至少在管理 etcd 时必须考虑到这一点，然后应该添加新节点并删除旧节点，因为 etcd 将它们视为不同的对等节点。那么升级，当然，显然是具有挑战性的。理想情况下，您应该能够在升级时满足大量请求。

管理 HA etcd 集群并不容易，需要很多有意识的决策和经验。

API 服务器以主动-主动模式运行。例如，如果我们有三个由三个 etcds 支持的 API 服务器，那么在任何给定的时间点，所有的 API 服务器都将是活动的。

**当你说主动-主动时，你到底是什么意思？**

假设我们在 API 服务器或多个弹性负载平衡器前使用 DNS，而不是单点故障场景。主动-主动意味着 API 服务器不会休眠，因为它可以在任何给定的时间点接收请求。

它可能总是服务于工作负载。与主动-被动相比，我们有五个调度程序，但其中只有一个在运行。

**这一切听起来非常复杂。有什么方法可以简化管理所有这些的过程吗？**

特殊兴趣小组(SIG)集群生命周期每周都会讨论这个问题:我们如何简化安装过程？如何让 Kubernetes 在 HA 模式下运行更容易？这样做时，需要考虑哪些关键问题？因此，我们将在 Kubernetes 官方文档上提供 HA 文档。

**kube ADM 如何提供帮助？**

kubeadm 是一个工具，您可以使用它从有机器的地方转到有 Kubernetes 集群的地方。kubeadm 的范围很小，因此它可以适用于很多地方。kubeadm 只是一个工具，它把你从没有 Kubernetes 带到 Kubernetes，并把集群缝合在一起。

是的，通过 kubeadm，我们制作了这个工具来减轻痛苦，也就是说，统一这个领域中存在的部署差异。

[在采访时]CNCF 有 23 个被批准的发行。他们在各方面都不同。它们在某种程度上都在复制相同的代码，对吗？他们都在安装 Kubernetes，并且有许多方法可以配置它，但是在一天结束时，您可能会以几乎相同的方式运行它们。所以 kubeadm 承担了这个任务，将这段公共代码统一到一个地方，并且它也是可扩展的。如果你不想要`kubeadm init`做的所有事情，你可以使用小工具箱`kubeadm phase`工具箱，它提供了一个接口来执行原子阶段(当引导一个 Kubernetes 集群时的逻辑工作项)——

**原子？**

是的，一个原子的工作，像生成它的证书或什么的。任何 Kubernetes 集群共有的工件都将包含在 kubeadm 阶段工具箱中。这就是 kubeadm 的计划。如果您打算在您的环境中设置一个高可用性集群，那么在手工部署时，您必须考虑很多事情。kubeadm 将在这方面提供很大帮助，因为这样您就不必考虑“我如何设置我的集群？我如何从没有 Kubernetes 到 Kubernetes？”你现在可以依靠 kubeadm 来完成这一部分。对于使用 kubeadm 的 HA 来说，现在你仍然必须做更高级的事情，比如，“嗯，我必须在所有三个 masters 上运行`kubeadm init`”，但这比必须手动做*所有事情*要好。

通过引导 HA etcd 集群，在所有主机上带外执行`kubeadm init`,并在前端设置某种高度可用的代理，例如使用 DNS，您可以使用 kubeadm 非常顺利地从头开始创建 HA Kubernetes 集群。作为 1.9 版 SIG 集群生命周期工作的一部分，我们正在记录如何做到这一点。

对于内部 DNS 服务(kube-dns 或 CoreDNS ),您必须考虑我前面谈到的中断场景。内部 DNS 服务 IP(通常以-10 结尾)对端点设备的 DNS 流量进行负载平衡。您应该为后备 pod 设置自动缩放，并要求副本数量大约为 3。您可以在 pod 上设置类似反相似性的东西，这样您就可以确保不会在同一个主服务器上运行所有(比如说五个)实例，因为它可能会停机并出现(小)中断(Kubernetes 稍后会自动修复 pod)。所以好的一面是，我们可以很容易地用 Kubernetes 做到这一切，而且我们可以免费获得许多功能。

**你和一直在做 Kubernetes 高可用性的人聊过吗？**

是啊。我肯定与经营 HA Kubernetes 的人有过互动——我不知道我是否能说出任何人的名字，甚至能记住个别案例，但的确，人们会这么做。但是，请注意，Kubernetes 中还有一些与 HA 相关的错误需要修复。问题是我们的端到端测试没有覆盖高可用的 Kubernetes，这是我们测试基础设施中的一个大洞。

据报道，在高可用性模式下运行 Kubernetes 存在一些问题。API 服务器/etcd 通信中有一个 gRPC 错误，导致 HA Kubernetes 在某些情况下失败。有一个 Kubernetes 问题正在跟踪这个问题，但正如所说，我们必须增加我们在这个领域的覆盖范围，因为现在在某些情况下，如果您在 HA 模式下运行 Kubernetes v1.9，您可能会崩溃。[编者按:问题已在 1.10 版中修复]

**在考虑 Kubernetes 高可用性时，您还会考虑哪些我们没有讨论过的问题？**

如果您在云上运行，您可能还需要考虑云提供商提供的不同可用性区域。您可能不希望将所有东西都放在同一个可用性区域中，因为在 Kubernetes 上运行的一些云提供商停机应用程序不会被重新安排到一个健康的区域，所以该区域中的虚拟机可能会出现故障。

另一件值得一提的事情是，关于 Kubernetes 的工作方式，有一个非常普遍的误解，那就是如果你的控制飞机坠毁了，什么都不会发生。许多用户认为，“我真的需要高可用性，因为如果我没有它，如果一个进程失败，我的所有应用程序都会崩溃。”事实并非如此。Kubernetes 以一种巧妙的方式设计了什么，你可以基本上失去你的主人，但你所有的工作负载仍然照常运行。唯一的问题是您的集群进入只读状态，因为当 API 服务器关闭时，您不能向集群写入新的状态。但是如果你只是重新启动你的主人，一切仍然会顺利进行。

这是一个非常聪明的设计。我认为这个灵感来自飞机，在飞行过程中有可能打开和关闭引擎或类似的东西。

这也告诉我们，您可能需要 HA，但也可能不需要。

例如，如果您有三个节点，您可能不需要三个主节点，因为对于一个小的 CI/CD 系统，您可能能够很好地处理一个小时的 API 服务器中断。

那太迷人了。是的，这就提出了一个问题，你是否真的需要 HA？

没错。因为在进入高可用性模式时会增加太多的复杂性，所以您应该再三考虑这种复杂性是否值得，而不是盲目地说服自己需要 HA。成本效益是与复杂性一起考虑的另一个参数。**

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>