# 教程:用 Unix 管道驯服您的访问日志

> 原文：<https://thenewstack.io/tutorial-tame-your-access-log-with-unix-pipes/>

上个月，开发人员社区注意到了 Jessie Frazelle 的一篇博客文章，她是一位受人尊敬的程序员，曾在 Docker、Google 和微软等知名公司工作过，并将自己描述为“所有 Linux 容器的黑客”在博文中，Frazelle 解释了[为什么她欣赏 Unix 的 pipe 命令](https://blog.jessfraz.com/post/for-the-love-of-pipes/)，称其为“卓越的软件设计……能够激发创造力，重视简单性，并且不会将用户限制在一个盒子里。”

“程序简单并且能做好一件事，就容易被破解。不过，美妙之处在于，与|这样的运营商相结合，该计划成为一个更大计划中的一步。”

我不得不同意。我的一个朋友曾经开玩笑说，你可以为 Unix 中的任何东西编写一个命令——但是那个命令可能有六行长，用管道连接一长串。

让我分享我自己最喜欢的真实世界的例子——使用管道从典型的 web 服务器的访问日志中提取一个非常方便的报告——一系列命令实际上有它们自己丰富多彩的历史。

## 半个世纪的管道？

[道格·麦克洛伊](https://www.cs.dartmouth.edu/~doug/)，从 1965 年到 1986 年领导贝尔实验室著名的计算技术研究部门，在 50 多年前首先提出了管道的想法。他在 1964 年写道“我们应该有一些耦合程序的方法，就像一个花园软管一样，”[——他实际上是在打字机上敲出来的——他说这种方法可以让程序员“在需要以另一种方式处理数据时，拧入另一个程序段。”](http://doc.cat-v.org/unix/pipes/)

它直到 1973 年才真正实现，但是麦克洛伊认为这是导致 Unix 哲学的发展，它将强大的个人工具结合起来，每个工具做好一件事。

13 年后的 1986 年，当他吹捧 pipes 是数学传奇人物 Donald Knuth 编写的 10 页 Pascal 程序的高级解决方案时，他仍然在想这个问题。

Knuth 的程序报告了文本文件中最常用的单词，并发表在计算机械协会通讯中 Jon Bentley 的“编程珍珠”专栏中。道格·麦克洛伊写了一封信来回应称赞 Knuth 聪明的专栏——然后他指出,[同样的事情只用六个 Unix 命令通过管道](http://www.leancrew.com/all-this/2011/12/more-shell-less-egg/)连接就可以复制。

`tr -cs A-Za-z '\n' |
tr A-Z a-z |
sort |
uniq -c |
sort -rn |
sed ${1}q` 

“UNIX 老手本能地知道如何在瞬间解决这个问题……”麦克洛伊写道，并补充说“上面给出的简单管道将足以立即得到答案，而不是下周或下个月。”

当我读到这个例子时，我很激动，因为我自己也偶然发现了麦克洛伊的部分解决方案——有用的三命令组合`sort | uniq -c | sort -rn`

## **管道与您的访问日志**

多年来，我一直使用相同的组合来处理来自我的 web 服务器的访问日志的数据(几十年来,一直遵循一种标准化的格式，包括每个请求的日期、时间和时区，以及它的起始 IP 地址和关于推荐者的任何信息)。

`68.186.130.95 - - [15/Feb/2019:08:18:46 -0800] "GET /sloop-john-b.htm HTTP/1.1" 200 2106 "https://www.google.com/" "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit
/537.36 (KHTML, like Gecko) Chrome/72.0.3626.96 Safari/537.36"`

我想知道的是是否有人给我的网站带来流量——以及在某一天哪些页面最受欢迎。

救援管道…

这有点像组装的，但是我注意到被访问的文件的名称总是第七个文本块(用空格分隔)。Unix 的 *awk* 命令确实可以让您打印出一行中的特定部分，在定义(用 F 标志)分隔这些部分之后。在我的例子中，那是一个空格。

``cat access.log | awk -F" " '{print $7}'``

这个命令给出了前一天访问过的每个文件的列表——将它们显示在我的屏幕上。显然，我可以将它们输入 Unix 的 less 命令，一次读取一屏——但是我真正想要的是一个只列出今天访问过的每个文件的报告——每个文件只列出一次*——理想情况下，从访问最多到访问最少排序(并显示计数)。*

 *幸运的是，这正是`sort | uniq -c | sort -rn` 所做的。sort 命令将整个列表按字母顺序排列——这允许 uniq 命令删除(同时计数)所有相邻的重复项。这个列表的唯一问题是它没有按照从大到小的顺序进行排序，但是通过 sort 命令第二次输出可以很容易地解决这个问题，这一次使用它的 r 标志表示“逆序”，最大的数字优先。

``cat access.log | awk -F" " '{print $7}' | sort | uniq -c | sort -nr``

然后将所有内容输入到 *more* 命令中，这样你就可以一次读满屏幕。

``cat access.log | awk -F" " '{print $7}' | sort | uniq -c | sort -nr | more``

或者，如果您只想查看最近的热门话题，可以将所有内容都输入到 *tail* 命令中。

``access.log | awk -F" " '{print $7}' | sort | uniq -c | sort | tail -20``

这是这些管道命令字符串中未被欣赏的乐趣之一。你可以改变一个参数，而你的光荣管道的其余部分仍然工作——这使得它非常容易实验替代输出。例如，我可以通过改变 awk 命令打印出的访问日志的哪一部分来生成同样有用的报告。比方说，我想看看哪个网站发送了最多的推荐，而不是哪些页面被浏览了最多。只需用 *$print 11* 替换 *print $7*

``cat access.log | awk -F" " '{print $11}' | sort | uniq -c | sort -nr | more``

例如，如果你只想看到来自谷歌的流量，这是很方便的。

有趣的实验才刚刚开始。这条管道最终成为一种存根，我用它来生成其他几个有用的日志处理报告。我编写了脚本，使用 grep 来查找对某个特定页面的访问——然后将它们放入我的 Unix bin 中，并给它们一个别名，这样，在命令行中键入一个单词，就会神奇地显示出相应网页今天被访问的频率报告。

``grep 'pagename' access.log | awk -F" " '{print $11}' | sort | uniq -c | sort -r | more``

有时我也会搜索 **access.log.0** (和 access.log 一起)来得到一份 48 小时的报告，其中也包括昨天的流量。

在我看来，烟斗的唯一问题是那种“沉醉于权力”的感觉。很快我就用 Perl 脚本复制了同样的功能，然后把它们都放在我的网络可访问的 cgi-bin 中，这样我就可以用我的网络浏览器访问我自己开发的实时网站分析。然后，我将它与 Firefox 的关键字功能结合起来，这样只需在浏览器中键入“hits”一词，而不是一个 URL，就可以将我带到那个 CGI 脚本的 URL——这将立即调出最新的报告(从我的访问日志中新抓取的)。

就在这个时候，我意识到我的朋友是对的。

您会有一个强大而不可动摇的信念，无论您想对可用的数据进行何种神奇的转换…

…几乎可以肯定，您可以通过一长串管道命令来实现这一点。

专题图片:Paul Goyette 创作的一堆管道的知识共享照片

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>*