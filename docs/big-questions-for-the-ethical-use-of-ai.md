# 人工智能的道德使用面临重大问题

> 原文：<https://thenewstack.io/big-questions-for-the-ethical-use-of-ai/>

在过去的几年里，机器学习和人工智能领域有了巨大的发展和进步。机器学习不仅支撑着我们许多人使用的各种日常工具，如虚拟助理或在线推荐引擎，还有可能在不久的将来，人工智能将在做出医疗诊断、T2 发现新药、T4 创造新材料等方面发挥更大的作用。

但是尽管有所有这些显著的可能性，人工智能也有可能弊大于利——有时是无意的，如当[人工智能做出有偏见的预测](https://thenewstack.io/when-ai-is-biased/)时，或者有时是故意的，当它被用来[制造错误信息](https://thenewstack.io/deep-learning-ai-tool-identifies-fake-news-with-automated-fact-checking/)或[说服 deepfakes](https://thenewstack.io/synthesize-fake-obama-video-artificial-neural-networks/) 。这种令人担忧的发展凸显了越来越多的关于如何道德地使用人工智能的讨论和监管的需求。

这一点现在尤其重要，因为一方面，人工智能模型正变得越来越强大，并在金融、商业和医疗保健等各个领域的决策过程中发挥着越来越有影响力的作用。

> 如果这种偏见在改变生活的决策基本上自动化的未来不受挑战，它可能会为“社会的行政和计算重组”铺平道路，削弱公民社会，使其成为威权主义生长的沃土。

如果不考虑人工智能如何开发和实施的伦理影响，未来的人工智能很可能会侵蚀隐私，帮助传播错误信息，并进一步恶化我们社会中已经存在的不平等。随着人工智能伦理问题的不断发展，这里有一些让专家和观察家都感到担忧的主要问题。

## 艾的偏见问题

虽然机器本身没有偏见，但研究问题的框架、数据的收集和解释以及机器学习模型的训练方式最终会影响人工智能模型如何做出预测。有时候，当数据以不均衡的方式收集时，或者当模型无意中反映了人类创造者的偏见时，这些偏见就会渗透进来。我们看到当这种固有的[算法偏见](https://thenewstack.io/hidden-gender-racial-biases-algorithms-can-big-deal/)影响信誉、雇用、录取甚至谁能假释谁不能假释的自动决策时会发生什么——导致不公平和歧视性的做法，否则这些做法将是非法的。

如果这种偏见在改变生活的决策基本上自动化的未来不受挑战，它可能会为“[社会的行政和计算重组](https://venturebeat.com/2021/01/22/center-for-applied-data-ethics-suggests-treating-ai-like-a-bureaucracy/)”铺平道路，削弱公民社会，使其成为威权主义增长的沃土。

为了解决这个问题，一些专家呼吁更广泛地使用[去偏置算法](https://www.zdnet.com/article/ai-bias-in-hiring-loans-dating-this-search-tool-aims-to-create-a-level-playing-field/)，而其他人在他们的建议中采取了更广泛的观点，以更新适用于数字技术的民权法律。也有可能实现“[监管沙箱](https://www.law.kuleuven.be/citip/blog/the-shifting-sands-of-regulatory-sandboxes-for-ai/)”来测试人工智能，以避免剥削来自边缘化社区的测试人员，并更广泛地“[去殖民化](https://thenewstack.io/researchers-look-at-how-algorithmic-coloniality-may-hamper-artificial-intelligence/)”该行业。除了在更广泛的公众中提升算法素养之外，一些人倡导建立一套公司和机构可以采用的自我监管最佳实践。

## 对可解释的人工智能的需求

正如一些人正确地指出的那样，许多围绕人工智能的伦理问题源于这样一个事实，即模型经常像某种不透明的“黑匣子”一样运行，以至于甚至它们的创造者也不完全确定为什么模型会做出它们所做的决定。因此，有必要对如何使人工智能更加透明进行更多的研究——换句话说，创造出可以被[解释的人工智能。其中模型的底层机制是透明的)并且更加](https://thenewstack.io/researchers-build-an-interpretable-ai-that-shows-how-it-thinks/)[可解释](https://thenewstack.io/how-human-trust-varies-with-different-types-of-explainable-ai/)(即。能够知道预测背后的“为什么”)。如果人工智能研究人员从一开始就考虑到可解释性和可解释性，从长远来看，这种努力可能有助于培养公众对机器学习系统的信任。

## 人工智能驱动的大规模监控

目前，围绕人工智能的最大伦理问题之一是隐私的丧失和自动监控的兴起。虽然大多数人都会同意使用人工智能追踪和识别罪犯是一件好事，但这种好处的另一面是，政府和执法部门正在使用这些相同的技术来收集大量的生物特征数据。这种做法是一种滑坡，否则守法的公民可能会因为行使自由发表言论或抗议政府的权利而成为目标并受到监控，从而导致潜在的滥用，如“[对身份盗窃的普遍跟踪和监控](https://thenewstack.io/clearviews-controversial-facial-recognition-ai-automates-mass-surveillance/)”——所有这些都是大规模自动化的。这一切都表明需要更强有力的监管，人工智能公司需要将隐私和安全措施作为一个基本组成部分，而不是在出现问题后做出反应*认错*。

## 大技术和“道德清洗”

也许没有什么比大型科技公司如何处理道德人工智能问题更令人不安的了。虽然所有的科技巨头似乎都在人工智能的道德使用上采取了某种自我监管措施——例如[接受道德宪章或建立道德委员会](https://www.theverge.com/2019/4/3/18293410/ai-artificial-intelligence-ethics-boards-charters-problem-big-tech)——但业内一些人怀疑这些行动是否是真诚的，或者只是一种“道德清洗”的策略，以避免更严格的政府监管。

作为大型科技对人工智能伦理的潜在问题的一个例子，人们可以看看谷歌在其伦理委员会上解雇[两名人工智能研究人员](https://www.theverge.com/2021/2/19/22292011/google-second-ethical-ai-researcher-fired)的争议，此前他们发表了[对大型自然语言处理模型的危险](https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/)的研究警告。另一个麻烦的例子来自亚马逊，出于隐私考虑，[宣布暂停](https://thenewstack.io/industry-facial-recognition-ai-moratoriums-dont-address-flaws-privacy-concerns/)向执法机构出售其面部识别技术，但上周有报道称，它计划在其送货车辆上安装使用人工智能技术的[“永远在线”摄像头。](https://www.theverge.com/2021/2/3/22265031/amazon-netradyne-driveri-survelliance-cameras-delivery-monitor-packages)

这种不一致似乎表明，大公司对自我监管行动和道德委员会的热情可能不是出于对人工智能应该被负责任地使用的真诚关注，而是更多地作为一种公关策略来保护他们的底线，并阻止监管监督。

最终，在人工智能的道德使用方面有许多复杂的问题需要解决:谁来决定什么被认为是道德的，什么不是？如何才能保证算法和数据被公平使用？公司和政府能做些什么来加强透明度和问责制？短期内没有简单的答案，但我们现在就问这个问题是至关重要的。

图片:cottonbro via Pexels

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>