# 你的 Kubernetes 控制平面有多少个节点？

> 原文：<https://thenewstack.io/how-many-nodes-for-your-kubernetes-control-plane/>

一个 [Kubernetes](https://thenewstack.io/category/kubernetes/) 集群通常由两类节点组成:运行应用程序的工作节点和控制集群的控制平面节点——在工作节点上调度作业，在负载需要时创建 pod 的新副本，等等。

控制平面运行允许群集提供高可用性、从工作节点故障中恢复、响应增加的 pod 需求等的组件。因此，确保控制平面的高可用性在生产环境中至关重要。

如果没有一个功能完整的控制平面，集群就无法对其当前状态进行任何更改，这意味着无法安排新的 pod。(您可以在控制平面节点上调度 pod，但不建议生产集群使用，因为您不希望工作负载需求占用使 Kubernetes 高度可用的组件的资源。您还消除了漏洞使工作负载能够访问控制平面秘密的机会，这将授予对集群的完全访问权。)

那么，如何确保控制平面高度可用呢？Kubernetes 通过在多个节点上复制控制平面功能来实现高可用性。但是应该使用多少个节点呢？

## 愿机会永远对你有利

这并不像“越多越好”那么简单控制平面的功能之一是提供用于配置 Kubernetes 本身的数据存储。该信息作为键值对存储在 etcd 数据库中。 [Etcd](https://etcd.io/) 使用仲裁系统，要求在提交任何数据库更新之前有一半以上的副本可用。因此，双节点控制平面不仅要求一个节点可用，还要求两个节点都可用(因为 2 的“一半以上”的整数是…2)。也就是说，从单节点控制平面到双节点控制平面会降低可用性，而不是提高可用性。

在双节点控制平面的情况下，当一个节点无法连接到另一个节点时，它不知道另一个节点是死的(在这种情况下，这个幸存的节点可以继续更新数据库)还是不可连接。如果两个节点都已启动，但无法相互访问，并且继续执行写入操作，则最终会出现裂脑情况。

现在，集群的两个部分拥有不一致的数据副本，无法协调它们。因此，更安全的情况是锁定数据库，防止任何节点进一步写入。而且，因为一个节点死亡的几率在两个节点的情况下比在一个节点的情况下高(实际上是两倍，假设它们是相同的节点)，所以两个节点的控制平面的可靠性比单个节点的差！

随着控制平面节点的扩展，这一逻辑同样适用— etcd 将始终要求一半以上的节点处于活动状态且可访问，以获得仲裁，这样数据库就可以执行更新。

因此，2 节点控制平面要求两个节点都开启。一个 3 节点控制平面也需要 2 个节点处于运行状态。一个 4 节点控制平面需要 3 个节点处于运行状态。因此，4 节点控制平面的可用性比 3 节点控制平面差，因为虽然两者都可能遭受单节点故障，并且都无法处理 2 节点故障，但在 4 节点群集中发生这种情况的几率更高。将一个节点添加到奇数大小的集群看起来更好(因为有更多的机器)，但是容错性更差，因为完全相同数量的节点可能会失败而不会失去仲裁，但是会有更多的节点失败。

因此，一般规则是在控制平面中总是运行奇数个节点。

## 过犹不及？

所以我们需要奇数个节点。这意味着 3 个节点，还是 5 个，还是 23 个？

etcd [文档说](https://etcd.io/docs/v3.5/faq/#what-is-maximum-cluster-size)“…一个 etcd 集群可能不应该有超过 7 个节点…一个 5 成员的 etcd 集群可以容忍两个成员的故障，这在大多数情况下已经足够了。尽管较大的群集提供了更好的容错能力，但写入性能会受到影响，因为数据必须在更多的机器上复制。”

Kubernetes [的医生说](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#scaling-up-etcd-clusters)“强烈建议在任何官方支持的规模下，为生产 Kubernetes 集群运行静态的五成员 etcd 集群。当需要更高的可靠性时，合理的扩展是将三个成员的集群升级到五个成员的集群。”

因此，这些文档都暗示扩展 etcd 集群只是为了容错，而不是为了性能，事实上，扩展成员会降低性能。

我们来看看这是不是真的。为了测试这一点，我在两台不同大小的机器上创建了 Kubernetes 集群，其控制平面的大小从三个节点到九个节点不等。在所有情况下，我都使用“堆叠式”etcd 拓扑，其中 etcd 服务在每个控制平面节点上运行。(与外部 etcd 拓扑相反，Kubernetes 控制平面和 etcd 集群使用单独的服务器。)

所有测试都是使用 Talos Linux v 1 . 2 . 0-alpha 1 完成的，它安装了 Kubernetes v1.24.2 和 etcd v 3 . 5 . 4，使用 AWS c3.4xlarge 实例运行测试。

### 结果

运行的测试取自 etcd 的[性能](https://etcd.io/docs/v3.5/op-guide/performance/)文档，使用 [etcd 基准工具](https://github.com/etcd-io/etcd/tree/main/tools/benchmark):

给领导写信:

```
go run  ./benchmark  --endpoints=$ENDPOINT   --target-leader   --conns=100  --clients=1000  put  --key-size=8  --sequential-keys  --total=100000  --val-size=256   --cert  ./certs/admin.crt  --key  ./certs/admin.key  --cacert  ./certs/ca.crt

```

给所有成员写信:

```
go run  ./benchmark  --endpoints=$ENDPOINTS  --conns=100  --clients=1000  put  --key-size=8  --sequential-keys  --total=100000  --val-size=256  --cert  ./certs/admin.crt  --key  ./certs/admin.key  --cacert  ./certs/ca.crt

```

线性化并发读取请求:

```
go run  ./benchmark  --endpoints=$ENDPOINTS   --conns=100  --clients=1000  range YOUR_KEY  --consistency=l  --total=100000   --cert  ./certs/admin.crt  --key  ./certs/admin.key  --cacert  ./certs/ca.crt

```

读取仅具有顺序一致性的请求(由任何 etcd 成员服务，而不是法定成员，以交换可能提供的陈旧数据):

```
go run  ./benchmark  --endpoints=$ENDPOINTS     --conns=100  --clients=1000  range YOUR_KEY  --consistency=s  --total=100000   --cert  ./certs/admin.crt  --key  ./certs/admin.key  --cacert  ./certs/ca.crt

```

每条记录显示了每项测试连续 3 次运行的每秒操作数，取平均值:

随着群集的扩展，除了向领导者提交写入以外的所有操作的性能都会提高。但是，写入领导者是最关键的操作，必须提交才能确保群集的完整性。随着集群的扩展，此类写入的性能会降低，因为它必须将它们提交给越来越多的成员。

从 3 节点 etcd 集群迁移到 5 节点集群会降低 4%到 5%的性能；7 节点集群会再减少 4%，9 节点集群会再减少 4%或 5%，总吞吐量比 3 节点集群低 13%。

因此，对于给定大小的控制平面计算机，3 节点群集将提供最佳性能，但只能容忍一个节点出现故障。对于大多数环境来说，这已经足够了(假设您有很好的监控和流程来及时处理故障节点)，但是如果您的应用程序需要非常高的可用性，并且能够同时容忍 2 个控制平面节点故障，那么 5 节点集群只会带来大约 5%的性能损失。

## 自动缩放怎么样？

现在应该很明显，etcd 集群的自动伸缩，即增加更多节点来响应高 CPU 负载，是一件坏事。正如我们从基准测试中看到的，向集群添加更多节点会降低性能，因为需要在更多成员之间同步更新。此外，自动扩展还会使您面临这样的情况:您可能会运行偶数个集群成员，至少在扩展操作发生时是短暂的，从而增加了节点故障影响 etcd 可用性的可能性。

事实上，Kubernetes 官方文档明确指出:

“一般规则是不要扩大或缩小 etcd 集群。不要为 etcd 集群配置任何自动扩展组。强烈建议始终为任何官方支持规模的生产 Kubernetes 集群运行静态的五成员 etcd 集群。”

使用自动缩放组从控制平面节点的故障中恢复，或者用具有更多 CPU 能力的控制平面节点替换控制平面节点是合理的(这就是亚马逊 Web 服务的 EKS 在谈论[控制面板自动缩放](https://aws.amazon.com/blogs/containers/amazon-eks-control-plane-auto-scaling-enhancements-improve-speed-by-4x/)时的意思)。然而，在更换控制平面成员时，甚至是发生故障的控制平面成员时，也需要注意一些细微之处——这不仅仅是添加一个新节点那么简单！

## 在紧急情况下，打碎玻璃，移除故障节点，然后添加新节点

从表面上看，添加一个新节点然后删除故障节点似乎与删除故障节点然后添加一个新节点是一样的。然而，前一种情况的风险更大。

要了解原因，请考虑一个简单的 3 节点集群。一个 3 节点群集的仲裁数为 2。如果一个节点出现故障，etcd 集群可以继续使用剩余的两个节点。但是，如果您现在向群集中添加一个新节点，quorum 将增加到 3，因为该群集现在是一个 4 节点群集，算上关闭的节点，我们需要一半以上的可用空间来防止裂脑。

如果新成员配置错误，并且无法加入集群，那么现在就有两个失败的节点，集群将会关闭并且不可恢复。因为只有两个节点启动，所需的法定人数为 3。

将此与首先移除故障节点进行比较。一旦我们删除了故障节点，我们现在就有了一个双节点集群，一个双节点仲裁，两个节点启动(因此我们不能容忍任何进一步的故障，但我们可以在这种状态下正常运行。)如果我们现在添加一个节点，创建一个 3 节点集群，quorum 仍为 2。如果新节点未能加入，我们的 3 节点集群中仍有 2 个节点，并且可以再次删除和重新添加新节点。

关键要点是，当 etcd 成员节点出现故障时，在尝试用新节点替换它之前，从 etcd 中删除故障节点。

Kubernetes 文档[在这里](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#replacing-a-failed-etcd-member)记录了制定这个的过程。然而，如果你运行的是 Talos Linux，它是专门为 Kubernetes 设计的[操作系统，那么这个过程就简单多了。Talos Linux 有助手功能，可以自动移除关闭的 etcd 节点:](https://www.siderolabs.com/platform/talos-os-for-kubernetes/) 

```
talosctl etcd remove-member     ip-172-31-41-76

```

```
kubectl delete node   ip-172-31-41-76

```

然后，您可以添加一个新的控制平面节点，对于 Talos Linux，这与使用用于创建其他控制平面节点的 *controlplane.yaml* 引导一个新节点一样简单。

还应该注意的是， [Talos Linux](https://www.talos.dev/) 使用 etcd 的学习者特性——所有新的控制平面节点作为非投票学习者加入 etcd，直到它们赶上所有事务。这意味着添加额外的节点不会增加仲裁，直到该节点成为可靠成员，然后自动提升为投票成员。但是，这种增加的安全性并没有改变在添加新节点之前删除故障节点的建议。

### 首先添加新节点，然后删除要替换的节点

要升级仍在运行的控制平面节点，并将其替换为具有更多 CPU 或内存的机器，操作顺序与节点出现故障时相反，即首先添加新节点，然后移除旧节点。

为了理解原因，考虑一个 3 节点 etcd 集群的情况，其中您希望将一个节点升级到更快的硬件。仲裁将需要 2 个节点启动，以便群集继续处理写入。

一种方法是首先移除要替换的节点。这将留下一个双节点集群，仲裁数为 2。然后添加新节点。这将使群集返回到仲裁数为 2 的 3 节点群集，但在过渡期间，没有容错能力—不合时宜的故障可能会使群集停止运行。

另一种方法是先添加新节点，创建一个 4 节点集群，其仲裁数为 3，然后删除一个节点。这将使群集返回到仲裁数为 2 的 3 节点群集，但在过渡期间，群集可以容忍一个节点出现故障。

因此，如果要替换的节点是可操作的，则在移除该节点之前添加新节点会更安全。

执行此操作的程序记录在 Kubernetes 网站[这里](https://etcd.io/docs/v3.6/op-guide/runtime-configuration/#remove-a-member)。Talos Linux 再次简化了这一过程:

*   通过使用 *controlplane.yaml* 文件引导来添加新的控制平面节点。
*   告诉被替换的节点离开集群:
    *talosctl-n 172 . 31 . 138 . 87 reset*
*   *kubectl 删除节点*

" *talosctl 复位*"导致节点被擦除。Talos 知道节点何时是控制平面节点，如果是这样，它将在重置时优雅地离开 etcd。

## 摘要

总结一下 Kubernetes 控制平面的规模和管理:

*   使用三个或五个控制平面节点运行集群。对于大多数用例来说，三个就足够了。五个将为您提供更好的可用性，但在所需的节点数量方面成本更高，而且每个节点可能需要更多的硬件资源来抵消大型集群中出现的性能下降。
*   实施良好的监控并建立流程，以便及时处理故障节点(并测试它们！)
*   即使有了可靠的监控和替换故障节点的程序，[备份 etcd](https://www.talos.dev/latest/advanced/disaster-recovery/) 和您的控制平面节点配置，以防范不可预见的灾难。
*   监控您的 etcd 集群的性能。如果 etcd 性能较低，请垂直缩放节点，而不是节点数量。
*   如果某个控制平面节点出现故障，请先将其删除，然后添加替换节点。
*   如果更换未发生故障的节点，请添加新节点，然后删除旧节点。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>