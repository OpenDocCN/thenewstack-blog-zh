<html>
<head>
<title>With Innovative Gaming Moves, Google's AI Becomes Go Grandmaster in 3 Days</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随着创新的游戏移动，谷歌的人工智能在3天内成为围棋大师</h1>
<blockquote>原文：<a href="https://thenewstack.io/innovative-gaming-moves-googles-ai-becomes-go-grandmaster-three-days/#0001-01-01">https://thenewstack.io/innovative-gaming-moves-googles-ai-becomes-go-grandmaster-three-days/#0001-01-01</a></blockquote><div><div id="tns-post-body-content">


<p class="translated">这似乎是很久以前的事了，但就在去年，在这个机器越来越智能的加速时代，一个旨在在古老而复杂微妙的围棋比赛中击败世界上最好的人类选手的人工智能实现了这一壮举。AlphaGo是由总部位于伦敦的人工智能实验室<a href="https://cloud.google.com/kubernetes-engine" class="ext-link" rel="external ">谷歌</a>的子公司<a href="https://deepmind.com/" target="_blank" class="ext-link" rel="external "> DeepMind </a>创造的围棋人工智能，去年当<a href="https://thenewstack.io/alphagos-win-human-go-champion-means-ai/" target="_blank" class="local-link">击败传奇棋手Lee Sedol </a>时，它让许多专家观察者感到惊讶，并在今年早些时候决定性地赢得了围棋世界冠军柯洁的三场比赛，然后退出比赛。</p>
<p class="translated">尽管AlphaGo有着明显的实力，但它依赖人类的专业知识来训练它。人工智能从超过100，000个大师级游戏的数据集学习，此外还从多次与自己玩游戏中收集到改进。然而，开发它的团队表示，使用过去比赛的数据可能会无意中给AlphaGo的表现“强加一个上限”。</p>
<p class="translated">现在，DeepMind表示，它已经开发出了一种更强大的围棋人工智能——alpha Go Zero——它能够学习下大师级别的围棋，而且它可以在完全没有任何人类干预的情况下自主学习。</p>
<p class="translated"><iframe loading="lazy" title="Discovering New Knowledge" src="https://www.youtube.com/embed/mJ4tEDMksWA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p>
<h2 class="translated">没有人类数据</h2>
<p class="translated">根据该公司最近发表在《自然》<a href="https://www.nature.com/articles/nature24270.epdf?referrer_access_token=227wNPPNRfgwssJ8LgtNkdRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-22SehS6IfIWP6NGb0V5cWu-EoVfAGki0u6km0LszOuzp5t4V7Cfz3E7s7jHnzbQ4qzuSk0MignCMZouLbzvn5mW4_ZI9kEVTtgqcdzq12Y3aiXmloMr_KNd0zP9S3BBjnv3VhV01dC25xz0WByl2IGns6O4YjkOFYnAHY3lwSzlCMDAtdapoeGE79u0gdfakFNmPSEa8mtT10pVafTZIW6Q%3D%3D&amp;tracking_referrer=www.theverge.com" target="_blank" class="ext-link" rel="external "><em/></a>上的发现，AlphaGo Zero一开始只精通围棋的基本规则，并且能够达到特级大师的水平——只用了<em>三天</em>。</p>
<p class="translated">它通过跳过AlphaGo早期版本使用的人类生成的训练数据来实现这一点，即来自人类业余爱好者和专业人士玩的数千场游戏的游戏数据，这些数据被输入到系统中。相反，AlphaGo Zero通过与自己对弈来学习掌握游戏，使用一种新形式的强化学习来有效地成为自己的老师。通过这样做，AlphaGo Zero甚至能够以100比0击败其机器前辈，这促使DeepMind团队将其称为“可以说是历史上最强的围棋选手”。</p>
<p class="translated"><img decoding="async" loading="lazy" class="aligncenter size-full wp-image-3412434" src="../Images/9cd613d29afbfb48b64a8b9b187cb4fc.png" alt="" data-original-src="https://storage.googleapis.com/cdn.thenewstack.io/media/2017/11/ff45c334-alphago-ratings-7.jpg"/></p>
<h2 class="translated"><em>白板</em>学习</h2>
<p class="translated">AlphaGo和AlphaGo Zero之间有一些有趣的差异，这使得最新的迭代如此强大和高效。正如DeepMind研究人员在他们的论文中解释的那样，<a href="https://www.nature.com/articles/nature24270.epdf?referrer_access_token=227wNPPNRfgwssJ8LgtNkdRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-22SehS6IfIWP6NGb0V5cWu-EoVfAGki0u6km0LszOuzp5t4V7Cfz3E7s7jHnzbQ4qzuSk0MignCMZouLbzvn5mW4_ZI9kEVTtgqcdzq12Y3aiXmloMr_KNd0zP9S3BBjnv3VhV01dC25xz0WByl2IGns6O4YjkOFYnAHY3lwSzlCMDAtdapoeGE79u0gdfakFNmPSEa8mtT10pVafTZIW6Q%3D%3D&amp;tracking_referrer=www.theverge.com" target="_blank" class="ext-link" rel="external "> <em> <span class="current-selection">桅杆</span> <span class="current-selection"> e </span> <span class="current-selection">环th </span> <span class="current-selection">例如</span><span class="current-selection">am</span><span class="current-selection">e of Go wi</span><span class="current-selection">th</span><span class="current-selection">o</span><span class="current-selection">u</span><span class="current-selection">t</span><span class="current-selection">h</span><span class="current-selection">um</span><span class="current-selection">an kn</span><span class="current-selection">o AlphaGo的最初版本有两个深度神经网络——一个是选择下一步棋的“政策网络”，另一个是从每个可能的位置评估和预测游戏获胜者的“价值网络”。 价值网络的任务是预测政策网络与自身博弈的赢家。</span></em></a></p>
<p class="translated">一旦这些神经网络得到训练，它们就会与蒙特卡洛树搜索结合起来——蒙特卡洛树搜索使用重复的随机采样算法来模拟各种行动和反行动的风险、可能的结果和概率。政策网络用于选择具有最佳结果的移动，而价值网络评估相对于树搜索的其余部分的位置。这种方法帮助AlphaGo决定采取哪一步棋，经常用暗示机器中某种幽灵“想象力”的意想不到的举动来惊讶它的人类竞争对手。</p>
<p class="translated">然而，AlphaGo Zero只使用一个神经网络，将政策和价值网络结合成一个网络，以便可以更有效地训练它。这与一种新形式的“自我游戏”强化学习算法结合使用，该算法在训练循环中融入了前瞻搜索功能，这意味着系统能够更快、更一致地学习。这个新系统还取消了前面提到的AlphaGo化身中的<span class="current-selection">蒙特卡罗方法模拟的随机“首次亮相”游戏，而是依靠它自己强大的神经网络来分析移动。</span></p>
<p class="translated">研究人员解释说:“(AlphaGo Zero)下棋时，神经网络会进行调整和更新，以预测下一步棋，以及游戏的最终赢家。”。“这个更新的神经网络然后与搜索算法重新组合，创建一个新的、更强的AlphaGo Zero版本，这个过程再次开始。在每次迭代中，系统的性能都会有少量的提高，自我游戏的质量也会提高，从而导致越来越准确的神经网络和越来越强的AlphaGo Zero版本。”</p>
<p class="translated">最终结果是:通过将人类完全从等式中移除，系统“不再受人类知识限制的约束”，可以学习从一张白纸开始，或<em> tabula rasa </em>，每次都是从世界上最强大的玩家:它自己开始。</p>
<p class="translated">更令人惊讶的是，一旦AlphaGo Zero在大约40天的自我训练和数百万场比赛后超过了之前最强版本所达到的特级大师水平，它随后似乎发展出了完全非传统和意想不到的机动——或者按照该团队的描述，“发现新知识”。AlphaGo Zero甚至放弃了围棋中某些传统的<a href="https://en.wikipedia.org/wiki/Joseki" target="_blank" class="ext-link" rel="external ">开局棋步和序列</a>，这些棋步和序列是人类在2500年的围棋历史中苦心开发的，转而支持自己的创新。</p>
<p class="translated"><img decoding="async" loading="lazy" class="aligncenter size-full wp-image-3412436" src="../Images/21f2a810c5a8aa7c31649f11e78ec41c.png" alt="" data-original-src="https://storage.googleapis.com/cdn.thenewstack.io/media/2017/11/9bcf8ce0-alphago-ratings.png"/></p>
<p class="translated">这里有许多潜在的影响:最重要的是，这种自我学习系统背后的算法很可能是开发一个强大的通用人工智能的一部分，它能够掌握许多不同的领域，就像人类可能做的那样——而不是所谓的只精通一项特定任务的弱人工智能<a href="https://en.wikipedia.org/wiki/Weak_AI" target="_blank" class="ext-link" rel="external "/>。另一个优势是不再需要大型数据集作为此类系统的培训素材，因为这些数据集通常成本高昂或难以获得，或者在某些情况下根本不存在。</p>
<p class="translated">AlphaGo Zero的首席程序员大卫·西尔弗(David Silver)表示:“<em>白板</em>学习对我们在DeepMind的目标和雄心极其重要。“原因是，如果你能实现<em> tabula rasa </em>学习，你就有了一个可以从围棋游戏移植到任何其他领域的代理。你把自己从你所在领域的细节中解放出来，然后你想出了一个通用的算法，它可以应用于任何地方。”</p>
<p class="translated">如果AlphaGo Zero能够在如此短的时间内实现的创新性飞跃是某种指标，那么这种“通用”算法可能会加速发现新的、意想不到的知识，以诊断和治疗疑难杂症，解决日益严重的气候危机，甚至帮助我们获得对宇宙奥秘的一些科学见解。从现在到那时还有很长的一段路要走，但是这样一个人工智能的未来现在看起来更有可能了。</p>
<p class="attribution translated">谷歌是新堆栈的赞助商。</p>
<p class="attribution translated">图片:DeepMind</p>


<div class="tns-logo-slug">
<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 68 31" version="1.1">
<title>Group</title>
<desc>Created with Sketch.</desc>
<g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
<g id="Group">
<path d="M24.002,29.619 L29.77,29.619 L29.77,15.808 C29.77,15.038 29.622,11.265 29.59,10.414 L29.77,10.414 C31.424,14.019 31.473,14.147 32.168,15.322 L39.65,29.618 L44.845,29.618 L44.845,0 L39.075,0 L39.075,11.064 C39.075,12.197 39.075,12.44 39.182,14.472 L39.325,17.468 L39.151,17.468 C39.034,17.267 38.596,16.173 38.467,15.929 C38.164,15.323 37.725,14.512 37.373,13.905 L30.031,0 L24,0 L24,29.619 L24.002,29.619 Z" id="Path-Copy" fill="#FF3287"/>
<path d="M56.948,0 C50.745,0 47.606,3.43 47.606,8.296 C47.606,14.114 51.036,15.404 55.518,17.132 C60.438,18.853 61.782,19.332 61.782,21.539 C61.782,24.225 58.969,24.867 57.401,24.867 C54.579,24.867 52.493,23.342 51.536,20.858 L47,24.185 C49.43,28.937 52.145,30.185 57.713,30.185 C59.364,30.185 62.059,29.74 63.727,28.694 C67.779,26.156 67.779,22.22 67.779,20.898 C67.779,18.129 66.531,16.207 66.178,15.726 C65.049,14.121 63.032,12.918 61.25,12.278 L57.084,10.914 C55.073,10.267 52.928,10.105 52.928,8.019 C52.928,7.707 53.008,5.528 56.288,5.319 L61.465,5.319 L61.465,0 C61.465,0 57.342,0 56.948,0 Z" id="Path-Copy-2" fill="#00AFF4"/>
<polygon id="Path" fill="#00AFF4" points="5.32907052e-15 1.77635684e-15 5.32907052e-15 5.319 7.572 5.319 7.572 29.564 14.132 29.564 14.132 5.319 21.544 5.319 21.544 1.77635684e-15"/>
</g>
</g>
</svg> </div>
</div>
</div>    
</body>
</html>