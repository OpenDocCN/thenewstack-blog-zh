<html>
<head>
<title>Divide and Conquer: Distributed Deep Learning Exponentially More Efficient in Search</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分而治之:分布式深度学习在搜索中效率成倍提高</h1>
<blockquote>原文：<a href="https://thenewstack.io/divide-and-conquer-distributed-deep-learning-exponentially-more-efficient-in-search/#0001-01-01">https://thenewstack.io/divide-and-conquer-distributed-deep-learning-exponentially-more-efficient-in-search/#0001-01-01</a></blockquote><div><div id="tns-post-body-content">


<p class="translated">对于许多人来说，网上购物已经成为一个更好的选择，而不是从一个商店跳到另一个商店，花几个小时寻找某个尺寸或最好的价格。毕竟，网上购物简单方便:输入搜索词，瞧！— 一页又一页的可能产品选择——不用出门就能找到。</p>
<p class="translated">但是你有没有想过当你输入搜索词时，在幕后发生了什么？检索这些搜索结果可能需要什么样的计算资源，尤其是当像亚马逊这样的公司据说在全球销售超过30亿件产品的时候？正如人们可能想象的那样，训练一个神经网络来理解用户的<a href="https://en.wikipedia.org/wiki/User_intent" target="_blank" rel="noopener noreferrer external " class="ext-link">语义意图</a>，然后从如此巨大的搜索空间中搜索、学习并产生相关结果，这将需要大量的计算能力——导致一个系统在未来很难扩大规模，特别是随着越来越多的产品被添加，以及传统的<a href="/farewell-moores-law/" target="_blank">基于硅的硬件难以跟上</a>。</p>
<p class="translated">用专业术语来说，专家称之为极端分类问题，因为这个问题有太多可能的结果。为了解决这个问题，莱斯大学的研究人员设计了一种简单而巧妙的“分治”算法，与其他大规模深度学习技术相比，训练时间快了7到10倍，内存占用少了2到4倍。这些结果是从亚马逊搜索数据集的测试中收集的，该数据集包含大约7000万个查询和超过4900万个产品。</p>
<p class="translated">莱斯大学计算机科学助理教授、<a href="https://arxiv.org/pdf/1910.13830.pdf" target="_blank" rel="noopener noreferrer external " class="ext-link">论文的</a>作者之一、<a href="https://www.cs.rice.edu/~as143/" target="_blank" rel="noopener noreferrer external " class="ext-link"> Anshumali Shrivastava </a>说:“我们利用了极端分类和<a href="https://en.wikipedia.org/wiki/Compressed_sensing" target="_blank" rel="noopener noreferrer external " class="ext-link">压缩感知</a>(一种信号处理技术)的基本联系，提出了一种指数级廉价的训练和推理算法。“最令人兴奋的是，该算法天生具有令人尴尬的并行性。因此，尽管更便宜，它也是无限可扩展的:我们可以最佳地使用计算能力。一般来说，本质上的算法改进很少是硬件友好的。通常情况下，情况正好相反。”</p>
<h2 class="translated">“搜索空间的指数缩减”</h2>
<p class="translated">该算法被称为“通过哈希合并的平均分类器”或MACH，其卓越的效率来自于它使用了一种被称为<a href="https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch" target="_blank" rel="noopener noreferrer external " class="ext-link"> count-min sketch </a>的概率数据结构。在不涉及太多数学细节的情况下，该算法使用<a href="https://en.wikipedia.org/wiki/Universal_hashing" target="_blank" rel="noopener noreferrer external " class="ext-link">通用哈希</a>将大量的类别减少为更少数量的并行和独立的分类任务，处理更少但不变的类别。</p>
<p class="translated">Shrivastava告诉我们:“我们的程序很简单，但非常反直觉，除非你看到它背后的数学。”该过程包括将对象(或产品)随机组合到“<a href="https://en.wikipedia.org/wiki/Bin_(computational_geometry)" target="_blank" rel="noopener noreferrer external " class="ext-link">箱”</a>中，并训练分类器来预测箱，而不是对象。在推断期间，预测的箱概率被组合以识别对象。”</p>
<p class="translated"><img decoding="async" loading="lazy" class="aligncenter size-full wp-image-9894510" src="../Images/76b9a4ff720f3c9d39357dce19f370ac.png" alt="" data-id="9894510" data-original-src="https://cdn.thenewstack.io/media/2020/01/cc1492cc-mach-rice-university.jpg"/></p>
<p class="translated">为了理解这在实际中可能如何工作，该论文的主要作者，Tharun Medini，以网上购物为例，提出了一个有趣的思想实验:“让我们保守估计产品的数量为1亿。传统方法将查询和所有1亿个产品投射到一个密集的<a href="https://en.wikipedia.org/wiki/Vector_space" target="_blank" rel="noopener noreferrer external " class="ext-link">向量空间</a>。为了检索最相关的产品，这些方法对所有产品向量执行最近邻搜索。这一步需要大量的计算，而且通常很浪费。”</p>
<p class="translated">MACH没有对1亿个产品进行培训，而是采取了一种不同的方法，将所有1亿个产品随机放入三个容器中。然后，马赫创造了另一个“世界”，并再次将1亿件产品随机分类到三个不同的箱子中。最值得注意的是，随机排序的1亿个产品的分布在每个世界都是不同的。为每个世界训练一个分类器，以便仅将搜索分配给三个箱，而不是其中的产品。</p>
<p class="translated">“现在，一个搜索被输入到第一个分类器，它显示bin，”Medini继续说道。相同的搜索被馈送到第二分类器，并表示箱1。最可能的类是这两个容器中的公共类。所以我们需要看看这两个箱子之间可能的交集。通过创建六个类(每个世界三个)，我们将搜索空间减少到整个产品空间的1/9(1/3 x 1/3)。如果我们再增加一个有三个箱子的世界，我们就有了1/27的公共搜索空间，而代价只是九个类。通过支付线性成本，我们得到了搜索空间的指数级缩减。因此，我们的方法比以前的方法更有效。”</p>
<p class="translated">该小组目前正在努力扩展MACH，以应对计算密集型挑战，如<a href="/harvards-new-open-source-ai-algorithm-simplifies-protein-folding-puzzle/" target="_blank">预测蛋白质结构</a>或定位基因组序列。正如该团队所指出的那样，由于他们的算法将计算需求降低了几个数量级，因此，它可以在提高任何处理大量可能结果的深度学习模型的效率方面发挥至关重要的作用——无论是在线搜索相关产品，还是处理问题的自然语言处理模型。</p>
<p class="translated">Shrivastava指出:“可能还有其他几个人工智能问题存在类似的减少。”“未来，我们应该睁大眼睛，寻找其他可以让人工智能更高效的基本数学工具。我们不能指望硬件能应付指数级增长的需求。他指出<a href="https://arxiv.org/abs/1906.02243" target="_blank" rel="noopener noreferrer external " class="ext-link">最近的报告</a>表明训练一个人工智能模型“在其一生中可以排放五辆汽车那么多的碳，因此迫切需要找到<a href="https://thenewstack.io/check-your-ml-carbon-footprint-with-the-machine-learning-emissions-calculator/" target="_blank" class="local-link">节能的机器学习算法</a>。保持人工智能发展的最大希望是找到更聪明、更有效的算法。"</p>
<p class="translated"><iframe loading="lazy" title="MACH - Extreme Classification in Log-Memory (NeurIPS 2019 short video)" src="https://www.youtube.com/embed/zHXy-AlzSxQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p>
<p class="translated">阅读团队的<a href="https://arxiv.org/pdf/1910.13830.pdf" class="ext-link" rel="external ">论文</a>，或者在<a href="https://github.com/Tharun24/MACH/" class="ext-link" rel="external "> Github </a>上查看马赫的代码。</p>
<p class="attribution translated">图片:莱斯大学</p>


<div class="tns-logo-slug">
<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 68 31" version="1.1">
<title>Group</title>
<desc>Created with Sketch.</desc>
<g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
<g id="Group">
<path d="M24.002,29.619 L29.77,29.619 L29.77,15.808 C29.77,15.038 29.622,11.265 29.59,10.414 L29.77,10.414 C31.424,14.019 31.473,14.147 32.168,15.322 L39.65,29.618 L44.845,29.618 L44.845,0 L39.075,0 L39.075,11.064 C39.075,12.197 39.075,12.44 39.182,14.472 L39.325,17.468 L39.151,17.468 C39.034,17.267 38.596,16.173 38.467,15.929 C38.164,15.323 37.725,14.512 37.373,13.905 L30.031,0 L24,0 L24,29.619 L24.002,29.619 Z" id="Path-Copy" fill="#FF3287"/>
<path d="M56.948,0 C50.745,0 47.606,3.43 47.606,8.296 C47.606,14.114 51.036,15.404 55.518,17.132 C60.438,18.853 61.782,19.332 61.782,21.539 C61.782,24.225 58.969,24.867 57.401,24.867 C54.579,24.867 52.493,23.342 51.536,20.858 L47,24.185 C49.43,28.937 52.145,30.185 57.713,30.185 C59.364,30.185 62.059,29.74 63.727,28.694 C67.779,26.156 67.779,22.22 67.779,20.898 C67.779,18.129 66.531,16.207 66.178,15.726 C65.049,14.121 63.032,12.918 61.25,12.278 L57.084,10.914 C55.073,10.267 52.928,10.105 52.928,8.019 C52.928,7.707 53.008,5.528 56.288,5.319 L61.465,5.319 L61.465,0 C61.465,0 57.342,0 56.948,0 Z" id="Path-Copy-2" fill="#00AFF4"/>
<polygon id="Path" fill="#00AFF4" points="5.32907052e-15 1.77635684e-15 5.32907052e-15 5.319 7.572 5.319 7.572 29.564 14.132 29.564 14.132 5.319 21.544 5.319 21.544 1.77635684e-15"/>
</g>
</g>
</svg> </div>
</div>
</div>    
</body>
</html>