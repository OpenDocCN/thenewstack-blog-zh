# 硬件独立性对于机器学习的创新至关重要

> 原文：<https://thenewstack.io/hardware-independence-is-critical-to-innovation-in-machine-learning/>

我们正处于风暴之中。一方面，全球芯片短缺看不到尽头，迫使大公司重新预测产品目标。另一方面，大型硬件公司(例如，[英特尔](https://thenewstack.io/intel-ceo-sheds-light-on-emerging-software-strategy/)、[英伟达](https://thenewstack.io/nvidias-cuda-12-is-here-to-bring-out-the-animal-in-gpus/)、 [Arm](https://thenewstack.io/big-three-in-cloud-prompts-arm-to-rethink-software/) )和更小、更专业的芯片初创公司(例如，脑波系统公司和[安培计算](https://thenewstack.io/arm-server-chips-get-a-boost-with-amperes-altra/))之间存在竞争，它们都在争夺人工智能的主导地位。

更糟糕的是，我们(ML 研究人员和实践者)生活在一个依赖硬件的世界，模型和软件必须根据特定的硬件特征和框架进行调整和优化。这与软件世界形成了鲜明的对比，在软件世界中，代码很容易移植，并且存在 DevOps 这样的既定标准。

这一点，加上当前的经济环境，正迫使 ML 行业将重点从创新转移到预算上。鉴于通货膨胀加剧和经济衰退的说法，许多公司都在寻求在不影响用户体验的情况下削减成本，但事实证明这很难。但这也不足为奇，因为 [90%的 ML 计算成本与推理生产工作负载相关](https://aws.amazon.com/blogs/machine-learning/reduce-ml-inference-costs-on-amazon-sagemaker-with-hardware-and-software-acceleration/)。

如果可以将这些工作负载从一个云实例迁移到另一个云实例，或者从云迁移到边缘，这可以节省资金，但也可能会降低性能，导致应用程序或服务无法使用。

这对软件行业的许多人来说是一个冲击，因为我们已经习惯了从最新和最伟大的芯片创新(如云 GPU)中以较高的性价比转移通用软件工作负载。但在 ML 开发的情况下不是这样。

首先，不同硬件上的特定型号的成本和性能变化很大，而且往往不可预测。第二，迁移生产中的 ML 模型需要专业人才，这是稀缺和昂贵的。第三，即使您有这方面的人才，也仍然需要数周的手动工作来迁移工作负载——这是按型号计算的。即使你克服了所有这些障碍，你也有 50%的机会获得成本或性能收益。

数十亿美元的时间、人才和 R&D 投入到解决这种依赖问题中，但收效甚微。这威胁着人工智能的创新。

如果我们要实现人工智能的承诺，我们作为一个行业必须实现硬件独立。实现硬件独立将加快创新，为模型部署打开混合选项，并最终节省实践者的时间和精力。

## 打破依赖障碍

我不会在这里讨论如何克服这个问题的所有要点，但底线是所有这些都可以通过自动化来解决。一旦您能够实现模型的可移植性和自动化部署，您就可以实现:

**更快的创新**:硬件依赖的负面影响可以在不同的场景中表现出来。如果部署应用程序所需的某个硬件在您所在的地区没有，或者根本没有，那该怎么办？例如，您希望部署在 GPU 上，但是可用性受到限制。比如说，将选择权转移到 CPU 上，可以减轻 GPU 的压力，让您的应用程序以最佳性能运行。

这是另一个例子。如果您试图扩展一个应用程序，但是您不能足够准确地预测应用程序的负载以添加/删除实例来满足需求的变化，该怎么办？硬件独立性使您能够轻松优化运行他们的模型，以便根据需要进行扩展。

这里的关键词是选择。有些硬件更适合某些应用程序，因此选择硬件目标至关重要。想想资源受限的环境，比如 edge，在那里您必须使用 edge 处理器。拥有独立性使从业者能够在任何环境中工作。这适用于任何环境:可穿戴设备、智能相机、自动驾驶汽车，你能想到的。

最终，通过支持硬件目标的选择来消除依赖性，可以帮助您利用现有资源，并通过最大限度地提高性能来延长现有资源的使用寿命。

**混合部署**:硬件独立性支持 ML 模型在内部部署和云到边缘之间迁移，甚至拆分。想想产生 art 的最新稳定扩散模型。要使其以当前形式部署在移动应用程序中变得可行，可能需要划分工作负载，一个在本地，一个在云中。或者想一个代表一种写作风格的语言模型。如果你在电脑上打字，你是在云中运行这个模型。但是如果你在手机上打字，你会希望在本地运行模型以获得响应。支持 ML 模型在不同硬件之间的流畅迁移将支持新的体验和 ML 对应用的模式影响。

**能源/成本效率**:硬件独立性和特定于目标的模型优化使用户可以选择在更节能或更具成本效益的地方运行。如果能够运行推理来突出从 GCP 英特尔 Cascade Lake 到 AWS Graviton3 的迁移优势，就可以快速显示成本节约。事实上，我们自己做了这项研究，发现进行这种迁移将为自然语言处理(NLP)模型(如 GPT-2)节省 73%的计算成本。这只是一个例子。未来硬件中专用功能单元的趋势将使这一点更加明显。

## ML 的未来取决于选择

我们生活在有趣的时代。ML 正准备在许多行业进行真正的变革。因此，企业正在大举投资。但是考虑到公司可以运行和部署应用的限制，投资回报率仍然是个问题。ML 的成功依赖于适应性和敏捷性，这两者目前都很缺乏。

无论未来如何，有一点是肯定的:硬件独立性将是 ML 成功的决定性因素。但是考虑到影响计算机可用性和灵活性的所有更大的力量，依赖于硬件的 ML 肯定会阻碍创新和商业运作。

<svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 68 31" version="1.1"><title>Group</title> <desc>Created with Sketch.</desc></svg>